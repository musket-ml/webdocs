{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"You have just found Musket Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster Goals and principles Compactness and declarative description Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: [] Reproducibility and ease of sharing As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder. Established way to store and compare results Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling. Flexibility and extensibility Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path Pipelines and IDE Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis. Generic pipeline Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Segmentation Pipeline Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs Classification Pipeline Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Home"},{"location":"#you-have-just-found-musket","text":"Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster","title":"You have just found Musket"},{"location":"#goals-and-principles","text":"","title":"Goals and principles"},{"location":"#compactness-and-declarative-description","text":"Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: []","title":"Compactness and declarative description"},{"location":"#reproducibility-and-ease-of-sharing","text":"As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder.","title":"Reproducibility and ease of sharing"},{"location":"#established-way-to-store-and-compare-results","text":"Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling.","title":"Established way to store and compare results"},{"location":"#flexibility-and-extensibility","text":"Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path","title":"Flexibility and extensibility"},{"location":"#pipelines-and-ide","text":"Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis.","title":"Pipelines and IDE"},{"location":"#generic-pipeline","text":"Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Generic pipeline"},{"location":"#segmentation-pipeline","text":"Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs","title":"Segmentation Pipeline"},{"location":"#classification-pipeline","text":"Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Classification Pipeline"},{"location":"classification/","text":"Classification training pipeline My puny attempt to build reusable training pipeline for image classification Motivation Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library Installation At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install classification_pipeline Note: this package requires python 3.6 Usage guide Training a model Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits. Image Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees Freezing/Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture. Custom datasets You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class Classification: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage. Composite losses Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. Cyclical learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"User guide"},{"location":"classification/#classification-training-pipeline","text":"My puny attempt to build reusable training pipeline for image classification","title":"Classification training pipeline"},{"location":"classification/#motivation","text":"Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library","title":"Motivation"},{"location":"classification/#installation","text":"At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install classification_pipeline Note: this package requires python 3.6","title":"Installation"},{"location":"classification/#usage-guide","text":"","title":"Usage guide"},{"location":"classification/#training-a-model","text":"Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits.","title":"Training a model"},{"location":"classification/#image-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees","title":"Image Augmentations"},{"location":"classification/#freezingunfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing/Unfreezing encoder"},{"location":"classification/#custom-datasets","text":"You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class Classification: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img","title":"Custom datasets"},{"location":"classification/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"classification/#multistage-training","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage.","title":"Multistage training"},{"location":"classification/#composite-losses","text":"Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions.","title":"Composite losses"},{"location":"classification/#cyclical-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300","title":"Cyclical learning rates"},{"location":"classification/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"classification/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"classification/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False)","title":"Using trained model"},{"location":"classification/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"classification/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"classification/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"classification/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"classification/#what-is-supported","text":"At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"classification/#custom-architectures-callbacks-metrics","text":"Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"classification/reference/","text":"","title":"Reference"},{"location":"generic/","text":"Reasons to use Generic Pipeline TODO: add more text from a general promo here Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Installation TODO: make sure this actually works. pip install generic_pipeline Project structure Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. main.py will be always executed, other files require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc. Launching TODO General train properties Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. There are many more properties to check in Reference of root properties Definining networks Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment. Built-in NN layers There are a lot of built-in NN layers, basically, all of those supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types Control layers Utility layers can be used to set control and data flow inside their bodies. Here are some examples: Simple Data Flow constructions inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1] Repeat and With declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120] Conditional layers declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid] Shared Weights #Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations Wrapper layers net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid] Manually controlling data flow net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here Datasets Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities. Callbacks Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here Stages stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Preprocessors Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min]) How to check training results In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"User guide"},{"location":"generic/#reasons-to-use-generic-pipeline","text":"TODO: add more text from a general promo here Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Reasons to use Generic Pipeline"},{"location":"generic/#installation","text":"TODO: make sure this actually works. pip install generic_pipeline","title":"Installation"},{"location":"generic/#project-structure","text":"Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. main.py will be always executed, other files require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc.","title":"Project structure"},{"location":"generic/#launching","text":"TODO","title":"Launching"},{"location":"generic/#general-train-properties","text":"Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. There are many more properties to check in Reference of root properties","title":"General train properties"},{"location":"generic/#definining-networks","text":"Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment.","title":"Definining networks"},{"location":"generic/#built-in-nn-layers","text":"There are a lot of built-in NN layers, basically, all of those supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types","title":"Built-in NN layers"},{"location":"generic/#control-layers","text":"Utility layers can be used to set control and data flow inside their bodies. Here are some examples:","title":"Control layers"},{"location":"generic/#simple-data-flow-constructions","text":"inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1]","title":"Simple Data Flow constructions"},{"location":"generic/#repeat-and-with","text":"declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120]","title":"Repeat and With"},{"location":"generic/#conditional-layers","text":"declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid]","title":"Conditional layers"},{"location":"generic/#shared-weights","text":"#Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations","title":"Shared Weights"},{"location":"generic/#wrapper-layers","text":"net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid]","title":"Wrapper layers"},{"location":"generic/#manually-controlling-data-flow","text":"net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here","title":"Manually controlling data flow"},{"location":"generic/#datasets","text":"Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities.","title":"Datasets"},{"location":"generic/#callbacks","text":"Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here","title":"Callbacks"},{"location":"generic/#stages","text":"stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Stages"},{"location":"generic/#preprocessors","text":"Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min])","title":"Preprocessors"},{"location":"generic/#how-to-check-training-results","text":"In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"How to check training results"},{"location":"generic/reference/","text":"Generic pipeline reference Pipeline root properties activation TODO: does it have any use in the root of the file? aggregation_metric type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout architecture type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork augmentation type : ```` TODO: does it have any use in the root of the file? Example: batch type : integer Sets up training batch size. Example: batch: 512 classes type : ```` TODO: does it have any use in generic pipeline? Example: callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : ```` Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : ```` Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] dataset_augmenter type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. TODO: check that description and example are correct? Example: dataset_augmenter: name: TheAugmenter parameter: test dropout type : ```` TODO: does it have any use in generic pipeline root? Example: declarations type : ```` Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"] extra_train_data type : ```` D Example: folds_count type : ```` D Example: freeze_encoder type : ```` D Example: final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] holdout type : ```` D Example: imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : ```` D Example: loss type : ```` Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : ```` D Example: metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : ```` D Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : ```` D Example: random_state type : ```` D Example: stages type : ```` D Example: stratified type : ```` D Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` D Example: testTimeAugmentation type : ```` D Example: transforms type : ```` D Example: validationSplit type : ```` D Example: Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] TODO: examples from lr_variation_callback.py look strange, also it is unclear how to number of steps is being set up. Example TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Layer types Input TODO: description Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example: GaussianNoise Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example: Dropout Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. Example: declarations: net: - dropout: 0.5 SpatialDropout1D Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example: LSTM Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units - Positive integer, dimensionality of the output space. return_sequences - Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state - Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. stateful - Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example: GlobalMaxPool1D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: GlobalAveragePooling1D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: BatchNormalization D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Concatenate D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Add D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Substract D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Mult D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Max D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Min D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Conv1D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Conv2D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: MaxPool1D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: MaxPool2D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: AveragePooling1D D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: CuDNNLSTM D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Dense D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Flatten D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Bidirectional D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Utility layers split D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-concat D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-concatenate D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-add D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-substract D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-mult D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-min D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-max D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-dot D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-dot-normalize D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: seq D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: input D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: cache D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: disk-cache D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-preprocessor D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-concat-preprocessor D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: seq-preprocessor D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: augmentation D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: pass D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-concat D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-add D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: Stage properties loss initial_weights epochs unfreeze_encoder lr callbacks extra_callbacks Preprocessors","title":"Reference"},{"location":"generic/reference/#generic-pipeline-reference","text":"","title":"Generic pipeline reference"},{"location":"generic/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"generic/reference/#activation","text":"TODO: does it have any use in the root of the file?","title":"activation"},{"location":"generic/reference/#aggregation_metric","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: aggregation_metric: matthews_correlation_holdout","title":"aggregation_metric"},{"location":"generic/reference/#architecture","text":"type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork","title":"architecture"},{"location":"generic/reference/#augmentation","text":"type : ```` TODO: does it have any use in the root of the file? Example:","title":"augmentation"},{"location":"generic/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 512","title":"batch"},{"location":"generic/reference/#classes","text":"type : ```` TODO: does it have any use in generic pipeline? Example:","title":"classes"},{"location":"generic/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"generic/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"generic/reference/#clipnorm","text":"type : ```` Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"generic/reference/#clipvalue","text":"type : ```` Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"generic/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"generic/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"generic/reference/#dataset_augmenter","text":"type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. TODO: check that description and example are correct? Example: dataset_augmenter: name: TheAugmenter parameter: test","title":"dataset_augmenter"},{"location":"generic/reference/#dropout","text":"type : ```` TODO: does it have any use in generic pipeline root? Example:","title":"dropout"},{"location":"generic/reference/#declarations","text":"type : ```` Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"]","title":"declarations"},{"location":"generic/reference/#extra_train_data","text":"type : ```` D Example:","title":"extra_train_data"},{"location":"generic/reference/#folds_count","text":"type : ```` D Example:","title":"folds_count"},{"location":"generic/reference/#freeze_encoder","text":"type : ```` D Example:","title":"freeze_encoder"},{"location":"generic/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"generic/reference/#holdout","text":"type : ```` D Example:","title":"holdout"},{"location":"generic/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"generic/reference/#inference_batch","text":"type : ```` D Example:","title":"inference_batch"},{"location":"generic/reference/#loss","text":"type : ```` Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"generic/reference/#lr","text":"type : ```` D Example:","title":"lr"},{"location":"generic/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"generic/reference/#num_seeds","text":"type : ```` D Example:","title":"num_seeds"},{"location":"generic/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"generic/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"generic/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"generic/reference/#preprocessing","text":"type : ```` D Example:","title":"preprocessing"},{"location":"generic/reference/#random_state","text":"type : ```` D Example:","title":"random_state"},{"location":"generic/reference/#stages","text":"type : ```` D Example:","title":"stages"},{"location":"generic/reference/#stratified","text":"type : ```` D Example:","title":"stratified"},{"location":"generic/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"generic/reference/#testsplitseed","text":"type : ```` D Example:","title":"testSplitSeed"},{"location":"generic/reference/#testtimeaugmentation","text":"type : ```` D Example:","title":"testTimeAugmentation"},{"location":"generic/reference/#transforms","text":"type : ```` D Example:","title":"transforms"},{"location":"generic/reference/#validationsplit","text":"type : ```` D Example:","title":"validationSplit"},{"location":"generic/reference/#callback-types","text":"","title":"Callback types"},{"location":"generic/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"generic/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"generic/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"generic/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] TODO: examples from lr_variation_callback.py look strange, also it is unclear how to number of steps is being set up. Example","title":"LRVariator"},{"location":"generic/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"generic/reference/#layer-types","text":"","title":"Layer types"},{"location":"generic/reference/#input","text":"TODO: description Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example:","title":"Input"},{"location":"generic/reference/#gaussiannoise","text":"Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example:","title":"GaussianNoise"},{"location":"generic/reference/#dropout_1","text":"Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. Example: declarations: net: - dropout: 0.5","title":"Dropout"},{"location":"generic/reference/#spatialdropout1d","text":"Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example:","title":"SpatialDropout1D"},{"location":"generic/reference/#lstm","text":"Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units - Positive integer, dimensionality of the output space. return_sequences - Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state - Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. stateful - Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example:","title":"LSTM"},{"location":"generic/reference/#globalmaxpool1d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"GlobalMaxPool1D"},{"location":"generic/reference/#globalaveragepooling1d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"GlobalAveragePooling1D"},{"location":"generic/reference/#batchnormalization","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"BatchNormalization"},{"location":"generic/reference/#concatenate","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Concatenate"},{"location":"generic/reference/#add","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Add"},{"location":"generic/reference/#substract","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Substract"},{"location":"generic/reference/#mult","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Mult"},{"location":"generic/reference/#max","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Max"},{"location":"generic/reference/#min","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Min"},{"location":"generic/reference/#conv1d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Conv1D"},{"location":"generic/reference/#conv2d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Conv2D"},{"location":"generic/reference/#maxpool1d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"MaxPool1D"},{"location":"generic/reference/#maxpool2d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"MaxPool2D"},{"location":"generic/reference/#averagepooling1d","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"AveragePooling1D"},{"location":"generic/reference/#cudnnlstm","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"CuDNNLSTM"},{"location":"generic/reference/#dense","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Dense"},{"location":"generic/reference/#flatten","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Flatten"},{"location":"generic/reference/#bidirectional","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"Bidirectional"},{"location":"generic/reference/#utility-layers","text":"","title":"Utility layers"},{"location":"generic/reference/#split","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split"},{"location":"generic/reference/#split-concat","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-concat"},{"location":"generic/reference/#split-concatenate","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-concatenate"},{"location":"generic/reference/#split-add","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-add"},{"location":"generic/reference/#split-substract","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-substract"},{"location":"generic/reference/#split-mult","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-mult"},{"location":"generic/reference/#split-min","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-min"},{"location":"generic/reference/#split-max","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-max"},{"location":"generic/reference/#split-dot","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-dot"},{"location":"generic/reference/#split-dot-normalize","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-dot-normalize"},{"location":"generic/reference/#seq","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"seq"},{"location":"generic/reference/#input_1","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"input"},{"location":"generic/reference/#cache","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"cache"},{"location":"generic/reference/#disk-cache","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"disk-cache"},{"location":"generic/reference/#split-preprocessor","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-preprocessor"},{"location":"generic/reference/#split-concat-preprocessor","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-concat-preprocessor"},{"location":"generic/reference/#seq-preprocessor","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"seq-preprocessor"},{"location":"generic/reference/#augmentation_1","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"augmentation"},{"location":"generic/reference/#pass","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"pass"},{"location":"generic/reference/#transform-concat","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"transform-concat"},{"location":"generic/reference/#transform-add","text":"D Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"transform-add"},{"location":"generic/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"generic/reference/#loss_1","text":"","title":"loss"},{"location":"generic/reference/#initial_weights","text":"","title":"initial_weights"},{"location":"generic/reference/#epochs","text":"","title":"epochs"},{"location":"generic/reference/#unfreeze_encoder","text":"","title":"unfreeze_encoder"},{"location":"generic/reference/#lr_1","text":"","title":"lr"},{"location":"generic/reference/#callbacks_1","text":"","title":"callbacks"},{"location":"generic/reference/#extra_callbacks","text":"","title":"extra_callbacks"},{"location":"generic/reference/#preprocessors","text":"","title":"Preprocessors"},{"location":"segmentation/","text":"Segmentation Training Pipeline Motivation Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library Installation At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install segmentation_pipeline Note: this package requires python 3.6 Usage guide Training a model Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Image and Mask Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees Freezing and Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture. Custom datasets Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used in my experiments with Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE: def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5) Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage. Composite losses Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. Cyclical learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Background Augmenter One interesting augentation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model. Examples Training background removal task(Pics Art Hackaton) in google collab FAQ How to continue training after crash? If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds) My notebooks constantly run out of memory, what can I do to reduce memory usage? One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3 How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter? BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5] How can I visualize images that are used for training (after augmentations)? You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch What I can do if i have some extra training data, that should not be included into validation, but should be used during the training? extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people How to get basic statistics across my folds/stages This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info() I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage? There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real What if I would like to build a really large ansemble of models? One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d) How to train on multiple gpus? cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"User guide"},{"location":"segmentation/#segmentation-training-pipeline","text":"","title":"Segmentation Training Pipeline"},{"location":"segmentation/#motivation","text":"Idea for this project came from my first attempts to participate in Kaggle competitions. My programmers heart was painfully damaged by looking on my own code as well as on other people kernels. Code was highly repetitive, suffering from numerous reimplementations of same or almost same things through the kernels, model/experiment configuration was often mixed with models code, in other words - from programmer perspective it all looked horrible. So I decided to extract repetitive things into framework that will work at least for me and will follow these statements: - experiment configurations should be cleanly separated from model definitions; - experiment configuration files should be easy to compare and should fully describe experiment that is being performed except for the dataset; - common blocks like an architecture, callbacks, storing model metrics, visualizing network predictions, should be written once and be a part of common library","title":"Motivation"},{"location":"segmentation/#installation","text":"At this moment library requires the latest version of imgaug which has not been published yet to pip, so installation requires execution of following two commands pip install git+https://github.com/aleju/imgaug pip install segmentation_pipeline Note: this package requires python 3.6","title":"Installation"},{"location":"segmentation/#usage-guide","text":"","title":"Usage guide"},{"location":"segmentation/#training-a-model","text":"Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically.","title":"Training a model"},{"location":"segmentation/#image-and-mask-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees","title":"Image and Mask Augmentations"},{"location":"segmentation/#freezing-and-unfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing and Unfreezing encoder"},{"location":"segmentation/#custom-datasets","text":"Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used in my experiments with Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE: def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5)","title":"Custom datasets"},{"location":"segmentation/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"segmentation/#multistage-training","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file like in the following example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Stage entries allow you to configure custom learning rate, balance of negative examples, callbacks, loss function and even initial weights which should be used on a particular stage.","title":"Multistage training"},{"location":"segmentation/#composite-losses","text":"Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions.","title":"Composite losses"},{"location":"segmentation/#cyclical-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300","title":"Cyclical learning rates"},{"location":"segmentation/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"segmentation/#background-augmenter","text":"One interesting augentation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve","title":"Background Augmenter"},{"location":"segmentation/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"segmentation/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False)","title":"Using trained model"},{"location":"segmentation/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"segmentation/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"segmentation/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"segmentation/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"segmentation/#what-is-supported","text":"At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"segmentation/#custom-architectures-callbacks-metrics","text":"Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"segmentation/#examples","text":"Training background removal task(Pics Art Hackaton) in google collab","title":"Examples"},{"location":"segmentation/#faq","text":"","title":"FAQ"},{"location":"segmentation/#how-to-continue-training-after-crash","text":"If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds)","title":"How to continue training after crash?"},{"location":"segmentation/#my-notebooks-constantly-run-out-of-memory-what-can-i-do-to-reduce-memory-usage","text":"One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3","title":"My notebooks constantly run out of memory, what can I do to reduce memory usage?"},{"location":"segmentation/#how-can-i-run-sepate-set-of-augmenters-on-initial-imagemask-when-replacing-backgrounds-with-background-augmenter","text":"BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5]","title":"How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter?"},{"location":"segmentation/#how-can-i-visualize-images-that-are-used-for-training-after-augmentations","text":"You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch","title":"How can I visualize images that are used for training (after augmentations)?"},{"location":"segmentation/#what-i-can-do-if-i-have-some-extra-training-data-that-should-not-be-included-into-validation-but-should-be-used-during-the-training","text":"extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people","title":"What I can do if i have some extra training data, that should not be included into validation, but should be used during the training?"},{"location":"segmentation/#how-to-get-basic-statistics-across-my-foldsstages","text":"This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info()","title":"How to get basic statistics across my folds/stages"},{"location":"segmentation/#i-have-some-callbacks-that-are-configured-globally-but-i-need-some-extra-callbacks-for-my-last-training-stage","text":"There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real","title":"I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage?"},{"location":"segmentation/#what-if-i-would-like-to-build-a-really-large-ansemble-of-models","text":"One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d)","title":"What if I would like to build a really large ansemble of models?"},{"location":"segmentation/#how-to-train-on-multiple-gpus","text":"cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"How to train on multiple gpus?"},{"location":"segmentation/reference/","text":"","title":"Reference"}]}