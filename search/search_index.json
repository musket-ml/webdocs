{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"You have just found Musket Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster There are some videos to check here . Main framework website: musket-ml.com Goals and principles Compactness and declarative description Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: [] Reproducibility and ease of sharing As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder. Established way to store and compare results Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling. Flexibility and extensibility Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path Pipelines and IDE Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis. Generic pipeline Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Segmentation Pipeline Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs Classification Pipeline Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Home"},{"location":"#you-have-just-found-musket","text":"Musket is a family of high-level frameworks written in Python and capable of running on top of Keras . It was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Use Musket if you need a deep learning framework that: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Provides IDE and visual tooling to make experimentation faster There are some videos to check here . Main framework website: musket-ml.com","title":"You have just found Musket"},{"location":"#goals-and-principles","text":"","title":"Goals and principles"},{"location":"#compactness-and-declarative-description","text":"Declarative description is always more compact and human-readable than imperative description. All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. This is a simple classification experiment, and half of these instructions can be actually omitted: #%Musket Classification 1.0 architecture: Xception classes: 101 activation: softmax weights: imagenet shape: [512, 512, 4] optimizer: Adam batch: 8 lr: 0.001 primary_metric: val_macro_f1 primary_metric_mode: max dataset: combinations_train: []","title":"Compactness and declarative description"},{"location":"#reproducibility-and-ease-of-sharing","text":"As each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Putting YAML files into git or sharing them in other way provides other team members with an easy way to reproduce the same experiments locally. Anyone can check your experiments, and add their own to the storage as the storage is simply a folder.","title":"Reproducibility and ease of sharing"},{"location":"#established-way-to-store-and-compare-results","text":"Musket is lazy by its nature. Each experiment starts with a simple YAML description. There may be many stages in training and prediction, starting with calculating datasets, preprocessing and finishing with inferring and calculating statistics, but for each stage Musket saves results in the sub-folders of experiment folder. When the experiment is launched, Musket checks, which result files are already in place and only runs what is needed. It is up to team members, what to share: pure YAML desciptions, YAML and final metrics (to compare experiment effectiveness), or also, potentially more heavy intermediate results so other team members can run experiments faster locally. It is easy to compare two experiments with each other by running any text compare tooling, experiments are just YAML text: As all experiment statistics is also saved as files, it is easy to compare experiment results and find the best ones by the same text files comparison tooling. IDE helps here, too, by adding results visualisation tooling.","title":"Established way to store and compare results"},{"location":"#flexibility-and-extensibility","text":"Declarative approach is good and compact, but sometimes we want to define some custom functionality. Musket supports lots of custom substances: dataset definitions, preprocessors, custom network layers, visualizers etc etc. Most of the time to define a custom thing, it is enough to put a python file into a top-level folder and define a function with an appropriate annotation, like this: @preprocessing.dataset_preprocessor def splitInput(input, parts:int): result = np.array_split(input,parts,axis=0) return result or this: @dataset_visualizer def visualize(val:PredictionItem): cache_path=context().path path = cache_path + \"/\" + str(val.id) + \".png\" if os.path.exists(path): return path ma = val.x/128 - preprocessors.moving_average(val.x/128, 8000) std = np.std(ma) ma[np.where(np.abs(ma) - 2 * std < 0)] = 0 v = ma fig, axs = plt.subplots(1, 1, constrained_layout=True, figsize=(15, 10)) v[:, 0] += 1 v[:, 2] -= 1 plt.ylim(-2, 2) axs.plot(v[:, 0], label='Phase 0') axs.plot(v[:, 1], label='Phase 1') axs.plot(v[:, 2], label='Phase 2') axs.legend() if sum(val.y) > 0: axs.set_title('bad wire:' + str(val.id)) plt.savefig(path) else: axs.set_title('normal wire:' + str(val.id)) plt.savefig(path) try: plt.close() except: pass return path","title":"Flexibility and extensibility"},{"location":"#pipelines-and-ide","text":"Musket is family of frameworks, not a single framework for a reason. There is a core part, a pipeline called Generic Pipeline , which is quite universal and can handle any type of tasks. Besides it, there are also specialized pipelines with YAML domain syntax better suited for a particular task like Segmentation Pipeline or Classification Pipeline . Such specialized frameworks has reduced flexibility, but more rapid prototyping and a whole set of useful built-ins. All of those pipelines are supported by musket IDE, which simplifies experiment running and result analysis.","title":"Pipelines and IDE"},{"location":"#generic-pipeline","text":"Generic pipeline has the most universal YAML-based domain-specific syntax of all pipelines. Its main feature is an ability to define custom neural networks in a declarative manner by declaring blocks basing on built-in blocks, and then referring custom blocks from other custom blocks. There is also a rich set of declarative instructions that control dataflow inside the network. Most elements like datasets, preprocessors, network blocks, loss functions, metrics etc can be customly defined in python code and later reused from YAML. imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: #- gaussianNoise: 0.0001 - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Generic pipeline"},{"location":"#segmentation-pipeline","text":"Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is much easier to define an architecture of the network, just name it: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs","title":"Segmentation Pipeline"},{"location":"#classification-pipeline","text":"Classification Pipeline has a lot of common parts with Generic pipeline too, and as in Segmentation Pipeline it is easy to define an architecture of the network, just name it and set the number of output classes: architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs","title":"Classification Pipeline"},{"location":"classification/","text":"Classification training pipeline Classification pipeline is a declarative pipeline for single and multi output image classification tasks. It can be also used for regression tasks. This package is a part of Musket ML framework. Reasons to use Classification Pipeline Classification Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Classification Pipeline has a lot of common parts with Generic pipeline , but it is easier to define an architecture of the network. Also there are a number of classification-specific features. The pipeline provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions It is easy to configure network architecture Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library Installation pip install classification_pipeline Note: this package requires python 3.6 This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here . Launching Launching experiments fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference Launching tasks task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference Launching project analysis analize.py script is designed to launch project-scope analysis. It is located in the musket_core root folder. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference Usage guide Training a model Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. Moreover, the whole fitting and prediction process can be launched with built-in script, the only really required python code is dataset definition to let the system know, which data to load. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits. Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Lets discover what's going on in more details: General train properties Lets take our standard example and check the following set of instructions: testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties Defining architecture Lets take a look at the following part of our example: architecture: DenseNet201 #let's select architecture that we would like to use classes: 28 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation weights: imagenet shape: [224, 224, 3] #This is our desired input image and mask size, everything will be resized to fit. The following three properties are required to set: architecture This property configures architecture that should be used. net , Linknet , PSP , FPN and more are supported. classes sets the number of classes that should be used. The following ones are optional, but commonly used: activation sets activation function that should be used in last layer. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. weights configures initial weights of the encoder. Image Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees augmentation property defines IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. In this example, Fliplr and Flipud keys are automatically mapped on Flip agugmenters , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example scale key of Affine is mapped on scale parameter of Affine augmenter . One interesting augementation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve Freezing and Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Both freeze_encoder and unfreeze_encoder can be put into the root section and inside the stage. Note: This option is not supported for DeeplabV3 architecture. Custom datasets You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class ClassificationDS: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img def getTrain()->datasets.DataSet: return ClassificationDS(\"images/\") Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities. Multi output classification Sometimes you need to create network that performs several classification tasks at the same moment, in this situation you need to declare classes , activation and loss as the lists of class counts, activation functions and losses like in the following snippet: classes: [ 4, 4 ] #define the number of classes activation: [sigmoid,sigmoid] loss: - binary_crossentropy - binary_crossentropy primary_metric: val_loss #the most interesting metric is val_binary_accuracy primary_metric_mode: min it is also very likely that you need to change primary metric to val_loss preparing dataset for multi output classification: When you use multi output classification your prediction item y should be a list of numpy arrays, like in the following sample: return PredictionItem(self.ids[item], image, [t0,t1,t2]) ```` length of this list should be equal to the number of network outputs #### Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. [stages](reference.md#stages) instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found [here](reference.md#stage-properties). ```yaml stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Advanced learning rates Dynamic learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly. LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. crops property sets the number of single dimension cells. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"User guide"},{"location":"classification/#classification-training-pipeline","text":"Classification pipeline is a declarative pipeline for single and multi output image classification tasks. It can be also used for regression tasks. This package is a part of Musket ML framework.","title":"Classification training pipeline"},{"location":"classification/#reasons-to-use-classification-pipeline","text":"Classification Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Classification Pipeline has a lot of common parts with Generic pipeline , but it is easier to define an architecture of the network. Also there are a number of classification-specific features. The pipeline provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions It is easy to configure network architecture Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library","title":"Reasons to use Classification Pipeline"},{"location":"classification/#installation","text":"pip install classification_pipeline Note: this package requires python 3.6 This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here .","title":"Installation"},{"location":"classification/#launching","text":"","title":"Launching"},{"location":"classification/#launching-experiments","text":"fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference","title":"Launching experiments"},{"location":"classification/#launching-tasks","text":"task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference","title":"Launching tasks"},{"location":"classification/#launching-project-analysis","text":"analize.py script is designed to launch project-scope analysis. It is located in the musket_core root folder. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference","title":"Launching project analysis"},{"location":"classification/#usage-guide","text":"","title":"Usage guide"},{"location":"classification/#training-a-model","text":"Let's start from a simple example of classification. Suppose, your data are structured as follows: a .cvs file with images ids and their labels and a folder with all these images. For training a neural network to classify these images all you need are few lines of python code: import musket_core from classification_pipeline import classification class ProteinDataGenerator: def __init__(self, paths, labels): self.paths, self.labels = paths, labels def __len__(self): return len(self.paths) def __getitem__(self, idx): X,y = self.__load_image(self.paths[idx]),self.labels[idx] return PredictionItem(self.paths[idx],X, y) def __load_image(self, path): R = Image.open(path + '_red.png') G = Image.open(path + '_green.png') B = Image.open(path + '_blue.png') im = np.stack(( np.array(R), np.array(G), np.array(B), ), -1) return im dataset = ProteinDataGenerator(paths,labels) cfg = classification.parse(\"config.yaml\") cfg.fit(dataset) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. architecture: DenseNet201 #pre-trained model we are going to use pooling: avg augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 28 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid weights: imagenet #we would like to start from network pretrained on imagenet dataset shape: [224, 224, 3] #our desired input image size, everything will be resized to fit testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #our batch size will be 16 lr: 0.005 copyWeights: true metrics: #we would like to track some metrics - binary_accuracy - macro_f1 primary_metric: val_binary_accuracy #the most interesting metric is val_binary_accuracy primary_metric_mode: max callbacks: #configure some minimal callbacks EarlyStopping: patience: 3 monitor: val_macro_f1 mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 10 #let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. Moreover, the whole fitting and prediction process can be launched with built-in script, the only really required python code is dataset definition to let the system know, which data to load. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/label tuples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits. Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Lets discover what's going on in more details:","title":"Training a model"},{"location":"classification/#general-train-properties","text":"Lets take our standard example and check the following set of instructions: testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties","title":"General train properties"},{"location":"classification/#defining-architecture","text":"Lets take a look at the following part of our example: architecture: DenseNet201 #let's select architecture that we would like to use classes: 28 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation weights: imagenet shape: [224, 224, 3] #This is our desired input image and mask size, everything will be resized to fit. The following three properties are required to set: architecture This property configures architecture that should be used. net , Linknet , PSP , FPN and more are supported. classes sets the number of classes that should be used. The following ones are optional, but commonly used: activation sets activation function that should be used in last layer. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. weights configures initial weights of the encoder.","title":"Defining architecture"},{"location":"classification/#image-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees augmentation property defines IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. In this example, Fliplr and Flipud keys are automatically mapped on Flip agugmenters , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example scale key of Affine is mapped on scale parameter of Affine augmenter . One interesting augementation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve","title":"Image Augmentations"},{"location":"classification/#freezing-and-unfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Both freeze_encoder and unfreeze_encoder can be put into the root section and inside the stage. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing and Unfreezing encoder"},{"location":"classification/#custom-datasets","text":"You can declare your own dataset class as in this example: from musket_core.datasets import PredictionItem import os import imageio import pandas as pd import numpy as np import cv2 class ClassificationDS: def __init__(self,imgPath): self.species = ['Black-grass', 'Charlock', 'Cleavers', 'Common Chickweed', 'Common wheat', 'Fat Hen', 'Loose Silky-bent', 'Maize', 'Scentless Mayweed', 'Shepherds Purse', 'Small-flowered Cranesbill', 'Sugar beet'] self.data = [] self.targets = [] self.ids = [] for s_id, s in enumerate(self.species): s_folder = os.path.join(imgPath,s) for file in os.listdir(s_folder): self.data.append(os.path.join(s_folder, file)) self.targets.append(s_id) self.ids.append(file) def __len__(self): return len(self.data) def __getitem__(self, item): item_file = self.data[item] target = self.targets[item] t = np.zeros(len(self.species)) t[target] = 1.0 image = self.read_image(item_file, (224,224)) return PredictionItem(self.ids[item], image, t) def read_image(self, filepath, target_size=None): img = cv2.imread(filepath, cv2.IMREAD_COLOR) img = cv2.resize(img.copy(), target_size, interpolation = cv2.INTER_AREA) return img def getTrain()->datasets.DataSet: return ClassificationDS(\"images/\") Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities.","title":"Custom datasets"},{"location":"classification/#multi-output-classification","text":"Sometimes you need to create network that performs several classification tasks at the same moment, in this situation you need to declare classes , activation and loss as the lists of class counts, activation functions and losses like in the following snippet: classes: [ 4, 4 ] #define the number of classes activation: [sigmoid,sigmoid] loss: - binary_crossentropy - binary_crossentropy primary_metric: val_loss #the most interesting metric is val_binary_accuracy primary_metric_mode: min it is also very likely that you need to change primary metric to val_loss","title":"Multi output classification"},{"location":"classification/#preparing-dataset-for-multi-output-classification","text":"When you use multi output classification your prediction item y should be a list of numpy arrays, like in the following sample: return PredictionItem(self.ids[item], image, [t0,t1,t2]) ```` length of this list should be equal to the number of network outputs #### Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. [stages](reference.md#stages) instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found [here](reference.md#stage-properties). ```yaml stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"preparing dataset for multi output classification:"},{"location":"classification/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"classification/#advanced-learning-rates","text":"","title":"Advanced learning rates"},{"location":"classification/#dynamic-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly.","title":"Dynamic learning rates"},{"location":"classification/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg = classification.parse(config.yaml) ds = SimplePNGMaskDataSet(\"./train\",\"./train_mask\") - ??????????????????? finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"classification/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. crops property sets the number of single dimension cells. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"classification/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image classification. Let's say, we need to run image classification on images in the directory and store results in csv file: predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment preds = cfg.predict_all_to_array(dataset_test, 0, 0) for i, item in enumerate(dataset_test): images.append(dataset_test.get_id(i)) p = np.argmax(preds[i]) predictions.append(dataset_test.get_label(p)) #Let's store results in csv df = pd.DataFrame.from_dict({'file': images, 'species': predictions}) df.to_csv('submission.csv', index=False)","title":"Using trained model"},{"location":"classification/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_all_to_array like in the following example: cfg.predict_all_to_array(dataset_test, [0,1,2,3,4], 0) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"classification/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"classification/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"classification/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"classification/#what-is-supported","text":"At this moment classification pipeline supports following pre-trained models: - Resnet - ResNet18 - ResNet34 - ResNet50 - ResNet101 - ResNet152 - ResNeXt50 - ResNeXt101 - VGG : - VGG16 - VGG19 - InceptionV3 - InceptionResNetV2 - Xception - MobileNet - MobileNetV2 - DenseNet : - DenseNet121 - DenseNet169 - DenseNet201 - NasNet : - NASNetMobile - NASNetLarge Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"classification/#custom-architectures-callbacks-metrics","text":"Classification pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in classification.custom_models dictionary. For example: classification.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"classification/reference/","text":"Classification pipeline reference Pipeline root properties activation type : string Activation function that should be used in last layer. In the case of binary classification it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid experiment_result type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout architecture type : string This property configures decoder architecture that should be used: At this moment classification pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN augmentation type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 batch type : integer Sets up training batch size. Example: batch: 512 classes type : integer Number of classes that should be used. Example: callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 crops type : integer Defines the number of crops to make from original image by setting the single number of single dimension cells. In example, the value of 3 will split the original image into 9 cells: 3 by horizontal and 3 by vertical. Example: crops: 3 dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] dataset_augmenter type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test dropout type : float Example: extra_train_data type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example: folds_count type : integer Number of folds to train. Default is 5. Example: freeze_encoder type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] holdout type : ```` Example: imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : integer Size of batch during inferring process. Example: loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: lr: 0.01 manualResize type : boolean Setting this property to true, will disable auto resize that is performed by pipeline Example: manualResize: true metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: shape type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3] stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: testTimeAugmentation type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example: transforms type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example: weights type : string This property configures initial weights of the encoder, supported values: imagenet Example: weights: imagenet Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Stage properties callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 epochs type : integer Number of epochs to train for this stage. Example: extra_callbacks freeze_encoder type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true initial_weights type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights negatives type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: unfreeze_encoder type : boolean Whether to unfreeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true validation_negatives type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter . fit script arguments fit.py project type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\" fit.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\" fit.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1 fit.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1 fit.py num_workers type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1 fit.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True fit.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True fit.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True fit.py only_report type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True fit.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\" fit.py folds type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2 task script arguments task.py project type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\" task.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\" task.py task type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\" task.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1 task.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1 task.py num_workers type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1 task.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True task.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True task.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True task.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\" analyze script arguments analyze.py inputFolder type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\" analyze.py output type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\" analyze.py onlyMetric type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\" analyze.py sortBy type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"Reference"},{"location":"classification/reference/#classification-pipeline-reference","text":"","title":"Classification pipeline reference"},{"location":"classification/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"classification/reference/#activation","text":"type : string Activation function that should be used in last layer. In the case of binary classification it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid","title":"activation"},{"location":"classification/reference/#experiment_result","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout","title":"experiment_result"},{"location":"classification/reference/#architecture","text":"type : string This property configures decoder architecture that should be used: At this moment classification pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN","title":"architecture"},{"location":"classification/reference/#augmentation","text":"type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"augmentation"},{"location":"classification/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 512","title":"batch"},{"location":"classification/reference/#classes","text":"type : integer Number of classes that should be used. Example:","title":"classes"},{"location":"classification/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"classification/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"classification/reference/#clipnorm","text":"type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"classification/reference/#clipvalue","text":"type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"classification/reference/#crops","text":"type : integer Defines the number of crops to make from original image by setting the single number of single dimension cells. In example, the value of 3 will split the original image into 9 cells: 3 by horizontal and 3 by vertical. Example: crops: 3","title":"crops"},{"location":"classification/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"classification/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"classification/reference/#dataset_augmenter","text":"type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test","title":"dataset_augmenter"},{"location":"classification/reference/#dropout","text":"type : float Example:","title":"dropout"},{"location":"classification/reference/#extra_train_data","text":"type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example:","title":"extra_train_data"},{"location":"classification/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example:","title":"folds_count"},{"location":"classification/reference/#freeze_encoder","text":"type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"freeze_encoder"},{"location":"classification/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"classification/reference/#holdout","text":"type : ```` Example:","title":"holdout"},{"location":"classification/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"classification/reference/#inference_batch","text":"type : integer Size of batch during inferring process. Example:","title":"inference_batch"},{"location":"classification/reference/#loss","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"classification/reference/#lr","text":"type : float Learning rate. Example: lr: 0.01","title":"lr"},{"location":"classification/reference/#manualresize","text":"type : boolean Setting this property to true, will disable auto resize that is performed by pipeline Example: manualResize: true","title":"manualResize"},{"location":"classification/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"classification/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"classification/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"classification/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"classification/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"classification/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"classification/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"classification/reference/#shape","text":"type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3]","title":"shape"},{"location":"classification/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"classification/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"classification/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"classification/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"classification/reference/#testtimeaugmentation","text":"type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example:","title":"testTimeAugmentation"},{"location":"classification/reference/#transforms","text":"type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"transforms"},{"location":"classification/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example:","title":"validationSplit"},{"location":"classification/reference/#weights","text":"type : string This property configures initial weights of the encoder, supported values: imagenet Example: weights: imagenet","title":"weights"},{"location":"classification/reference/#callback-types","text":"","title":"Callback types"},{"location":"classification/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"classification/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"classification/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"classification/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example","title":"LRVariator"},{"location":"classification/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"classification/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"classification/reference/#callbacks_1","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"classification/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"classification/reference/#extra_callbacks","text":"","title":"extra_callbacks"},{"location":"classification/reference/#freeze_encoder_1","text":"type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"freeze_encoder"},{"location":"classification/reference/#initial_weights","text":"type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights","title":"initial_weights"},{"location":"classification/reference/#negatives","text":"type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"negatives"},{"location":"classification/reference/#loss_1","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"classification/reference/#lr_1","text":"type : float Learning rate. Example:","title":"lr"},{"location":"classification/reference/#unfreeze_encoder","text":"type : boolean Whether to unfreeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"unfreeze_encoder"},{"location":"classification/reference/#validation_negatives","text":"type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"validation_negatives"},{"location":"classification/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"classification/reference/#cache","text":"Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"cache"},{"location":"classification/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"classification/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"classification/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"classification/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"classification/reference/#augmentation_1","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter .","title":"augmentation"},{"location":"classification/reference/#fit-script-arguments","text":"","title":"fit script arguments"},{"location":"classification/reference/#fitpy-project","text":"type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\"","title":"fit.py project"},{"location":"classification/reference/#fitpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\"","title":"fit.py name"},{"location":"classification/reference/#fitpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1","title":"fit.py num_gpus"},{"location":"classification/reference/#fitpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1","title":"fit.py gpus_per_net"},{"location":"classification/reference/#fitpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1","title":"fit.py num_workers"},{"location":"classification/reference/#fitpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True","title":"fit.py allow_resume"},{"location":"classification/reference/#fitpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True","title":"fit.py force_recalc"},{"location":"classification/reference/#fitpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True","title":"fit.py launch_tasks"},{"location":"classification/reference/#fitpy-only_report","text":"type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True","title":"fit.py only_report"},{"location":"classification/reference/#fitpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\"","title":"fit.py cache"},{"location":"classification/reference/#fitpy-folds","text":"type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2","title":"fit.py folds"},{"location":"classification/reference/#task-script-arguments","text":"","title":"task script arguments"},{"location":"classification/reference/#taskpy-project","text":"type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\"","title":"task.py project"},{"location":"classification/reference/#taskpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\"","title":"task.py name"},{"location":"classification/reference/#taskpy-task","text":"type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\"","title":"task.py task"},{"location":"classification/reference/#taskpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1","title":"task.py num_gpus"},{"location":"classification/reference/#taskpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1","title":"task.py gpus_per_net"},{"location":"classification/reference/#taskpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1","title":"task.py num_workers"},{"location":"classification/reference/#taskpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True","title":"task.py allow_resume"},{"location":"classification/reference/#taskpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True","title":"task.py force_recalc"},{"location":"classification/reference/#taskpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True","title":"task.py launch_tasks"},{"location":"classification/reference/#taskpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\"","title":"task.py cache"},{"location":"classification/reference/#analyze-script-arguments","text":"","title":"analyze script arguments"},{"location":"classification/reference/#analyzepy-inputfolder","text":"type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\"","title":"analyze.py inputFolder"},{"location":"classification/reference/#analyzepy-output","text":"type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\"","title":"analyze.py output"},{"location":"classification/reference/#analyzepy-onlymetric","text":"type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\"","title":"analyze.py onlyMetric"},{"location":"classification/reference/#analyzepy-sortby","text":"type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"analyze.py sortBy"},{"location":"docker/","text":"Building Musket ML image Building musket_ml image FROM python:3.6-buster WORKDIR /usr/src/app RUN pip install musket_ml cd musket_ml docker image build -t pythonmusket:1.0 . build and run server image cd ../server docker image build -t server:1.0 . docker container run --publish 8000:9393 --detach --name server_instance server:1.0 where 8000 - host port, 9393 docker instance port redirect docker instance's stderr and stdout to current terminal docker container attach server_instance stop instance docker container rm --force server_instance","title":"Home"},{"location":"docker/#building-musket-ml-image","text":"Building musket_ml image FROM python:3.6-buster WORKDIR /usr/src/app RUN pip install musket_ml cd musket_ml docker image build -t pythonmusket:1.0 .","title":"Building Musket ML image"},{"location":"docker/#build-and-run-server-image","text":"cd ../server docker image build -t server:1.0 . docker container run --publish 8000:9393 --detach --name server_instance server:1.0 where 8000 - host port, 9393 docker instance port","title":"build and run server image"},{"location":"docker/#redirect-docker-instances-stderr-and-stdout-to-current-terminal","text":"docker container attach server_instance","title":"redirect docker instance's stderr and stdout to current terminal"},{"location":"docker/#stop-instance","text":"docker container rm --force server_instance","title":"stop instance"},{"location":"generic/","text":"Generic pipeline Reasons to use Generic Pipeline Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs Installation Prerequisites The package has many prerequisites, but some of them are recommended to be installed manually. Tensorflow package of versions of 1.14 and below split into CPU and GPU ones. Moreover, Tensorflow may be more or less compatible with the version of CUDA/CUDNN installed. Here is the repository containing lots of pre-built Tensorflow wheels for Windows: tensorflow-windows-wheel . It can be used to choose the wheel depending on system architecture, CUDA/CUDNN version, CPU/GPU and Python version. Read more in Tensorflow installation guide . Keras has no strong dependency on Tensorflow, but in our common setup they run in pair. We used the 2.2.4 one. Shapely requires compilation on install on Linux/MacOS or pre-built version on Windows. Here are the pre-built Shapely wheels for Windows . Choosing your installation type It is recommended to install to a virtual environment in order to avoid dependency version conflicts. Global pip installation Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pip install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pip install Keras==2.2.4 pip install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl pip install musket_ml Virtual environment installation (recommended) virtualenv installation (recommended) This type of installation uses virtualenv manager for creating your virtual environment. Create a new virtual environment: virtualenv ./musket This will create musket folder and place a copy of your python, pip and wheel inside. Activate the new virtual environment: On Posix systems: source ./musket/bin/activate On Windows: .\\musket\\Scripts\\activate Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pip install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pip install Keras==2.2.4 pip install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl Now install musket: pip install musket_ml Experiment launches and other activity should be performed when this environment is activated. When you are done working with musket, you can deactivate the environment by launching: virtualenv deactivate pipenv installation This type of installation uses pipenv manager for creating your virtual environment. Install pipenv if needed: pip install --user pipenv Create new environment by launching: mkdir musket cd musket pipenv --python 3.6 Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pipenv install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pipenv install Keras==2.2.4 pipenv install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl Now install musket: pipenv install musket_ml Experiment launches and other activity should be performed when this environment is activated or using pipenv. So, the first approach is to activate the environment by launching pipenv shell while inside musket folder. Or, alternativelly, prefix all experiment management commands with pipenv run , in example, instead of running musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"D:\\work\\salt\\data\\cache\" run pipenv run musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"D:\\work\\salt\\data\\cache\" Other packages musket_ml joins all mainstream pipelines that belong to Musket-ML framework. In particular, besides musket_core generic pipeline, it includes classification_pipeline classification pipeline , segmentation_pipeline segmentation pipeline and musket_text text support . To install only the generic pipeline, follow the same instructions, but use musket_core wheel instead of musket_ml . Project structure Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. all modules in this folder will be always executed, other modules require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc. Launching Launching experiments fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference Launching tasks task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference Launching project analysis analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference General train properties Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files that are not located in modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Files from the modules folder are imported automatically testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties Definining networks Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment. Built-in NN layers There are a lot of built-in NN layers, basically, we support all layers that are supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types Control layers Utility layers can be used to set control and data flow inside their bodies. Here are some examples: Simple Data Flow constructions inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1] Repeat and With declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120] Conditional layers declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid] Shared Weights #Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations Wrapper layers net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid] Manually controlling data flow net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here Datasets Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities. Callbacks Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here Stages Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Advanced learning rates Dynamic learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly. LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Preprocessors Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min]) How to check training results In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"User guide"},{"location":"generic/#generic-pipeline","text":"","title":"Generic pipeline"},{"location":"generic/#reasons-to-use-generic-pipeline","text":"Generic Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. It provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions Allows to define custom neural networks in a declarative style, by building it from blocks Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library All experiments are declared in YAML dialect with lots of defaults, allowing to describe an initial experiment in several lines and then set more details if needed. Here is a relatively complex example, most of the statements can be omitted: imports: [ layers, preprocessors ] declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] preprocess: - rescale: 10 - get_delta_from_average - cache preprocessing: preprocess testSplit: 0.4 architecture: net optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs","title":"Reasons to use Generic Pipeline"},{"location":"generic/#installation","text":"","title":"Installation"},{"location":"generic/#prerequisites","text":"The package has many prerequisites, but some of them are recommended to be installed manually. Tensorflow package of versions of 1.14 and below split into CPU and GPU ones. Moreover, Tensorflow may be more or less compatible with the version of CUDA/CUDNN installed. Here is the repository containing lots of pre-built Tensorflow wheels for Windows: tensorflow-windows-wheel . It can be used to choose the wheel depending on system architecture, CUDA/CUDNN version, CPU/GPU and Python version. Read more in Tensorflow installation guide . Keras has no strong dependency on Tensorflow, but in our common setup they run in pair. We used the 2.2.4 one. Shapely requires compilation on install on Linux/MacOS or pre-built version on Windows. Here are the pre-built Shapely wheels for Windows .","title":"Prerequisites"},{"location":"generic/#choosing-your-installation-type","text":"It is recommended to install to a virtual environment in order to avoid dependency version conflicts.","title":"Choosing your installation type"},{"location":"generic/#global-pip-installation","text":"Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pip install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pip install Keras==2.2.4 pip install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl pip install musket_ml","title":"Global pip installation"},{"location":"generic/#virtual-environment-installation-recommended","text":"","title":"Virtual environment installation (recommended)"},{"location":"generic/#virtualenv-installation-recommended","text":"This type of installation uses virtualenv manager for creating your virtual environment. Create a new virtual environment: virtualenv ./musket This will create musket folder and place a copy of your python, pip and wheel inside. Activate the new virtual environment: On Posix systems: source ./musket/bin/activate On Windows: .\\musket\\Scripts\\activate Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pip install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pip install Keras==2.2.4 pip install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl Now install musket: pip install musket_ml Experiment launches and other activity should be performed when this environment is activated. When you are done working with musket, you can deactivate the environment by launching: virtualenv deactivate","title":"virtualenv installation (recommended)"},{"location":"generic/#pipenv-installation","text":"This type of installation uses pipenv manager for creating your virtual environment. Install pipenv if needed: pip install --user pipenv Create new environment by launching: mkdir musket cd musket pipenv --python 3.6 Install Tensorflow, Keras and Shapely as described in pre-requisites. In example, if you have downloaded particular Tensorflow wheel into C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl and particular Shapely wheel into C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl , run: pipenv install C:\\downloads\\tensorflow_gpu-1.12.0-cp36-cp36m-win_amd64.whl pipenv install Keras==2.2.4 pipenv install C:\\downloads\\Shapely-1.6.4.post1-cp36-cp36m-win_amd64.whl Now install musket: pipenv install musket_ml Experiment launches and other activity should be performed when this environment is activated or using pipenv. So, the first approach is to activate the environment by launching pipenv shell while inside musket folder. Or, alternativelly, prefix all experiment management commands with pipenv run , in example, instead of running musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"D:\\work\\salt\\data\\cache\" run pipenv run musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"D:\\work\\salt\\data\\cache\"","title":"pipenv installation"},{"location":"generic/#other-packages","text":"musket_ml joins all mainstream pipelines that belong to Musket-ML framework. In particular, besides musket_core generic pipeline, it includes classification_pipeline classification pipeline , segmentation_pipeline segmentation pipeline and musket_text text support . To install only the generic pipeline, follow the same instructions, but use musket_core wheel instead of musket_ml .","title":"Other packages"},{"location":"generic/#project-structure","text":"Each experiment is simply a folder with YAML file inside, it is easy to store and run experiment. Project is a folder with the following structure inside: project_name experiments experiment1 config.yaml experiment2 config.yaml summary.yaml metrics metrics-0.0.csv metrics-1.0.csv metrics-2.0.csv metrics-3.0.csv metrics-4.0.csv modules main.py arbitrary_module.py common.yaml The only required part is experiments folder with at least one arbitrary-named experiment subfolder having config.yaml file inside. Each experiment starts with its configuration, other files are being added by the pipeline during th training. common.yaml file may be added to set instructions, which will be applied to all project experiments. modules folder may be added to set python files in project scope, so custom yaml declarations can be mapped onto python classes and functions defined inside such files. all modules in this folder will be always executed, other modules require imports instruction. summary.yaml and metrics folders inside each experiment appear after the experiment training is executed. There are more potential files, like intermediate results cache files etc.","title":"Project structure"},{"location":"generic/#launching","text":"","title":"Launching"},{"location":"generic/#launching-experiments","text":"fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference","title":"Launching experiments"},{"location":"generic/#launching-tasks","text":"task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference","title":"Launching tasks"},{"location":"generic/#launching-project-analysis","text":"analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference","title":"Launching project analysis"},{"location":"generic/#general-train-properties","text":"Lets take our standard example and check the following set of instructions: imports: [ layers, preprocessors ] testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 12 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - matthews_correlation primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss imports imports python files that are not located in modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Files from the modules folder are imported automatically testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties","title":"General train properties"},{"location":"generic/#definining-networks","text":"Lets check the next part of our example: declarations: collapseConv: parameters: [ filters,size, pool] body: - conv1d: [filters,size,relu ] - conv1d: [filters,size,relu ] - batchNormalization: {} - collapse: pool net: - repeat(2): - collapseConv: [ 20, 7, 10 ] - cudnnlstm: [40, true ] - cudnnlstm: [40, true ] - attention: 718 - dense: [3, sigmoid] architecture: net Here, declarations instruction set up network blocks collapseConv and net . collapseConv block defines its input parameters (those are YAML-level parameters, not actual network tensors), and body defines the sub-blocks of the block. net block has no parameters, so its sub-blocks come right inside the net . Following are built-in layers used inside both blocks: conv1d batchNormalization cudnnlstm attention dense And data / control-flow instructions: collapse repeat Also, net block uses collapseConv block by stating collapseConv: [ 20, 7, 10 ] , where collapseConv ordered parameters [ 20, 7, 10 ] come in YAML array. architecture instruction sets net block as the entry point for the whole experiment.","title":"Definining networks"},{"location":"generic/#built-in-nn-layers","text":"There are a lot of built-in NN layers, basically, we support all layers that are supported by Keras. Here are just a few: Dropout LSTM GlobalMaxPool1D BatchNormalization Concatenate Conv2D Dense More can be found here: Layer types","title":"Built-in NN layers"},{"location":"generic/#control-layers","text":"Utility layers can be used to set control and data flow inside their bodies. Here are some examples:","title":"Control layers"},{"location":"generic/#simple-data-flow-constructions","text":"inceptionBlock: parameters: [channels] with: padding: same body: - split-concatenate: - Conv2D: [channels,1] - seq: - Conv2D: [channels*3,1] - Conv2D: [channels,3] - seq: - Conv2D: [channels*4,1] - Conv2D: [channels,1] - seq: - Conv2D: [channels,2] - Conv2D: [channels,1]","title":"Simple Data Flow constructions"},{"location":"generic/#repeat-and-with","text":"declarations: convBlock: parameters: [channels] with: padding: same body: - repeat(5): - Conv2D: [channels*_,1] net: - convBlock: [120]","title":"Repeat and With"},{"location":"generic/#conditional-layers","text":"declarations: c2d: parameters: [size, pool,mp] body: - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - Conv1D: [100,size,relu] - if(mp): MaxPool1D: pool net: - c2d: [4,4,False] - c2d: [4,4,True] - Dense: [4, sigmoid]","title":"Conditional layers"},{"location":"generic/#shared-weights","text":"#Basic example with sequencial model declarations: convBlock: parameters: [channels] shared: true with: padding: same body: - Conv2D: [channels,1] - Conv2D: [channels,1] net: - convBlock: [3] #weights of convBlock will be shared between invocations - convBlock: [3] #weights of convBlock will be shared between invocations","title":"Shared Weights"},{"location":"generic/#wrapper-layers","text":"net: #- gaussianNoise: 0.0001 #- collapseConv: [ 20, 7, 10 ] #- collapseConv: [ 20, 7, 10 ] - bidirectional: - cudnnlstm: [30, true ] - bidirectional: - cudnnlstm: [50, true ] - attention: 200 - dense: [64, relu] - dense: [3, sigmoid]","title":"Wrapper layers"},{"location":"generic/#manually-controlling-data-flow","text":"net: inputs: [i1,i2] outputs: [d1,d2] body: - c2d: args: [4,4] name: o1 inputs: i1 - c2d: args: [4,4] name: o2 inputs: i2 - dense: units: 4 activation: sigmoid inputs: o1 name: d1 - dense: units: 4 activation: sigmoid inputs: o2 name: d2 Full list can be found here","title":"Manually controlling data flow"},{"location":"generic/#datasets","text":"Datasets allow to define the ways to load data for this particular project. As this pipeline is designed to support an arbitrary data, the only way to add dataset is to put in some custom python code and then refer it from YAML: class DischargeData(datasets.DataSet): def __init__(self,ids,normalize=True, flatten=False): self.normalize=normalize self.flatten = flatten self.cache={} self.ids=list(set(list(ids))) def __getitem__(self, item): item=self.ids[item] if item in self.cache: return self.cache[item] ps= PredictionItem(item,getX(item,self.normalize),getY(item,self.flatten)) #self.cache[item]=ps return ps def __len__(self): return len(self.ids) def getTrain(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(ids,normalize,flatten) def getTest(normalize=True,flatten=False)->datasets.DataSet: return DischargeData(test_ids,normalize,flatten) Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [false,false] datasets: test: getTest: [false,false] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities.","title":"Datasets"},{"location":"generic/#callbacks","text":"Lets check the following block from out main example: callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 8 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 We set up two callback, which are being invoked during the training time: EarlyStopping that monitors metrics and stops training if results doesnt get better, and val_binary_accuracy and ReduceLROnPlateau , which reduces learning rate for the same reason. The list of callbacks can be found here","title":"Callbacks"},{"location":"generic/#stages","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"Stages"},{"location":"generic/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"generic/#advanced-learning-rates","text":"","title":"Advanced learning rates"},{"location":"generic/#dynamic-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly.","title":"Dynamic learning rates"},{"location":"generic/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"generic/#preprocessors","text":"Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocess instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. preprocess: - rescale: 10 - get_delta_from_average - disk-cache import numpy as np from musket_core import preprocessing def moving_average(input, n=1000) : ret = np.cumsum(input, dtype=float, axis=0) ret[n:] = ret[n:] - ret[:-n] ret[0:n] = ret[-n:] return ret / n @preprocessing.dataset_preprocessor def get_delta_from_average(input): m = moving_average(input[:, :]) m1 = moving_average(input[:, :],100) #m2 = moving_average(input[:, :], 10000) d = input[:, :] - m d1 = input[:, :] - m1 #d2 = input[:, :] - m2 input=input/input.max() d1 = d1 / d1.max() # d2 = d2 / d2.max() d = d / d.max() return np.concatenate([d,d1,input]) @preprocessing.dataset_preprocessor def rescale(input,size): mean=np.mean(np.reshape(input, (input.shape[0] // size ,size, 3)), axis=1) max=np.max(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) min = np.min(np.reshape(input, (input.shape[0] // size, size, 3)), axis=1) return np.concatenate([mean,max,min])","title":"Preprocessors"},{"location":"generic/#how-to-check-training-results","text":"In experiment folder metrics subfolder contain a CSV report file for each fold and stage. summary.yaml file in the experiment folder contain the statistics for the whole experiment.","title":"How to check training results"},{"location":"generic/getting_started/","text":"Requirements TODO Getting dataset from Kaggle Installing kaggle stuff This should be done only once. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles. Downloading TGS Salt competition dataset Go to TGS Salt Identification competition and Accept the rules on the Rules tab. Make salt folder somewhere and create dataset subdirectory. Open console with salt/dataset folder as current and invoke kaggle competitions download -c tgs-salt-identification-challenge command. This will download dataset files. Then invoke unzip train.zip -d train to unzip train.zip files in to train folder. Running the experiment musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=2 --gpus_per_net=2 --num_workers=1 --cache \"D:\\work\\salt\\data\"","title":"Getting started"},{"location":"generic/getting_started/#requirements","text":"TODO","title":"Requirements"},{"location":"generic/getting_started/#getting-dataset-from-kaggle","text":"","title":"Getting dataset from Kaggle"},{"location":"generic/getting_started/#installing-kaggle-stuff","text":"This should be done only once. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles.","title":"Installing kaggle stuff"},{"location":"generic/getting_started/#downloading-tgs-salt-competition-dataset","text":"Go to TGS Salt Identification competition and Accept the rules on the Rules tab. Make salt folder somewhere and create dataset subdirectory. Open console with salt/dataset folder as current and invoke kaggle competitions download -c tgs-salt-identification-challenge command. This will download dataset files. Then invoke unzip train.zip -d train to unzip train.zip files in to train folder.","title":"Downloading TGS Salt competition dataset"},{"location":"generic/getting_started/#running-the-experiment","text":"musket fit --project \"D:\\work\\salt\" --name \"exp01\" --num_gpus=2 --gpus_per_net=2 --num_workers=1 --cache \"D:\\work\\salt\\data\"","title":"Running the experiment"},{"location":"generic/reference/","text":"Generic pipeline reference Pipeline root properties experiment_result type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout architecture type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork batch type : integer Sets up training batch size. Example: batch: 512 callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] declarations type : complex Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"] extra_train_data type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example: extra_train_data: more_people folds_count type : integer Number of folds to train. Default is 5. Example: folds_count: 3 final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : integer Size of batch during inferring process. Example: loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: testTimeAugmentation type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example: validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. This property is only used if fold count is 1. Example: Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] absSize : - size in batches relSize : - size in fractions of epoch periodEpochs : - period in epochs periodSteps : - period in batches then : - LRVariator that should manage learning rate after this Example LRVariator: fromVal: 0 toVal: 0.00005 style: linear relSize: 0.05 # lets go for 1/20 of epoch then: LRVariator: fromVal: 0.00005 toVal: 0 relSize: 2 # lets go for 2 of epochs style: linear TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Layer types Input This layer is not intended to be used directly Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example: GaussianNoise Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example: Dropout Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. seed - integer; integer to use as random seed Example: declarations: net: - dropout: 0.5 SpatialDropout1D Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example: LSTM Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Activation function to use for the recurrent step (see activations ). Default: hard sigmoid ( hard_sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. Example: GlobalMaxPool1D Global max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example: GlobalAveragePooling1D Global average pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example: BatchNormalization Batch normalization layer. Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format=\"channels_first\" , set axis=1 in BatchNormalization . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Example: Concatenate Layer that concatenates a list of inputs. Example: - concatenate: [lstmBranch,textFeatureBranch] Add Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - add: [first,second] Substract ayer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Example: - substract: [first,second] Mult Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - mult: [first,second] Max Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - max: [first,second] Min Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - min: [first,second] Conv1D 1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", or (None, 128) for variable-length sequences with 128 features per step. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: Conv2D 2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: MaxPool1D Max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example: MaxPool2D Max pooling operation for spatial data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example: AveragePooling1D Average pooling for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example: CuDNNLSTM Fast LSTM implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example: Dense Regular densely-connected NN layer. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example: Flatten Flattens the input. Does not affect the batch size. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example: Bidirectional Bidirectional wrapper for RNNs. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Example: Utility layers split Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Number of outputs is equal to a number of children. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-concat Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128] split-concatenate Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows (equal to the usage of Concatenate layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128] split-add Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is an addition of child flows (equal to the usage of Add layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-substract Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a substraction of child flows (equal to the usage of Substract layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-mult Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a multiplication of child flows (equal to the usage of Mult layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-min Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a minimum of child flows (equal to the usage of Min layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-max Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a maximum of child flows (equal to the usage of Max layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: split-dot Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: split-dot-normalize Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product with normalization of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: seq Executes child elements as a sequence of operations, one by one. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input Overrides current input with what is listed. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input: [firstRef, secondRef] pass Forwards data from this branch Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: transform-concat: - pass - Conv1D: [10,1,\"relu\"] transform-concat passes input tensors through layers, and then concatenates outputs Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-concat: - Conv1D: [10,1,\"relu\"] - Conv1D: [10,2,\"relu\"] transform-add passes input tensors through layers, and then adds outputs Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-add: - Conv1D: [10,1,\"relu\"] - Conv1D: [10,2,\"relu\"] Stage properties callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 epochs type : integer Number of epochs to train for this stage. Example: extra_callbacks Allows to specify a list of additional callbacks that should be applied to this stage initial_weights type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights negatives type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: lr: 0.01 validation_negatives type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. Caches its input in memory, including the full flow. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - cache: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example: fit script arguments fit.py project type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\" fit.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\" fit.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1 fit.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1 fit.py num_workers type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1 fit.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True fit.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True fit.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True fit.py only_report type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True fit.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\" fit.py folds type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2 task script arguments task.py project type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\" task.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\" task.py task type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\" task.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1 task.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1 task.py num_workers type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1 task.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True task.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True task.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True task.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\" analyze script arguments analyze.py inputFolder type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\" analyze.py output type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\" analyze.py onlyMetric type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\" analyze.py sortBy type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"Reference"},{"location":"generic/reference/#generic-pipeline-reference","text":"","title":"Generic pipeline reference"},{"location":"generic/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"generic/reference/#experiment_result","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout","title":"experiment_result"},{"location":"generic/reference/#architecture","text":"type : string Name of the declaration that will be used as an entry point or root of the main network. Example: declarations: utilityDeclaration1: utilityDeclaration2: mainNetwork: - utilityDeclaration1: [] - dense: [1,\"sigmoid\"] architecture: mainNetwork","title":"architecture"},{"location":"generic/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 512","title":"batch"},{"location":"generic/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"generic/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"generic/reference/#clipnorm","text":"type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"generic/reference/#clipvalue","text":"type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"generic/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"generic/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"generic/reference/#declarations","text":"type : complex Sets up network layer building blocks. Each declaration is an object with a key setting up declaration name and value being a complex object containing parameters array listing this layer parameters and body containing an array of sub-layers or control statements, If layer has no parameters, parameters property may be ommitted and body contents may come directly inside layer definition. See Layer types for details regarding building blocks. Example: declarations: lstm2: parameters: [count] body: - bidirectional: - cuDNNLSTM: [count, true] - bidirectional: - cuDNNLSTM: [count/2, false] net: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - gaussianNoise: 0.05 - lstm2: [300] #- dropout: 0.5 - dense: [1,\"sigmoid\"]","title":"declarations"},{"location":"generic/reference/#extra_train_data","text":"type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example: extra_train_data: more_people","title":"extra_train_data"},{"location":"generic/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example: folds_count: 3","title":"folds_count"},{"location":"generic/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"generic/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"generic/reference/#inference_batch","text":"type : integer Size of batch during inferring process. Example:","title":"inference_batch"},{"location":"generic/reference/#loss","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"generic/reference/#lr","text":"type : float Learning rate. Example:","title":"lr"},{"location":"generic/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"generic/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"generic/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"generic/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"generic/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"generic/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"generic/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"generic/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"generic/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"generic/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"generic/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"generic/reference/#testtimeaugmentation","text":"type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example:","title":"testTimeAugmentation"},{"location":"generic/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. This property is only used if fold count is 1. Example:","title":"validationSplit"},{"location":"generic/reference/#callback-types","text":"","title":"Callback types"},{"location":"generic/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"generic/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"generic/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"generic/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] absSize : - size in batches relSize : - size in fractions of epoch periodEpochs : - period in epochs periodSteps : - period in batches then : - LRVariator that should manage learning rate after this Example LRVariator: fromVal: 0 toVal: 0.00005 style: linear relSize: 0.05 # lets go for 1/20 of epoch then: LRVariator: fromVal: 0.00005 toVal: 0 relSize: 2 # lets go for 2 of epochs style: linear","title":"LRVariator"},{"location":"generic/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"generic/reference/#layer-types","text":"","title":"Layer types"},{"location":"generic/reference/#input","text":"This layer is not intended to be used directly Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. shape - array of integers; input shape Example:","title":"Input"},{"location":"generic/reference/#gaussiannoise","text":"Apply additive zero-centered Gaussian noise. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. stddev - float; standard deviation of the noise distribution. Example:","title":"GaussianNoise"},{"location":"generic/reference/#dropout","text":"Applies Dropout to the input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float; float between 0 and 1. Fraction of the input units to drop. seed - integer; integer to use as random seed Example: declarations: net: - dropout: 0.5","title":"Dropout"},{"location":"generic/reference/#spatialdropout1d","text":"Spatial 1D version of Dropout. This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. rate - float between 0 and 1. Fraction of the input units to drop. Example:","title":"SpatialDropout1D"},{"location":"generic/reference/#lstm","text":"Long Short-Term Memory layer Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). Default: hyperbolic tangent ( tanh ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). recurrent_activation : Activation function to use for the recurrent step (see activations ). Default: hard sigmoid ( hard_sigmoid ). If you pass None , no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs. recurrent_dropout : Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state. implementation : Implementation mode, either 1 or 2. Mode 1 will structure its operations as a larger number of smaller dot products and additions, whereas mode 2 will batch them into fewer, larger operations. These modes will have different performance profiles on different hardware and for different applications. return_sequences : Boolean. Whether to return the last output in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. The returned elements of the states list are the hidden state and the cell state, respectively. go_backwards : Boolean (default False). If True, process the input sequence backwards and return the reversed sequence. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. unroll : Boolean (default False). If True, the network will be unrolled, else a symbolic loop will be used. Unrolling can speed-up a RNN, although it tends to be more memory-intensive. Unrolling is only suitable for short sequences. Example:","title":"LSTM"},{"location":"generic/reference/#globalmaxpool1d","text":"Global max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example:","title":"GlobalMaxPool1D"},{"location":"generic/reference/#globalaveragepooling1d","text":"Global average pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps). Example:","title":"GlobalAveragePooling1D"},{"location":"generic/reference/#batchnormalization","text":"Batch normalization layer. Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. axis : Integer, the axis that should be normalized (typically the features axis). For instance, after a Conv2D layer with data_format=\"channels_first\" , set axis=1 in BatchNormalization . momentum : Momentum for the moving mean and the moving variance. epsilon : Small float added to variance to avoid dividing by zero. center : If True, add offset of beta to normalized tensor. If False, beta is ignored. scale : If True, multiply by gamma . If False, gamma is not used. When the next layer is linear (also e.g. nn.relu ), this can be disabled since the scaling will be done by the next layer. beta_initializer : Initializer for the beta weight. gamma_initializer : Initializer for the gamma weight. moving_mean_initializer : Initializer for the moving mean. moving_variance_initializer : Initializer for the moving variance. beta_regularizer : Optional regularizer for the beta weight. gamma_regularizer : Optional regularizer for the gamma weight. beta_constraint : Optional constraint for the beta weight. gamma_constraint : Optional constraint for the gamma weight. Example:","title":"BatchNormalization"},{"location":"generic/reference/#concatenate","text":"Layer that concatenates a list of inputs. Example: - concatenate: [lstmBranch,textFeatureBranch]","title":"Concatenate"},{"location":"generic/reference/#add","text":"Layer that adds a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - add: [first,second]","title":"Add"},{"location":"generic/reference/#substract","text":"ayer that subtracts two inputs. It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape. Example: - substract: [first,second]","title":"Substract"},{"location":"generic/reference/#mult","text":"Layer that multiplies (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - mult: [first,second]","title":"Mult"},{"location":"generic/reference/#max","text":"Layer that computes the maximum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - max: [first,second]","title":"Max"},{"location":"generic/reference/#min","text":"Layer that computes the minimum (element-wise) a list of inputs. It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape). Example: - min: [first,second]","title":"Min"},{"location":"generic/reference/#conv1d","text":"1D convolution layer (e.g. temporal convolution). This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format=\"channels_last\", or (None, 128) for variable-length sequences with 128 features per step. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of a single integer, specifying the length of the 1D convolution window. strides : An integer or tuple/list of a single integer, specifying the stride length of the convolution. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : One of \"valid\" , \"causal\" or \"same\" (case-insensitive). \"valid\" means \"no padding\". \"same\" results in padding the input such that the output has the same length as the original input. \"causal\" results in causal (dilated) convolutions, e.g. output[t] does not depend on input[t + 1:] . A zero padding is used such that the output has the same length as the original input. Useful when modeling temporal data where the model should not violate the temporal order. See WaveNet: A Generative Model for Raw Audio, section 2.1 . data_format : A string, one of \"channels_last\" (default) or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, steps, channels) (default format for temporal data in Keras) while \"channels_first\" corresponds to inputs with shape (batch, channels, steps) . dilation_rate : an integer or tuple/list of a single integer, specifying the dilation rate to use for dilated convolution. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any strides value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Conv1D"},{"location":"generic/reference/#conv2d","text":"2D convolution layer (e.g. spatial convolution over images). This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well. When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format=\"channels_last\". Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. filters : Integer, the dimensionality of the output space (i.e. the number of output filters in the convolution). kernel_size : An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. Can be a single integer to specify the same value for all spatial dimensions. strides : An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width. Can be a single integer to specify the same value for all spatial dimensions. Specifying any stride value != 1 is incompatible with specifying any dilation_rate value != 1. padding : one of \"valid\" or \"same\" (case-insensitive). Note that \"same\" is slightly inconsistent across backends with strides != 1, as described here data_format : A string, one of \"channels_last\" or \"channels_first\" . The ordering of the dimensions in the inputs. \"channels_last\" corresponds to inputs with shape (batch, height, width, channels) while \"channels_first\" corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". dilation_rate : an integer or tuple/list of 2 integers, specifying the dilation rate to use for dilated convolution. Can be a single integer to specify the same value for all spatial dimensions. Currently, specifying any dilation_rate value != 1 is incompatible with specifying any stride value != 1. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Conv2D"},{"location":"generic/reference/#maxpool1d","text":"Max pooling operation for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the max pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example:","title":"MaxPool1D"},{"location":"generic/reference/#maxpool2d","text":"Max pooling operation for spatial data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal). (2, 2) will halve the input in both spatial dimension. If only one integer is specified, the same window length will be used for both dimensions. strides : Integer, tuple of 2 integers, or None. Strides values. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, height, width, channels) while channels_first corresponds to inputs with shape (batch, channels, height, width) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example:","title":"MaxPool2D"},{"location":"generic/reference/#averagepooling1d","text":"Average pooling for temporal data. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. pool_size : Integer, size of the average pooling windows. strides : Integer, or None. Factor by which to downscale. E.g. 2 will halve the input. If None, it will default to pool_size . padding : One of \"valid\" or \"same\" (case-insensitive). data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch, steps, features) while channels_first corresponds to inputs with shape (batch, features, steps) . Example:","title":"AveragePooling1D"},{"location":"generic/reference/#cudnnlstm","text":"Fast LSTM implementation with CuDNN . Can only be run on GPU, with the TensorFlow backend. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. kernel_initializer : Initializer for the kernel weights matrix, used for the linear transformation of the inputs. (see initializers ). recurrent_initializer : Initializer for the recurrent_kernel weights matrix, used for the linear transformation of the recurrent state. (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). unit_forget_bias : Boolean. If True, add 1 to the bias of the forget gate at initialization. Setting it to true will also force bias_initializer=\"zeros\" . This is recommended in Jozefowicz et al. (2015) . kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). recurrent_regularizer : Regularizer function applied to the recurrent_kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). recurrent_constraint : Constraint function applied to the recurrent_kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). return_sequences : Boolean. Whether to return the last output. in the output sequence, or the full sequence. return_state : Boolean. Whether to return the last state in addition to the output. stateful : Boolean (default False). If True, the last state for each sample at index i in a batch will be used as initial state for the sample of index i in the following batch. Example:","title":"CuDNNLSTM"},{"location":"generic/reference/#dense","text":"Regular densely-connected NN layer. Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True ). Note: if the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel . Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. units : Positive integer, dimensionality of the output space. activation : Activation function to use (see activations ). If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x ). use_bias : Boolean, whether the layer uses a bias vector. kernel_initializer : Initializer for the kernel weights matrix (see initializers ). bias_initializer : Initializer for the bias vector (see initializers ). kernel_regularizer : Regularizer function applied to the kernel weights matrix (see regularizer ). bias_regularizer : Regularizer function applied to the bias vector (see regularizer ). activity_regularizer : Regularizer function applied to the output of the layer (its \"activation\"). (see regularizer ). kernel_constraint : Constraint function applied to the kernel weights matrix (see constraints ). bias_constraint : Constraint function applied to the bias vector (see constraints ). Example:","title":"Dense"},{"location":"generic/reference/#flatten","text":"Flattens the input. Does not affect the batch size. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. data_format : A string, one of channels_last (default) or channels_first . The ordering of the dimensions in the inputs. The purpose of this argument is to preserve weight ordering when switching a model from one data format to another. channels_last corresponds to inputs with shape (batch, ..., channels) while channels_first corresponds to inputs with shape (batch, channels, ...) . It defaults to the image_data_format value found in your Keras config file at ~/.keras/keras.json . If you never set it, then it will be \"channels_last\". Example:","title":"Flatten"},{"location":"generic/reference/#bidirectional","text":"Bidirectional wrapper for RNNs. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. layer : Recurrent instance. merge_mode : Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list. weights : Initial weights to load in the Bidirectional model Example:","title":"Bidirectional"},{"location":"generic/reference/#utility-layers","text":"","title":"Utility layers"},{"location":"generic/reference/#split","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Number of outputs is equal to a number of children. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split"},{"location":"generic/reference/#split-concat","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128]","title":"split-concat"},{"location":"generic/reference/#split-concatenate","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a concatenation of child flows (equal to the usage of Concatenate layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: - split-concat: - word_indexes_embedding: [ embeddings/glove.840B.300d.txt ] - word_indexes_embedding: [ embeddings/paragram_300_sl999.txt ] - word_indexes_embedding: [ embeddings/wiki-news-300d-1M.vec] - lstm2: [128]","title":"split-concatenate"},{"location":"generic/reference/#split-add","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is an addition of child flows (equal to the usage of Add layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-add"},{"location":"generic/reference/#split-substract","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a substraction of child flows (equal to the usage of Substract layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-substract"},{"location":"generic/reference/#split-mult","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a multiplication of child flows (equal to the usage of Mult layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-mult"},{"location":"generic/reference/#split-min","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a minimum of child flows (equal to the usage of Min layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-min"},{"location":"generic/reference/#split-max","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a maximum of child flows (equal to the usage of Max layer). Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example:","title":"split-max"},{"location":"generic/reference/#split-dot","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-dot"},{"location":"generic/reference/#split-dot-normalize","text":"Splits current flow into several ones. Each child is a separate flow with an input equal to the input of the split operation. Output is a dot product with normalization of child flows. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"split-dot-normalize"},{"location":"generic/reference/#seq","text":"Executes child elements as a sequence of operations, one by one. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"seq"},{"location":"generic/reference/#input_1","text":"Overrides current input with what is listed. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: input: [firstRef, secondRef]","title":"input"},{"location":"generic/reference/#pass","text":"Forwards data from this branch Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: transform-concat: - pass - Conv1D: [10,1,\"relu\"]","title":"pass"},{"location":"generic/reference/#transform-concat","text":"passes input tensors through layers, and then concatenates outputs Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-concat: - Conv1D: [10,1,\"relu\"] - Conv1D: [10,2,\"relu\"]","title":"transform-concat"},{"location":"generic/reference/#transform-add","text":"passes input tensors through layers, and then adds outputs Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. **** - Example: transform-add: - Conv1D: [10,1,\"relu\"] - Conv1D: [10,2,\"relu\"]","title":"transform-add"},{"location":"generic/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"generic/reference/#callbacks_1","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"generic/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"generic/reference/#extra_callbacks","text":"Allows to specify a list of additional callbacks that should be applied to this stage","title":"extra_callbacks"},{"location":"generic/reference/#initial_weights","text":"type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights","title":"initial_weights"},{"location":"generic/reference/#negatives","text":"type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"negatives"},{"location":"generic/reference/#loss_1","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"generic/reference/#lr_1","text":"type : float Learning rate. Example: lr: 0.01","title":"lr"},{"location":"generic/reference/#validation_negatives","text":"type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"validation_negatives"},{"location":"generic/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"generic/reference/#cache","text":"Caches its input. Caches its input in memory, including the full flow. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - cache:","title":"cache"},{"location":"generic/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"generic/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"generic/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"generic/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"generic/reference/#augmentation","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. Example:","title":"augmentation"},{"location":"generic/reference/#fit-script-arguments","text":"","title":"fit script arguments"},{"location":"generic/reference/#fitpy-project","text":"type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\"","title":"fit.py project"},{"location":"generic/reference/#fitpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\"","title":"fit.py name"},{"location":"generic/reference/#fitpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1","title":"fit.py num_gpus"},{"location":"generic/reference/#fitpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1","title":"fit.py gpus_per_net"},{"location":"generic/reference/#fitpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1","title":"fit.py num_workers"},{"location":"generic/reference/#fitpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True","title":"fit.py allow_resume"},{"location":"generic/reference/#fitpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True","title":"fit.py force_recalc"},{"location":"generic/reference/#fitpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True","title":"fit.py launch_tasks"},{"location":"generic/reference/#fitpy-only_report","text":"type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True","title":"fit.py only_report"},{"location":"generic/reference/#fitpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\"","title":"fit.py cache"},{"location":"generic/reference/#fitpy-folds","text":"type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2","title":"fit.py folds"},{"location":"generic/reference/#task-script-arguments","text":"","title":"task script arguments"},{"location":"generic/reference/#taskpy-project","text":"type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\"","title":"task.py project"},{"location":"generic/reference/#taskpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\"","title":"task.py name"},{"location":"generic/reference/#taskpy-task","text":"type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\"","title":"task.py task"},{"location":"generic/reference/#taskpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1","title":"task.py num_gpus"},{"location":"generic/reference/#taskpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1","title":"task.py gpus_per_net"},{"location":"generic/reference/#taskpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1","title":"task.py num_workers"},{"location":"generic/reference/#taskpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True","title":"task.py allow_resume"},{"location":"generic/reference/#taskpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True","title":"task.py force_recalc"},{"location":"generic/reference/#taskpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True","title":"task.py launch_tasks"},{"location":"generic/reference/#taskpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\"","title":"task.py cache"},{"location":"generic/reference/#analyze-script-arguments","text":"","title":"analyze script arguments"},{"location":"generic/reference/#analyzepy-inputfolder","text":"type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\"","title":"analyze.py inputFolder"},{"location":"generic/reference/#analyzepy-output","text":"type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\"","title":"analyze.py output"},{"location":"generic/reference/#analyzepy-onlymetric","text":"type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\"","title":"analyze.py onlyMetric"},{"location":"generic/reference/#analyzepy-sortby","text":"type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"analyze.py sortBy"},{"location":"ide/getting_started/","text":"Getting started Download Download Musket IDE for Windows Download Musket IDE for Linux Nightly builds: We publish nightly build to the Source forge Installation Install all of Musket ML Python packages. Read installation instructions here . Note: if only Generic pipeline package of Musket ML is installed, you way also consider to Install Segmentation Pipeline , Classification Pipeline and Musket Text . Unzip and launch executable ( ds-ide.exe for Windows and ds-ide for MacOS). Watch this in action Most of the following contents can be also checked in action in this video . Setting up a project Launch File->New... from the main menu. Choose Musket->Musket Project in the New dialog. Click Next , enter new project name and click Please configure an interpreter... to set up python interpreter. Try Config first in PATH option first, if it fail o auto-detect you python, use other options. Click Finish and accept opening the new perspective. This will create the project structure for you, details regarding the structure can be found here . Getting dataset from Kaggle Installing kaggle stuff This should be done only once, first time you are getting something from Kaggle. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles. Downloading the dataset Back to IDE. Select your new project root in project explorer, right-click and select New->Other ... Choose Musket->Kaggle Dataset . Click Next and select competition , enter salt into a search box and click Search . Select tgs-salt-identification-chellenge and click Finish . This will start dataset download, check its progress in console. Analysing dataset In project explorer, double-click on the newly downloaded train.csv Dataset editor displays CSV contents statistics, if the data is recognizable. Analyzers combo allows to switch between dataset analysis modes. In this simple binary segmentation there is an only suitable analyzer, but in other uses cases there are more. So, the statistics on the screenshot is the output of the analyzer. Besides that, analyzer also splits and sorts data, and we can see that on data tab. Visualizers are used to display that data, in our case, as we got raw images, there is an only visualizer. Masks are also displayed. Creating an experiment Following should be done to make an experiment from the dataset: Click the Generate button in the toolbar in the top-right corner of the viewer. Choose a name like train_ds and Generate Musket Wrappers , click Ok . Accept to configure an experiment, enter any name like exp01 and click Finish . You now have exp01 folder inside experiments folder and config.yaml file there. datasets.py is also generated inside modules folder. Editing an experiment We've got a default experiment for binary segmentation generated in the previous chapter. You can always find it in project explorer: Its contents can be edited by double-clicking on the experiment. Lets make some minor changes to experiment code. Change the shape to be well eaten by the network to: Change shape instruction for shape: [224,224, 3] Lets reduce the count of folds from default 5 to 3 to speed things up: Add folds_count: 3 instruction to the root level. Add some holdout so we can track the effectiveness of the trained experiment: Add testSplit: 0.2 instruction to the root level. And add an instructions to dump validation and holdout predictions to CSV right after the training. Add dumpPredictionsToCSV: true instruction to the root level. Running an experiment Click on Launch experiment in the tool bar of the editor. This should launch an experiment, you can track what's going on in the console. Checking results and logs When the experiment is finished, overal statistics should appear in the Results and logs tab: It must also generate summary.yaml file in the experiment folder and metrics*.csv files in metrics subfolder. Check these files, they contain lots of useful statistics. Logs tab display logs in graphical form. Log combo switches between different logs we got, in our case there is one per fold. We've got following metrics declared in the experiment YAML code: metrics: #we would like to track some metrics - binary_accuracy - dice So the Metric combo lists all of them (for the specific fold) and loss: Checking predictions As we dumped our validation and holdout predictions to CSV by adding dumpPredictionsToCSV: true instruction to the root level of our YAML, now we have a bunch of CSV files in the predictions folder of our experiment. You can use those files directly, our just click the links that appear on the Results and logs / Results tab: This opens up the viewer with visualizers and analyzers that we already seen, this time for particular prediction. Statistics tab, as usual, displays some chart. Check out Analyzers combo, this time there are more of them. Data tab, which grouping is affected by the current analyzer displays samples with mask and prediction. What is next Check the videos to find out what else can be done using Musket IDE: tutorials .","title":"Getting started"},{"location":"ide/getting_started/#getting-started","text":"","title":"Getting started"},{"location":"ide/getting_started/#download","text":"Download Musket IDE for Windows Download Musket IDE for Linux","title":"Download"},{"location":"ide/getting_started/#nightly-builds","text":"We publish nightly build to the Source forge","title":"Nightly builds:"},{"location":"ide/getting_started/#installation","text":"Install all of Musket ML Python packages. Read installation instructions here . Note: if only Generic pipeline package of Musket ML is installed, you way also consider to Install Segmentation Pipeline , Classification Pipeline and Musket Text . Unzip and launch executable ( ds-ide.exe for Windows and ds-ide for MacOS).","title":"Installation"},{"location":"ide/getting_started/#watch-this-in-action","text":"Most of the following contents can be also checked in action in this video .","title":"Watch this in action"},{"location":"ide/getting_started/#setting-up-a-project","text":"Launch File->New... from the main menu. Choose Musket->Musket Project in the New dialog. Click Next , enter new project name and click Please configure an interpreter... to set up python interpreter. Try Config first in PATH option first, if it fail o auto-detect you python, use other options. Click Finish and accept opening the new perspective. This will create the project structure for you, details regarding the structure can be found here .","title":"Setting up a project"},{"location":"ide/getting_started/#getting-dataset-from-kaggle","text":"","title":"Getting dataset from Kaggle"},{"location":"ide/getting_started/#installing-kaggle-stuff","text":"This should be done only once, first time you are getting something from Kaggle. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles.","title":"Installing kaggle stuff"},{"location":"ide/getting_started/#downloading-the-dataset","text":"Back to IDE. Select your new project root in project explorer, right-click and select New->Other ... Choose Musket->Kaggle Dataset . Click Next and select competition , enter salt into a search box and click Search . Select tgs-salt-identification-chellenge and click Finish . This will start dataset download, check its progress in console.","title":"Downloading the dataset"},{"location":"ide/getting_started/#analysing-dataset","text":"In project explorer, double-click on the newly downloaded train.csv Dataset editor displays CSV contents statistics, if the data is recognizable. Analyzers combo allows to switch between dataset analysis modes. In this simple binary segmentation there is an only suitable analyzer, but in other uses cases there are more. So, the statistics on the screenshot is the output of the analyzer. Besides that, analyzer also splits and sorts data, and we can see that on data tab. Visualizers are used to display that data, in our case, as we got raw images, there is an only visualizer. Masks are also displayed.","title":"Analysing dataset"},{"location":"ide/getting_started/#creating-an-experiment","text":"Following should be done to make an experiment from the dataset: Click the Generate button in the toolbar in the top-right corner of the viewer. Choose a name like train_ds and Generate Musket Wrappers , click Ok . Accept to configure an experiment, enter any name like exp01 and click Finish . You now have exp01 folder inside experiments folder and config.yaml file there. datasets.py is also generated inside modules folder.","title":"Creating an experiment"},{"location":"ide/getting_started/#editing-an-experiment","text":"We've got a default experiment for binary segmentation generated in the previous chapter. You can always find it in project explorer: Its contents can be edited by double-clicking on the experiment. Lets make some minor changes to experiment code. Change the shape to be well eaten by the network to: Change shape instruction for shape: [224,224, 3] Lets reduce the count of folds from default 5 to 3 to speed things up: Add folds_count: 3 instruction to the root level. Add some holdout so we can track the effectiveness of the trained experiment: Add testSplit: 0.2 instruction to the root level. And add an instructions to dump validation and holdout predictions to CSV right after the training. Add dumpPredictionsToCSV: true instruction to the root level.","title":"Editing an experiment"},{"location":"ide/getting_started/#running-an-experiment","text":"Click on Launch experiment in the tool bar of the editor. This should launch an experiment, you can track what's going on in the console.","title":"Running an experiment"},{"location":"ide/getting_started/#checking-results-and-logs","text":"When the experiment is finished, overal statistics should appear in the Results and logs tab: It must also generate summary.yaml file in the experiment folder and metrics*.csv files in metrics subfolder. Check these files, they contain lots of useful statistics. Logs tab display logs in graphical form. Log combo switches between different logs we got, in our case there is one per fold. We've got following metrics declared in the experiment YAML code: metrics: #we would like to track some metrics - binary_accuracy - dice So the Metric combo lists all of them (for the specific fold) and loss:","title":"Checking results and logs"},{"location":"ide/getting_started/#checking-predictions","text":"As we dumped our validation and holdout predictions to CSV by adding dumpPredictionsToCSV: true instruction to the root level of our YAML, now we have a bunch of CSV files in the predictions folder of our experiment. You can use those files directly, our just click the links that appear on the Results and logs / Results tab: This opens up the viewer with visualizers and analyzers that we already seen, this time for particular prediction. Statistics tab, as usual, displays some chart. Check out Analyzers combo, this time there are more of them. Data tab, which grouping is affected by the current analyzer displays samples with mask and prediction.","title":"Checking predictions"},{"location":"ide/getting_started/#what-is-next","text":"Check the videos to find out what else can be done using Musket IDE: tutorials .","title":"What is next"},{"location":"instance_segmentation/","text":"Instance Segmentation Training Pipeline Instance Segmentation Pipeline was developed in order to enable using the MMDetection framework by means of Musket ML. This package is not yet completely ready for production use Installation pip install git+https://github.com/musket_ml/instance_segmentation_pipeline Note: this package requires python 3.6 This package is not a part of Musket ML framework yet, Launching Launching experiments fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference Launching tasks task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference Launching project analysis analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference Usage guide Dataset format The pipline must be used with datasets which return prediction items of certain structure. Suppose that our prediction item represents a training example with some image and N objects on it. x must contain image data represented by a numpy array of shape (height, width, 3) . y must be a tuple (labels, bboxes, masks) where labels is a length N one dimansional numpy array of integers which contains object labels. Note that zero label is reserved for background. bboxes is a float array of shape (N, 4) which contains object bounding boxes. Note that bounding box coordinates order must be [minY, minX, maxY, maxX] . masks is an integer or boolean numpy array of shape (N, height, width) which contains object masks. Note that integer mask should contain 1 for object pixels and 0 for background. Training a model Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from musket_core import generic from get_some_gataset import getDataset ds = getDataset #some dataset which has the required format cfg = generic.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. #%Musket MMDetection 1.0 classes: 46 shape: [800, 1333] imagesPerGpu: 1 folds_count: 1 testSplit: 0.1 stages: - epochs: 3 dataset: getTrain2: configPath: '../../data/configs/htc/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e-1.py' weightsPath: '../../data/checkpoints/Hybrid-Task-Cascade-(HTC)/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e_20190408-0e50669c-nohead.pth' General train properties The following property is required to set: configPath path to MMDetection config. The following ones are optional, but commonly used: classes sets the number of classes that should be used. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. weightsPath path to initial weights of the model.","title":"User guide"},{"location":"instance_segmentation/#instance-segmentation-training-pipeline","text":"Instance Segmentation Pipeline was developed in order to enable using the MMDetection framework by means of Musket ML. This package is not yet completely ready for production use","title":"Instance Segmentation Training Pipeline"},{"location":"instance_segmentation/#installation","text":"pip install git+https://github.com/musket_ml/instance_segmentation_pipeline Note: this package requires python 3.6 This package is not a part of Musket ML framework yet,","title":"Installation"},{"location":"instance_segmentation/#launching","text":"","title":"Launching"},{"location":"instance_segmentation/#launching-experiments","text":"fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference","title":"Launching experiments"},{"location":"instance_segmentation/#launching-tasks","text":"task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference","title":"Launching tasks"},{"location":"instance_segmentation/#launching-project-analysis","text":"analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference","title":"Launching project analysis"},{"location":"instance_segmentation/#usage-guide","text":"","title":"Usage guide"},{"location":"instance_segmentation/#dataset-format","text":"The pipline must be used with datasets which return prediction items of certain structure. Suppose that our prediction item represents a training example with some image and N objects on it. x must contain image data represented by a numpy array of shape (height, width, 3) . y must be a tuple (labels, bboxes, masks) where labels is a length N one dimansional numpy array of integers which contains object labels. Note that zero label is reserved for background. bboxes is a float array of shape (N, 4) which contains object bounding boxes. Note that bounding box coordinates order must be [minY, minX, maxY, maxX] . masks is an integer or boolean numpy array of shape (N, height, width) which contains object masks. Note that integer mask should contain 1 for object pixels and 0 for background.","title":"Dataset format"},{"location":"instance_segmentation/#training-a-model","text":"Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from musket_core import generic from get_some_gataset import getDataset ds = getDataset #some dataset which has the required format cfg = generic.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. #%Musket MMDetection 1.0 classes: 46 shape: [800, 1333] imagesPerGpu: 1 folds_count: 1 testSplit: 0.1 stages: - epochs: 3 dataset: getTrain2: configPath: '../../data/configs/htc/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e-1.py' weightsPath: '../../data/checkpoints/Hybrid-Task-Cascade-(HTC)/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e_20190408-0e50669c-nohead.pth'","title":"Training a model"},{"location":"instance_segmentation/#general-train-properties","text":"The following property is required to set: configPath path to MMDetection config. The following ones are optional, but commonly used: classes sets the number of classes that should be used. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. weightsPath path to initial weights of the model.","title":"General train properties"},{"location":"instance_segmentation/reference/","text":"Instance Segmentation pipeline reference Pipeline root properties classes type : integer Number of classes that should be segmented. Example: configPath type : string Path to MMDetection config file. Should be absolute or relative to the musket config file. dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] folds_count type : integer Number of folds to train. Default is 5. Example: holdout type : ```` Example: imagesPerGpu type : integer Number of images in a batch to be processed by single GPU. MMDetection does not allow specifying batch size directly, it only allows setting how much images are processed by each GPU at a time. Thus, batch size is imagesPerGpu multiplied by gpus_per_net . Example: imagesPerGpu: 2 imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py multiscaleMode type : string Can be range or value (default). Setting value to range allows using two dimensional integer arrays as shape values for specifying possible ranges of train shapes. num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: shape type : one or two dimensional array of integers Shape of the model input. All images are automatically scaled to this shape before being processed by the model. The exact meaning of the parameter can be: One dimensional array is simply understood as as [height, width] for train, validation and infering shapes. Two dimensional array is understood as array of shapes. Train shape is chosen randomly from the array for each train sample, and the first shape is always taken on validation and infering. With the multiscaleMode parameter set to range a two element two dimensional array [[h1,w1], [h2,w2]] is understood as possible range for train shapes: train height and width are randomly chosen from [h1, h2] and [w1, w2] intervals respectively. Like in the previous case, the first shape is always taken on validation and infering. stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example: weightsPath type : string Path to the model pretreined weights. Should be absolute or relative to the musket config file. Stage properties epochs type : integer Number of epochs to train for this stage. Example: Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter . fit script arguments fit.py project type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\" fit.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\" fit.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1 fit.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1 fit.py num_workers type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1 fit.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True fit.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True fit.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True fit.py only_report type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True fit.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\" fit.py folds type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2 task script arguments task.py project type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\" task.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\" task.py task type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\" task.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1 task.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1 task.py num_workers type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1 task.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True task.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True task.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True task.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\" analyze script arguments analyze.py inputFolder type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\" analyze.py output type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\" analyze.py onlyMetric type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\" analyze.py sortBy type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"Reference"},{"location":"instance_segmentation/reference/#instance-segmentation-pipeline-reference","text":"","title":"Instance Segmentation pipeline reference"},{"location":"instance_segmentation/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"instance_segmentation/reference/#classes","text":"type : integer Number of classes that should be segmented. Example:","title":"classes"},{"location":"instance_segmentation/reference/#configpath","text":"type : string Path to MMDetection config file. Should be absolute or relative to the musket config file.","title":"configPath"},{"location":"instance_segmentation/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"instance_segmentation/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"instance_segmentation/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example:","title":"folds_count"},{"location":"instance_segmentation/reference/#holdout","text":"type : ```` Example:","title":"holdout"},{"location":"instance_segmentation/reference/#imagespergpu","text":"type : integer Number of images in a batch to be processed by single GPU. MMDetection does not allow specifying batch size directly, it only allows setting how much images are processed by each GPU at a time. Thus, batch size is imagesPerGpu multiplied by gpus_per_net . Example: imagesPerGpu: 2","title":"imagesPerGpu"},{"location":"instance_segmentation/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"instance_segmentation/reference/#multiscalemode","text":"type : string Can be range or value (default). Setting value to range allows using two dimensional integer arrays as shape values for specifying possible ranges of train shapes.","title":"multiscaleMode"},{"location":"instance_segmentation/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"instance_segmentation/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"instance_segmentation/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"instance_segmentation/reference/#shape","text":"type : one or two dimensional array of integers Shape of the model input. All images are automatically scaled to this shape before being processed by the model. The exact meaning of the parameter can be: One dimensional array is simply understood as as [height, width] for train, validation and infering shapes. Two dimensional array is understood as array of shapes. Train shape is chosen randomly from the array for each train sample, and the first shape is always taken on validation and infering. With the multiscaleMode parameter set to range a two element two dimensional array [[h1,w1], [h2,w2]] is understood as possible range for train shapes: train height and width are randomly chosen from [h1, h2] and [w1, w2] intervals respectively. Like in the previous case, the first shape is always taken on validation and infering.","title":"shape"},{"location":"instance_segmentation/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"instance_segmentation/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"instance_segmentation/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"instance_segmentation/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"instance_segmentation/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example:","title":"validationSplit"},{"location":"instance_segmentation/reference/#weightspath","text":"type : string Path to the model pretreined weights. Should be absolute or relative to the musket config file.","title":"weightsPath"},{"location":"instance_segmentation/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"instance_segmentation/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"instance_segmentation/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"instance_segmentation/reference/#cache","text":"Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"cache"},{"location":"instance_segmentation/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"instance_segmentation/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"instance_segmentation/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"instance_segmentation/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"instance_segmentation/reference/#augmentation","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter .","title":"augmentation"},{"location":"instance_segmentation/reference/#fit-script-arguments","text":"","title":"fit script arguments"},{"location":"instance_segmentation/reference/#fitpy-project","text":"type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\"","title":"fit.py project"},{"location":"instance_segmentation/reference/#fitpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\"","title":"fit.py name"},{"location":"instance_segmentation/reference/#fitpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1","title":"fit.py num_gpus"},{"location":"instance_segmentation/reference/#fitpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1","title":"fit.py gpus_per_net"},{"location":"instance_segmentation/reference/#fitpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1","title":"fit.py num_workers"},{"location":"instance_segmentation/reference/#fitpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True","title":"fit.py allow_resume"},{"location":"instance_segmentation/reference/#fitpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True","title":"fit.py force_recalc"},{"location":"instance_segmentation/reference/#fitpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True","title":"fit.py launch_tasks"},{"location":"instance_segmentation/reference/#fitpy-only_report","text":"type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True","title":"fit.py only_report"},{"location":"instance_segmentation/reference/#fitpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\"","title":"fit.py cache"},{"location":"instance_segmentation/reference/#fitpy-folds","text":"type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2","title":"fit.py folds"},{"location":"instance_segmentation/reference/#task-script-arguments","text":"","title":"task script arguments"},{"location":"instance_segmentation/reference/#taskpy-project","text":"type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\"","title":"task.py project"},{"location":"instance_segmentation/reference/#taskpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\"","title":"task.py name"},{"location":"instance_segmentation/reference/#taskpy-task","text":"type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\"","title":"task.py task"},{"location":"instance_segmentation/reference/#taskpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1","title":"task.py num_gpus"},{"location":"instance_segmentation/reference/#taskpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1","title":"task.py gpus_per_net"},{"location":"instance_segmentation/reference/#taskpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1","title":"task.py num_workers"},{"location":"instance_segmentation/reference/#taskpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True","title":"task.py allow_resume"},{"location":"instance_segmentation/reference/#taskpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True","title":"task.py force_recalc"},{"location":"instance_segmentation/reference/#taskpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True","title":"task.py launch_tasks"},{"location":"instance_segmentation/reference/#taskpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\"","title":"task.py cache"},{"location":"instance_segmentation/reference/#analyze-script-arguments","text":"","title":"analyze script arguments"},{"location":"instance_segmentation/reference/#analyzepy-inputfolder","text":"type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\"","title":"analyze.py inputFolder"},{"location":"instance_segmentation/reference/#analyzepy-output","text":"type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\"","title":"analyze.py output"},{"location":"instance_segmentation/reference/#analyzepy-onlymetric","text":"type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\"","title":"analyze.py onlyMetric"},{"location":"instance_segmentation/reference/#analyzepy-sortby","text":"type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"analyze.py sortBy"},{"location":"segmentation/","text":"Segmentation Training Pipeline This package is a part of Musket ML framework. Reasons to use Segmentation Pipeline Segmentation Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is easier to define an architecture of the network. Also there are a number of segmentation-specific features. The pipeline provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions It is easy to configure network architecture Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library Installation pip install segmentation_pipeline Note: this package requires python 3.6 This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here . Launching Launching experiments fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference Launching tasks task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result Working directory must point to the musket_core root folder. In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference Launching project analysis analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference Usage guide Training a model Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. Moreover, the whole fitting and prediction process can be launched with built-in script, the only really required python code is dataset definition to let the system know, which data to load. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Lets discover what's going on in more details: General train properties Lets take our standard example and check the following set of instructions: testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties Defining architecture Lets take a look at the following part of our example: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. The following three properties are required to set: backbone This property configures encoder that should be used. Different kinds of FPN , PSP , Linkenet , UNet and more are supported. architecture This property configures decoder architecture that should be used. net , Linknet , PSP , FPN and more are supported. classes sets the number of classes that should be used. The following ones are optional, but commonly used: activation sets activation function that should be used in last layer. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. encoder_weights configures initial weights of the encoder. Image and Mask Augmentations Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees augmentation property defines IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. In this example, Fliplr and Flipud keys are automatically mapped on Flip agugmenters , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example scale key of Affine is mapped on scale parameter of Affine augmenter . One interesting augementation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve Freezing and Unfreezing encoder Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Both freeze_encoder and unfreeze_encoder can be put into the root section and inside the stage. Note: This option is not supported for DeeplabV3 architecture. Custom datasets Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used to support Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE(datasets.DataSet): def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5) def getTrain()->datasets.DataSet: return SegmentationRLE(\"train.csv\",\"images/\") Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities. Multistage training Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Balancing your data One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False Advanced learning rates Dynamic learning rates As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly. LR Finder Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images: Training on crops Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. crops property sets the number of single dimension cells. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective. Using trained model Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False) Ensembling predictions And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True . Custom evaluation code Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr Accessing model You may get trained keras model by calling: cfg.load_model(fold, stage) . Analyzing experiments results Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py What is supported? At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library . Custom architectures, callbacks, metrics Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model. Examples Training background removal task(Pics Art Hackaton) in google collab FAQ How to continue training after crash? If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds) My notebooks constantly run out of memory, what can I do to reduce memory usage? One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3 How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter? BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5] How can I visualize images that are used for training (after augmentations)? You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch What I can do if i have some extra training data, that should not be included into validation, but should be used during the training? extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people How to get basic statistics across my folds/stages This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info() I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage? There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real What if I would like to build a really large ansemble of models? One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d) How to train on multiple gpus? cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"User guide"},{"location":"segmentation/#segmentation-training-pipeline","text":"This package is a part of Musket ML framework.","title":"Segmentation Training Pipeline"},{"location":"segmentation/#reasons-to-use-segmentation-pipeline","text":"Segmentation Pipeline was developed with a focus of enabling to make fast and simply-declared experiments, which can be easily stored, reproduced and compared to each other. Segmentation Pipeline has a lot of common parts with Generic pipeline , but it is easier to define an architecture of the network. Also there are a number of segmentation-specific features. The pipeline provides the following features: Allows to describe experiments in a compact and expressive way Provides a way to store and compare experiments in order to methodically find the best deap learning solution Easy to share experiments and their results to work in a team Experiment configurations are separated from model definitions It is easy to configure network architecture Provides great flexibility and extensibility via support of custom substances Common blocks like an architecture, callbacks, model metrics, predictions vizualizers and others should be written once and be a part of a common library","title":"Reasons to use Segmentation Pipeline"},{"location":"segmentation/#installation","text":"pip install segmentation_pipeline Note: this package requires python 3.6 This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here .","title":"Installation"},{"location":"segmentation/#launching","text":"","title":"Launching"},{"location":"segmentation/#launching-experiments","text":"fit.py script is designed to launch experiment training. In order to run the experiment or a number of experiments, A typical command line may look like this: musket fit --project \"path/to/project\" --name \"experiment_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the fit script reference","title":"Launching experiments"},{"location":"segmentation/#launching-tasks","text":"task.py script is designed to launch experiment training. Tasks must be defined in the project python scope and marked by an annotation like this: from musket_core import tasks, model @tasks.task def measure2(m: model.ConnectedModel): return result Working directory must point to the musket_core root folder. In order to run the experiment or a number of experiments, A typical command line may look like this: python -m musket_core.task --project \"path/to/project\" --name \"experiment_name\" --task \"task_name\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"path/to/cache/folder\" --project points to the root of the project --name is the name of the project sub-folder containing experiment yaml file. --task is the name of the task function. --num_gpus sets number of GPUs to use during experiment launch. --gpus_per_net is a maximum number of GPUs to use per single experiment. --num_workers sets number of workers to use. --cache points to a cache folder to store the temporary data. Other parameters can be found in the task script reference","title":"Launching tasks"},{"location":"segmentation/#launching-project-analysis","text":"analize.py script is designed to launch project-scope analysis. Note that only experiments, which training is already finished will be covered. musket analize --inputFolder \"path/to/project\" --inputFolder points to a folder to search for finished experiments in. Typically, project root. Other parameters can be found in the analyze script reference","title":"Launching project analysis"},{"location":"segmentation/#usage-guide","text":"","title":"Usage guide"},{"location":"segmentation/#training-a-model","text":"Let's start from the absolutely minimalistic example. Let's say that you have two folders, one of them contains jpeg images, and another one - png files with segmentation masks for these images. And you need to train a neural network that will do segmentation for you. In this extremely simple setup all that you need is to type following 5 lines of python code: from segmentation_pipeline.impl.datasets import SimplePNGMaskDataSet from segmentation_pipeline import segmentation ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg = segmentation.parse(\"config.yaml\") cfg.fit(ds) Looks simple, but there is a config.yaml file in the code, and probably it is the place where everything actually happens. backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use augmentation: Fliplr: 0.5 #let's define some minimal augmentations on images Flipud: 0.5 classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy callbacks: #Let's configure some minimal callbacks EarlyStopping: patience: 15 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 4 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 loss: binary_crossentropy #We use simple binary_crossentropy loss stages: - epochs: 100 #Let's go for 100 epochs So as you see, we have decomposed our task in two parts, code that actually trains the model and experiment configuration , which determines the model and how it should be trained from the set of predefined building blocks. Moreover, the whole fitting and prediction process can be launched with built-in script, the only really required python code is dataset definition to let the system know, which data to load. What does this code actually do behind the scenes? it splits your data into 5 folds, and trains one model per fold; it takes care of model checkpointing, generates example image/mask/segmentation triples, collects training metrics. All this data will be stored in the folders just near your config.yaml ; All your folds are initialized from fixed default seed, so different experiments will use exactly the same train/validation splits Also, datasets can be specified directly in your config file in more generic way, see examples ds_1, ds_2, ds_3 in \"segmentation_training_pipeline/examples/people\" folder. In this case you can just call cfg.fit() without providing dataset programmatically. Lets discover what's going on in more details:","title":"Training a model"},{"location":"segmentation/#general-train-properties","text":"Lets take our standard example and check the following set of instructions: testSplit: 0.4 optimizer: Adam #Adam optimizer is a good default choice batch: 16 #Our batch size will be 16 metrics: #We would like to track some metrics - binary_accuracy - iou primary_metric: val_binary_accuracy #and the most interesting metric is val_binary_accuracy loss: binary_crossentropy #We use simple binary_crossentropy loss testSplit Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. optimizer sets the optimizer. batch sets the training batch size. metrics sets the metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. primary_metric Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. loss sets the loss function. if your network has multiple outputs, you also may pass a list of loss functions (one per output) Framework supports composing loss as a weighted sum of predefined loss functions. For example, following construction loss: binary_crossentropy+0.1*dice_loss will result in loss function which is composed from binary_crossentropy and dice_loss functions. There are many more properties to check in Reference of root properties","title":"General train properties"},{"location":"segmentation/#defining-architecture","text":"Lets take a look at the following part of our example: backbone: mobilenetv2 #let's select classifier backbone for our network architecture: DeepLabV3 #let's select segmentation architecture that we would like to use classes: 1 #we have just one class (mask or no mask) activation: sigmoid #one class means that our last layer should use sigmoid activation encoder_weights: pascal_voc #we would like to start from network pretrained on pascal_voc dataset shape: [320, 320, 3] #This is our desired input image and mask size, everything will be resized to fit. The following three properties are required to set: backbone This property configures encoder that should be used. Different kinds of FPN , PSP , Linkenet , UNet and more are supported. architecture This property configures decoder architecture that should be used. net , Linknet , PSP , FPN and more are supported. classes sets the number of classes that should be used. The following ones are optional, but commonly used: activation sets activation function that should be used in last layer. shape set the desired shape of the input picture and mask, in the form heigth, width, number of channels. Input will be resized to fit. encoder_weights configures initial weights of the encoder.","title":"Defining architecture"},{"location":"segmentation/#image-and-mask-augmentations","text":"Framework uses awesome imgaug library for augmentation, so you only need to configure your augmentation process in declarative way like in the following example: augmentation: Fliplr: 0.5 Flipud: 0.5 Affine: scale: [0.8, 1.5] #random scalings translate_percent: x: [-0.2,0.2] #random shifts y: [-0.2,0.2] rotate: [-16, 16] #random rotations on -16,16 degrees shear: [-16, 16] #random shears on -16,16 degrees augmentation property defines IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. In this example, Fliplr and Flipud keys are automatically mapped on Flip agugmenters , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example scale key of Affine is mapped on scale parameter of Affine augmenter . One interesting augementation option when doing background removal task is replacing backgrounds with random images. We support this with BackgroundReplacer augmenter: augmentation: BackgroundReplacer: path: ./bg #path to folder with backgrounds rate: 0.5 #fraction of original backgrounds to preserve","title":"Image and Mask Augmentations"},{"location":"segmentation/#freezing-and-unfreezing-encoder","text":"Freezing encoder is often used with transfer learning. If you want to start with frozen encoder just add freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true in your experiments configuration, then on some stage configuration just add unfreeze_encoder: true to stage settings. Both freeze_encoder and unfreeze_encoder can be put into the root section and inside the stage. Note: This option is not supported for DeeplabV3 architecture.","title":"Freezing and Unfreezing encoder"},{"location":"segmentation/#custom-datasets","text":"Training data and masks are not necessarily stored in files, so sometimes you need to declare your own dataset class, for example, the following code was used to support Airbus ship detection challenge to decode segmentation masks from rle encoded strings stored in csv file from segmentation_pipeline.impl.datasets import PredictionItem import os from segmentation_pipeline.impl import rle import imageio import pandas as pd class SegmentationRLE(datasets.DataSet): def __init__(self,path,imgPath): self.data=pd.read_csv(path); self.values=self.data.values; self.imgPath=imgPath; self.ship_groups=self.data.groupby('ImageId'); self.masks=self.ship_groups['ImageId']; self.ids=list(self.ship_groups.groups.keys()) pass def __len__(self): return len(self.masks) def __getitem__(self, item): pixels=self.ship_groups.get_group(self.ids[item])[\"EncodedPixels\"] return PredictionItem(self.ids[item] + str(), imageio.imread(os.path.join(self.imgPath,self.ids[item])), rle.masks_as_image(pixels) > 0.5) def getTrain()->datasets.DataSet: return SegmentationRLE(\"train.csv\",\"images/\") Now, if this python code sits somewhere in python files located in modules folder of the project, and that file is referred by imports instruction, following YAML can refer it: dataset: getTrain: [] dataset sets the main training dataset. datasets sets up a list of available data sets to be referred by other entities.","title":"Custom datasets"},{"location":"segmentation/#multistage-training","text":"Sometimes you need to split your training into several stages. You can easily do it by adding several stage entries in your experiment configuration file. stages instruction allows to set up stages of the train process, where for each stage it is possible to set some specific training options like the number of epochs, learning rate, loss, callbacks, etc. Full list of stage properties can be found here . stages: - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs - epochs: 100 #Let's go for 100 epochs stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"Multistage training"},{"location":"segmentation/#balancing-your-data","text":"One common case is the situation when part of your images does not contain any objects of interest, like in Airbus ship detection challenge . More over your data may be to heavily inbalanced, so you may want to rebalance it. Alternatively you may want to inject some additional images that do not contain objects of interest to decrease amount of false positives that will be produced by the framework. These scenarios are supported by negatives and validation_negatives settings of training stage configuration, these settings accept following values: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file if you are using this setting your dataset class must support isPositive method which returns true for indexes which contain positive examples: def isPositive(self, item): pixels=self.ddd.get_group(self.ids[item])[\"EncodedPixels\"] for mask in pixels: if isinstance(mask, str): return True; return False","title":"Balancing your data"},{"location":"segmentation/#advanced-learning-rates","text":"","title":"Advanced learning rates"},{"location":"segmentation/#dynamic-learning-rates","text":"As told in Cyclical learning rates for training neural networks CLR policies can provide quicker converge for some neural network tasks and architectures. We support them by adopting Brad Kenstler CLR callback for Keras. If you want to use them, just add CyclicLR in your experiment configuration file as shown below: callbacks: EarlyStopping: patience: 40 monitor: val_binary_accuracy verbose: 1 CyclicLR: base_lr: 0.0001 max_lr: 0.01 mode: triangular2 step_size: 300 There are also ReduceLROnPlateau and LRVariator options to modify learning rate on the fly.","title":"Dynamic learning rates"},{"location":"segmentation/#lr-finder","text":"Estimating optimal learning rate for your model is an important thing, we support this by using slightly changed version of Pavel Surmenok - Keras LR Finder cfg= segmentation.parse(people-1.yaml) ds=SimplePNGMaskDataSet(\"./train\",\"./train_mask\") finder=cfg.lr_find(ds,start_lr=0.00001,end_lr=1,epochs=5) finder.plot_loss(n_skip_beginning=20, n_skip_end=5) plt.show() finder.plot_loss_change(sma=20, n_skip_beginning=20, n_skip_end=5, y_lim=(-0.01, 0.01)) plt.show() will result in this couple of helpful images:","title":"LR Finder"},{"location":"segmentation/#training-on-crops","text":"Your images can be too large to train model on them. In this case you probably want to train model on crops. All that you need to do is to specify number of splits per axis. For example, following lines in config shape: [768, 768, 3] crops: 3 will lead to splitting each image/mask into 9 cells (3 horizontal splits and 3 vertical splits) and training model on these splits. Augmentations will be run separately on each cell. crops property sets the number of single dimension cells. During prediction time, your images will be split into these cells, prediction will be executed on each cell, and then results will be assembled in single final mask. Thus the whole process of cropping will be invisible from a consumer perspective.","title":"Training on crops"},{"location":"segmentation/#using-trained-model","text":"Okey, our model is trained, now we need to actually do image segmentation. Let's say, we need to run image segmentation on images in the directory and store results in csv file: from segmentation_pipeline import segmentation from segmentation_pipeline.impl.rle import rle_encode from skimage.morphology import remove_small_objects, remove_small_holes import pandas as pd #this is our callback which is called for every image def onPredict(file_name, img, data): threshold = 0.25 predictions = data[\"pred\"] imgs = data[\"images\"] post_img = remove_small_holes(remove_small_objects(img.arr > threshold)) rle = rle_encode(post_img) predictions.append(rle) imgs.append(file_name[:file_name.index(\".\")]) pass cfg= segmentation.parse(\"config.yaml\") predictions = [] images = [] #Now let's use best model from fold 0 to do image segmentation on images from images_to_segment cfg.predict_in_directory(\"./images_to_segment\", 0, 0, onPredict, {\"pred\": predictions, \"images\": images}) #Let's store results in csv df = pd.DataFrame.from_dict({'image': images, 'rle_mask': predictions}) df.to_csv('baseline_submission.csv', index=False)","title":"Using trained model"},{"location":"segmentation/#ensembling-predictions","text":"And what if you want to ensemble models from several folds? Just pass a list of fold numbers to predict_in_directory like in the following example: cfg.predict_in_directory(\"./images_to_segment\", [0,1,2,3,4], onPredict, {\"pred\": predictions, \"images\": images}) Another supported option is to ensemble results from extra test time augmentation (flips) by adding keyword arg ttflips=True .","title":"Ensembling predictions"},{"location":"segmentation/#custom-evaluation-code","text":"Sometimes you need to run custom evaluation code. In such case you may use: evaluateAll method, which provides an iterator on the batches containing original images, training masks and predicted masks for batch in cfg.evaluateAll(ds,2): for i in range(len(batch.predicted_maps_aug)): masks = ds.get_masks(batch.data[i]) for d in range(1,20): cur_seg = binary_opening(batch.predicted_maps_aug[i].arr > d/20, np.expand_dims(disk(2), -1)) cm = rle.masks_as_images(rle.multi_rle_encode(cur_seg)) pr = f2(masks, cm); total[d]=total[d]+pr","title":"Custom evaluation code"},{"location":"segmentation/#accessing-model","text":"You may get trained keras model by calling: cfg.load_model(fold, stage) .","title":"Accessing model"},{"location":"segmentation/#analyzing-experiments-results","text":"Okey, we have done a lot of experiments and now we need to compare the results and understand what works better. This repository contains script which may be used to analyze folder containing sub folders with experiment configurations and results. This script gathers all configurations, diffs them by doing structural diff, then for each configuration it averages metrics for all folds and generates csv file containing metrics and parameters that was actually changed in your experiment like in the following example This script accepts following arguments: inputFolder - root folder to search for experiments configurations and results output - file to store aggregated metrics onlyMetric - if you specify this option all other metrics will not be written in the report file sortBy - metric that should be used to sort results Example: python analize.py --inputFolder ./experiments --output ./result.py","title":"Analyzing experiments results"},{"location":"segmentation/#what-is-supported","text":"At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : encoder_weights: pascal_voc Each architecture also supports some specific options, list of options is documented in segmentation RAML library . Supported augmentations are documented in augmentation RAML library . Callbacks are documented in callbacks RAML library .","title":"What is supported?"},{"location":"segmentation/#custom-architectures-callbacks-metrics","text":"Segmentation pipeline uses keras custom objects registry to find entities, so if you need to use custom loss function, activation or metric all that you need to do is to register it in Keras as: keras.utils.get_custom_objects()[\"my_loss\"]= my_loss If you want to inject new architecture, you should register it in segmentation.custom_models dictionary. For example: segmentation.custom.models['MyUnet']=MyUnet where MyUnet is a function that accepts architecture parameters as arguments and returns an instance of keras model.","title":"Custom architectures, callbacks, metrics"},{"location":"segmentation/#examples","text":"Training background removal task(Pics Art Hackaton) in google collab","title":"Examples"},{"location":"segmentation/#faq","text":"","title":"FAQ"},{"location":"segmentation/#how-to-continue-training-after-crash","text":"If you would like to continue training after crash, call setAllowResume method before calling fit cfg= segmentation.parse(\"./people-1.yaml\") cfg.setAllowResume(True) ds=SimplePNGMaskDataSet(\"./pics/train\",\"./pics/train_mask\") cfg.fit(ds)","title":"How to continue training after crash?"},{"location":"segmentation/#my-notebooks-constantly-run-out-of-memory-what-can-i-do-to-reduce-memory-usage","text":"One way to reduce memory usage is to limit augmentation queue limit which is 50 by default, like in the following example: segmentation_pipeline.impl.datasets.AUGMENTER_QUEUE_LIMIT = 3","title":"My notebooks constantly run out of memory, what can I do to reduce memory usage?"},{"location":"segmentation/#how-can-i-run-sepate-set-of-augmenters-on-initial-imagemask-when-replacing-backgrounds-with-background-augmenter","text":"BackgroundReplacer: rate: 0.5 path: ./bg augmenters: #this augmenters will run on original image before replacing background Affine: scale: [0.8, 1.5] translate_percent: x: [-0.2,0.2] y: [-0.2,0.2] rotate: [-16, 16] shear: [-16, 16] erosion: [0,5]","title":"How can I run sepate set of augmenters on initial image/mask when replacing backgrounds with Background Augmenter?"},{"location":"segmentation/#how-can-i-visualize-images-that-are-used-for-training-after-augmentations","text":"You should set showDataExamples to True like in the following sample cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") cfg.showDataExamples=True if will lead to generation of training images samples and storing them in examples folder at the end of each epoch","title":"How can I visualize images that are used for training (after augmentations)?"},{"location":"segmentation/#what-i-can-do-if-i-have-some-extra-training-data-that-should-not-be-included-into-validation-but-should-be-used-during-the-training","text":"extra_data=NotzeroSimplePNGMaskDataSet(\"./phaces/all\",\"./phaces/masks\") #My dataset that should be added to training segmentation.extra_train[\"people\"] = extra_data and in the config file: extra_train_data: people","title":"What I can do if i have some extra training data, that should not be included into validation, but should be used during the training?"},{"location":"segmentation/#how-to-get-basic-statistics-across-my-foldsstages","text":"This code sample will return primary metric stats over folds/stages cfg= segmentation.parse(\"./no_erosion_aug_on_masks/people-1.yaml\") metrics = cfg.info()","title":"How to get basic statistics across my folds/stages"},{"location":"segmentation/#i-have-some-callbacks-that-are-configured-globally-but-i-need-some-extra-callbacks-for-my-last-training-stage","text":"There are two possible ways how you may configure callbacks on stage level: override all global callbacks with callbacks setting. add your own custom callbacks with extra_callbacks setting. In the following sample CyclingRL callback is only appended to the sexond stage of training: loss: binary_crossentropy stages: - epochs: 20 negatives: real - epochs: 200 extra_callbacks: CyclicLR: base_lr: 0.000001 max_lr: 0.0001 mode: triangular step_size: 800 negatives: real","title":"I have some callbacks that are configured globally, but I need some extra callbacks for my last training stage?"},{"location":"segmentation/#what-if-i-would-like-to-build-a-really-large-ansemble-of-models","text":"One option to do this, is to store predictions for each file and model in numpy array, and then sum these predictions like in the following sample: cfg.predict_to_directory(\"./pics/test\",\"./pics/arr1\", [0, 1, 4, 2], 1, ttflips=True,binaryArray=True) cfg.predict_to_directory(\"./pics/test\", \"./pics/arr\", [0, 1, 4, 2], 2, ttflips=True, binaryArray=True) segmentation.ansemblePredictions(\"./pics/test\",[\"./pics/arr/\",\"./pics/arr1/\"],onPredict,d)","title":"What if I would like to build a really large ansemble of models?"},{"location":"segmentation/#how-to-train-on-multiple-gpus","text":"cfg.gpus=4 #or another number matching to the count of gpus that you have","title":"How to train on multiple gpus?"},{"location":"segmentation/getting_started/","text":"Getting dataset from Kaggle Installing kaggle stuff This should be done only once. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles. Downloading TGS Salt competition dataset Go to TGS Salt Identification competition and Accept the rules on the Rules tab. Make salt folder somewhere and create data subdirectory. Open console with salt/data folder as current and invoke kaggle competitions download -c tgs-salt-identification-challenge command. This will download dataset files. Then invoke unzip train.zip -d train to unzip train.zip files in to train folder. Adding an experiment Create experiments folder inside salt folder. Create exp01 folder inside experiments folder. Create config.yaml file inside exp01 folder. Put the following code inside config.yaml : #%Musket Segmentation 1.0 backbone: resnet34 #let's select classifier backbone for our network architecture: Unet #pre-trained model we are going to use augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 1 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid shape: [224,224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 8 #our batch size will be 16 lr: 0.001 metrics: #we would like to track some metrics - binary_accuracy - dice primary_metric: val_dice #the most interesting metric is val_binary_accuracy primary_metric_mode: max folds_count: 5 testSplit: 0.2 dumpPredictionsToCSV: true callbacks: #configure some minimal callbacks EarlyStopping: patience: 10 monitor: val_dice mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 50 dataset: getTrain: [] final_metrics: [ dice_with_custom_treshold_true_negative_is_one ] #You may use more then one metric here experiment_result: dice_with_custom_treshold_true_negative_is_one testTimeAugmentation: Horizontal_and_vertical You can find the details regarding this code in User guide . We can greatly speed up the training process by reducing the number of folds from 5 to 1 by replacing folds_count: 5 with folds_count: 1 , but this will train the only fold. Reducing the number of epochs will also speed things up by the cost of the quality: replace - epochs: 50 with - epochs: 20 if you wish so. Adding dataset Note the following instruction in our experiment YAML: dataset: getTrain: [] This instruction expects a python function somewhere on the scope, which is named getTrain and that should return dataset. Lets add it: Create modules folder inside salt folder. In modules folder create a file datasets.py (file name can be really anything). Put the following code in the file: from musket_core import image_datasets def getTrain(): return image_datasets.BinarySegmentationDataSet([\"train/images\"],\"train.csv\",\"id\",\"rle_mask\") First argument sets the images folder inside data . Second argument points to the CSV, third - CSV column with image IDs. The forth one points to CSV column with RLE mask. Running the experiment Launch the console and run the following command, taking into account that ..salt should be replaced with the path to the project top-level salt directory. musket fit --project \"...salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"...salt\\data\\cache\" This will launch the training process. Checking experiment results When the training process complete, exp01 experiment folder will contain the new summary.yaml file with contents similar to the following: allStages: binary_accuracy: {max: 0.94829979903931, mean: 0.9432402460543085, min: 0.9348129166258212, std: 0.004766299735936636} binary_accuracy_holdout: 0.9447703656504265 dice: {max: 0.8946749116884245, mean: 0.8799628551168702, min: 0.8512279018375117, std: 0.015545121737934034} dice_holdout: 0.8792530163407674 dice_with_custom_treshold_true_negative_is_one: {max: 0.7921144300184988, mean: 0.7906020228394381, min: 0.7885881826799147, std: 0.0012594068050284218} dice_with_custom_treshold_true_negative_is_one_holdout: 0.7918140261625017 dice_with_custom_treshold_true_negative_is_one_treshold: {max: 0.5700000000000001, mean: 0.5680000000000001, min: 0.56, std: 0.0040000000000000036} dice_with_custom_treshold_true_negative_is_one_treshold_holdout: 0.56 cfgName: config.yaml completed: true folds: [0, 1, 2, 3, 4] stages: - binary_accuracy: {max: 0.94829979903931, mean: 0.9432402460543085, min: 0.9348129166258212, std: 0.004766299735936636} binary_accuracy_holdout: 0.9447703656504265 dice: {max: 0.8946749116884245, mean: 0.8799628551168702, min: 0.8512279018375117, std: 0.015545121737934034} dice_holdout: 0.8792530163407674 dice_with_custom_treshold_true_negative_is_one: {max: 0.7924899348384953, mean: 0.7882949615678969, min: 0.7825054457176929, std: 0.003500868603109435} dice_with_custom_treshold_true_negative_is_one_holdout: 0.7918140261625017 dice_with_custom_treshold_true_negative_is_one_treshold: {max: 0.6, mean: 0.5680000000000001, min: 0.51, std: 0.03310589071449368} dice_with_custom_treshold_true_negative_is_one_treshold_holdout: 0.56 subsample: 1.0 Lets take a closer look: completed: true indicates that the training was completed. Sections of allStages and stages differ when there are more than a single stage, but in our case data inside are the same. binary_accuracy and dice values indicate metric results on validation. Those values appear in summary due to those metrics were listed in metrics section of config.yaml . As we have multiple folds and each fold has its own results, all metrics list max, min, mean and std values. Due to the testSplit instruction in config.yaml we have a holdout, so there is *_holdout values for each metrics indicating metric results on holdout dataset. As dice metric was referred in primary_metric instruction in config.yaml , there are lots of other values for this metric, their names speak for themselves. Besides summary.yaml file, which contain the top-level results and may ommitted if something fails in the training process, there are more detailed and precises logs inside metrics subfolder of exp01 folder. There are files named metrics-X.Y.csv , where X is the fold number, and Y is the stage number. Lets take a look: epoch,binary_accuracy,dice,loss,lr,val_binary_accuracy,val_dice,val_loss 0,0.8144591341726481,0.5552861321416458,0.4154038934037089,0.001,0.7323275666683913,0.026507374974244158,0.7329991146922111 1,0.8632630387321114,0.6899019529577345,0.331550125265494,0.001,0.7443207234144211,0.012612525901568005,0.5845782220363617 The first column is an epoch number. Then there are all metrics listed on training set, then loss and learning rate, and finally same metrics and loss on validation. These values allow to see how the training was advancing.","title":"Getting started"},{"location":"segmentation/getting_started/#getting-dataset-from-kaggle","text":"","title":"Getting dataset from Kaggle"},{"location":"segmentation/getting_started/#installing-kaggle-stuff","text":"This should be done only once. Run pip install kaggle in console. Log into Kaggle Click on a profile in the top-right corner and choose My Account On the account page find Api section and click Create New API Token . This will launch the download of kaggle.json token file. Put the file into ~/.kaggle/kaggle.json or C:\\Users\\<Windows-username>\\.kaggle\\kaggle.json depending on OS. Note: there are potential troubles of creating C:\\Users\\<Windows-username>\\.kaggle using windows explorer. To create this folder from console, run cmd and launch the following commands: cd C:\\Users\\<Windows-username> , mkdir .kaggle . Consult to Kaggle API in case of other troubles.","title":"Installing kaggle stuff"},{"location":"segmentation/getting_started/#downloading-tgs-salt-competition-dataset","text":"Go to TGS Salt Identification competition and Accept the rules on the Rules tab. Make salt folder somewhere and create data subdirectory. Open console with salt/data folder as current and invoke kaggle competitions download -c tgs-salt-identification-challenge command. This will download dataset files. Then invoke unzip train.zip -d train to unzip train.zip files in to train folder.","title":"Downloading TGS Salt competition dataset"},{"location":"segmentation/getting_started/#adding-an-experiment","text":"Create experiments folder inside salt folder. Create exp01 folder inside experiments folder. Create config.yaml file inside exp01 folder. Put the following code inside config.yaml : #%Musket Segmentation 1.0 backbone: resnet34 #let's select classifier backbone for our network architecture: Unet #pre-trained model we are going to use augmentation: #define some minimal augmentations on images Fliplr: 0.5 Flipud: 0.5 classes: 1 #define the number of classes activation: sigmoid #as we have multilabel classification, the activation for last layer is sigmoid shape: [224,224, 3] #our desired input image size, everything will be resized to fit optimizer: Adam #Adam optimizer is a good default choice batch: 8 #our batch size will be 16 lr: 0.001 metrics: #we would like to track some metrics - binary_accuracy - dice primary_metric: val_dice #the most interesting metric is val_binary_accuracy primary_metric_mode: max folds_count: 5 testSplit: 0.2 dumpPredictionsToCSV: true callbacks: #configure some minimal callbacks EarlyStopping: patience: 10 monitor: val_dice mode: max verbose: 1 ReduceLROnPlateau: patience: 2 factor: 0.3 monitor: val_binary_accuracy mode: max cooldown: 1 verbose: 1 loss: binary_crossentropy #we use binary_crossentropy loss stages: - epochs: 50 dataset: getTrain: [] final_metrics: [ dice_with_custom_treshold_true_negative_is_one ] #You may use more then one metric here experiment_result: dice_with_custom_treshold_true_negative_is_one testTimeAugmentation: Horizontal_and_vertical You can find the details regarding this code in User guide . We can greatly speed up the training process by reducing the number of folds from 5 to 1 by replacing folds_count: 5 with folds_count: 1 , but this will train the only fold. Reducing the number of epochs will also speed things up by the cost of the quality: replace - epochs: 50 with - epochs: 20 if you wish so.","title":"Adding an experiment"},{"location":"segmentation/getting_started/#adding-dataset","text":"Note the following instruction in our experiment YAML: dataset: getTrain: [] This instruction expects a python function somewhere on the scope, which is named getTrain and that should return dataset. Lets add it: Create modules folder inside salt folder. In modules folder create a file datasets.py (file name can be really anything). Put the following code in the file: from musket_core import image_datasets def getTrain(): return image_datasets.BinarySegmentationDataSet([\"train/images\"],\"train.csv\",\"id\",\"rle_mask\") First argument sets the images folder inside data . Second argument points to the CSV, third - CSV column with image IDs. The forth one points to CSV column with RLE mask.","title":"Adding dataset"},{"location":"segmentation/getting_started/#running-the-experiment","text":"Launch the console and run the following command, taking into account that ..salt should be replaced with the path to the project top-level salt directory. musket fit --project \"...salt\" --name \"exp01\" --num_gpus=1 --gpus_per_net=1 --num_workers=1 --cache \"...salt\\data\\cache\" This will launch the training process.","title":"Running the experiment"},{"location":"segmentation/getting_started/#checking-experiment-results","text":"When the training process complete, exp01 experiment folder will contain the new summary.yaml file with contents similar to the following: allStages: binary_accuracy: {max: 0.94829979903931, mean: 0.9432402460543085, min: 0.9348129166258212, std: 0.004766299735936636} binary_accuracy_holdout: 0.9447703656504265 dice: {max: 0.8946749116884245, mean: 0.8799628551168702, min: 0.8512279018375117, std: 0.015545121737934034} dice_holdout: 0.8792530163407674 dice_with_custom_treshold_true_negative_is_one: {max: 0.7921144300184988, mean: 0.7906020228394381, min: 0.7885881826799147, std: 0.0012594068050284218} dice_with_custom_treshold_true_negative_is_one_holdout: 0.7918140261625017 dice_with_custom_treshold_true_negative_is_one_treshold: {max: 0.5700000000000001, mean: 0.5680000000000001, min: 0.56, std: 0.0040000000000000036} dice_with_custom_treshold_true_negative_is_one_treshold_holdout: 0.56 cfgName: config.yaml completed: true folds: [0, 1, 2, 3, 4] stages: - binary_accuracy: {max: 0.94829979903931, mean: 0.9432402460543085, min: 0.9348129166258212, std: 0.004766299735936636} binary_accuracy_holdout: 0.9447703656504265 dice: {max: 0.8946749116884245, mean: 0.8799628551168702, min: 0.8512279018375117, std: 0.015545121737934034} dice_holdout: 0.8792530163407674 dice_with_custom_treshold_true_negative_is_one: {max: 0.7924899348384953, mean: 0.7882949615678969, min: 0.7825054457176929, std: 0.003500868603109435} dice_with_custom_treshold_true_negative_is_one_holdout: 0.7918140261625017 dice_with_custom_treshold_true_negative_is_one_treshold: {max: 0.6, mean: 0.5680000000000001, min: 0.51, std: 0.03310589071449368} dice_with_custom_treshold_true_negative_is_one_treshold_holdout: 0.56 subsample: 1.0 Lets take a closer look: completed: true indicates that the training was completed. Sections of allStages and stages differ when there are more than a single stage, but in our case data inside are the same. binary_accuracy and dice values indicate metric results on validation. Those values appear in summary due to those metrics were listed in metrics section of config.yaml . As we have multiple folds and each fold has its own results, all metrics list max, min, mean and std values. Due to the testSplit instruction in config.yaml we have a holdout, so there is *_holdout values for each metrics indicating metric results on holdout dataset. As dice metric was referred in primary_metric instruction in config.yaml , there are lots of other values for this metric, their names speak for themselves. Besides summary.yaml file, which contain the top-level results and may ommitted if something fails in the training process, there are more detailed and precises logs inside metrics subfolder of exp01 folder. There are files named metrics-X.Y.csv , where X is the fold number, and Y is the stage number. Lets take a look: epoch,binary_accuracy,dice,loss,lr,val_binary_accuracy,val_dice,val_loss 0,0.8144591341726481,0.5552861321416458,0.4154038934037089,0.001,0.7323275666683913,0.026507374974244158,0.7329991146922111 1,0.8632630387321114,0.6899019529577345,0.331550125265494,0.001,0.7443207234144211,0.012612525901568005,0.5845782220363617 The first column is an epoch number. Then there are all metrics listed on training set, then loss and learning rate, and finally same metrics and loss on validation. These values allow to see how the training was advancing.","title":"Checking experiment results"},{"location":"segmentation/reference/","text":"Segmentation pipeline reference Pipeline root properties activation type : string Activation function that should be used in last layer. In the case of binary segmentation it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid experiment_result type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout architecture type : string This property configures decoder architecture that should be used: At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN augmentation type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 backbone type : string This property configures encoder that should be used: FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC : batch type : integer Sets up training batch size. Example: batch: 8 classifier type : string classes type : integer Number of classes that should be segmented. Example: callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 compressPredictionsAsInts type : boolean Whether to represent predictions as int8 (0..255). This settings allows to trade amount of space that is consumbed by prediction to a little bit of accuracy Example: compressPredictionsAsInts: true copyWeights type : boolean Whether to copy saved weights. Example: copyWeights: true clipnorm type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0 clipvalue type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5 crops type : integer Defines the number of crops to make from original image by setting the single number of single dimension cells. In example, the value of 3 will split the original image into 9 cells: 3 by horizontal and 3 by vertical. Example: crops: 3 dataset type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false] datasets type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false] dataset_augmenter type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test dropout type : float Example: encoder_weights type : string This property configures initial weights of the encoder, supported values: imagenet Example: encoder_weights: imagenet extra_train_data type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example: folds_count type : integer Number of folds to train. Default is 5. Example: freeze_encoder type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true final_metrics type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure] holdout type : ```` Example: imports type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py inference_batch type : integer Size of batch during inferring process. Example: loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: lr: 0.01 manualResize type : boolean Setting this property to true, will disable auto resize that is performed by pipeline Example: manualResize: true metrics type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation num_seeds type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example: optimizer type : string Sets the optimizer. Example: optimizer: Adam primary_metric type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1 primary_metric_mode type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max preprocessing type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: random_state type : integer The seed of randomness. Example: shape type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3] stages type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01 stratified type : boolean Whether to use stratified strategy when splitting training set. Example: testSplit type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4 testSplitSeed type : ```` Seed of randomness for the split of the training set. Example: testTimeAugmentation type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example: transforms type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 validationSplit type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example: Callback types EarlyStopping Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 CyclicLR Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular LRVariator Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example TensorBoard This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch Stage properties callbacks type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1 epochs type : integer Number of epochs to train for this stage. Example: extra_callbacks freeze_encoder type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true initial_weights type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights negatives type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file loss type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy lr type : float Learning rate. Example: unfreeze_encoder type : boolean Whether to unfreeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true validation_negatives type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file Preprocessors type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: cache Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: disk-cache Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache: split-preprocessor An analogue of split for preprocessor operations. Example: split-concat-preprocessor An analogue of split-concat for preprocessor operations. Example: seq-preprocessor An analogue of seq for preprocessor operations. Example: augmentation Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter . fit script arguments fit.py project type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\" fit.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\" fit.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1 fit.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1 fit.py num_workers type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1 fit.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True fit.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True fit.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True fit.py only_report type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True fit.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\" fit.py folds type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2 fit.py time type : string This setting is not intented to be used directly. Example: -m musket_core.fit task script arguments task.py project type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\" task.py name type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\" task.py task type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\" task.py num_gpus type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1 task.py gpus_per_net type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1 task.py num_workers type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1 task.py allow_resume type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True task.py force_recalc type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True task.py launch_tasks type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True task.py cache type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\" analyze script arguments analyze.py inputFolder type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\" analyze.py output type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\" analyze.py onlyMetric type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\" analyze.py sortBy type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"Reference"},{"location":"segmentation/reference/#segmentation-pipeline-reference","text":"","title":"Segmentation pipeline reference"},{"location":"segmentation/reference/#pipeline-root-properties","text":"","title":"Pipeline root properties"},{"location":"segmentation/reference/#activation","text":"type : string Activation function that should be used in last layer. In the case of binary segmentation it usually should be sigmoid if you have more then one class than most likely you need to use softmax , but actually you are free to use any activation function that is registered in Keras Example: activation: sigmoid","title":"activation"},{"location":"segmentation/reference/#experiment_result","text":"type : string Metric to calculate against the combination of all stages and report in allStages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: experiment_result: matthews_correlation_holdout","title":"experiment_result"},{"location":"segmentation/reference/#architecture","text":"type : string This property configures decoder architecture that should be used: At this moment segmentation pipeline supports following architectures: Unet Linknet PSP FPN DeeplabV3 Example: architecture: FPN","title":"architecture"},{"location":"segmentation/reference/#augmentation","text":"type : complex IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"augmentation"},{"location":"segmentation/reference/#backbone","text":"type : string This property configures encoder that should be used: FPN , PSP , Linkenet , UNet architectures support following backbones: VGGNet vgg16 vgg19 ResNet resnet18 resnet34 resnet50 resnet101 resnet152 ResNext resnext50 resnext101 DenseNet densenet121 densenet169 densenet201 Inception-v3 Inception-ResNet-v2 All them support the weights pretrained on ImageNet : encoder_weights: imagenet At this moment DeeplabV3 architecture supports following backbones: - MobileNetV2 - Xception Deeplab supports weights pretrained on PASCAL VOC :","title":"backbone"},{"location":"segmentation/reference/#batch","text":"type : integer Sets up training batch size. Example: batch: 8","title":"batch"},{"location":"segmentation/reference/#classifier","text":"type : string","title":"classifier"},{"location":"segmentation/reference/#classes","text":"type : integer Number of classes that should be segmented. Example:","title":"classes"},{"location":"segmentation/reference/#callbacks","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"segmentation/reference/#compresspredictionsasints","text":"type : boolean Whether to represent predictions as int8 (0..255). This settings allows to trade amount of space that is consumbed by prediction to a little bit of accuracy Example: compressPredictionsAsInts: true","title":"compressPredictionsAsInts"},{"location":"segmentation/reference/#copyweights","text":"type : boolean Whether to copy saved weights. Example: copyWeights: true","title":"copyWeights"},{"location":"segmentation/reference/#clipnorm","text":"type : float Maximum clip norm of a gradient for an optimizer. Example: clipnorm: 1.0","title":"clipnorm"},{"location":"segmentation/reference/#clipvalue","text":"type : float Clip value of a gradient for an optimizer. Example: clipvalue: 0.5","title":"clipvalue"},{"location":"segmentation/reference/#crops","text":"type : integer Defines the number of crops to make from original image by setting the single number of single dimension cells. In example, the value of 3 will split the original image into 9 cells: 3 by horizontal and 3 by vertical. Example: crops: 3","title":"crops"},{"location":"segmentation/reference/#dataset","text":"type : complex object Key is a name of the python function in scope, which returns training data set. Value is an array of parameters to pass to a function. Example: dataset: getTrain: [false,false]","title":"dataset"},{"location":"segmentation/reference/#datasets","text":"type : map containing complex objects Sets up a list of available data sets to be referred by other entities. For each object, key is a name of the python function in scope, which returns training dataset. Value is an array of parameters to pass to a function. Example: datasets: test: getTest: [false,false]","title":"datasets"},{"location":"segmentation/reference/#dataset_augmenter","text":"type : complex object Sets up a custom augmenter function to be applied to a dataset. Object must have a name property, whic will be used as a name of the python function in scope. Other object properties are mapped as function arguments. Example: dataset_augmenter: name: TheAugmenter parameter: test","title":"dataset_augmenter"},{"location":"segmentation/reference/#dropout","text":"type : float Example:","title":"dropout"},{"location":"segmentation/reference/#encoder_weights","text":"type : string This property configures initial weights of the encoder, supported values: imagenet Example: encoder_weights: imagenet","title":"encoder_weights"},{"location":"segmentation/reference/#extra_train_data","text":"type : string Name of the additional dataset that will be added (per element) to the training dataset before train launching. Example:","title":"extra_train_data"},{"location":"segmentation/reference/#folds_count","text":"type : integer Number of folds to train. Default is 5. Example:","title":"folds_count"},{"location":"segmentation/reference/#freeze_encoder","text":"type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"freeze_encoder"},{"location":"segmentation/reference/#final_metrics","text":"type : array of strings Metrics to calculate against every stage and report in stages section of summary.yaml file after all experiment instances are finished. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: final_metrics: [measure]","title":"final_metrics"},{"location":"segmentation/reference/#holdout","text":"type : ```` Example:","title":"holdout"},{"location":"segmentation/reference/#imports","text":"type : array of strings Imports python files from modules folder of the project and make their properly annotated contents to be available to be referred from YAML. Example: imports: [ layers, preprocessors ] this will import layers.py and preprocessors.py","title":"imports"},{"location":"segmentation/reference/#inference_batch","text":"type : integer Size of batch during inferring process. Example:","title":"inference_batch"},{"location":"segmentation/reference/#loss","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"segmentation/reference/#lr","text":"type : float Learning rate. Example: lr: 0.01","title":"lr"},{"location":"segmentation/reference/#manualresize","text":"type : boolean Setting this property to true, will disable auto resize that is performed by pipeline Example: manualResize: true","title":"manualResize"},{"location":"segmentation/reference/#metrics","text":"type : array of strings Array of metrics to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: metrics: #We would like to track some metrics - binary_accuracy - binary_crossentropy - matthews_correlation","title":"metrics"},{"location":"segmentation/reference/#num_seeds","text":"type : integer If set, training process (for all folds) will be executed num_seeds times, each time resetting the random seeds. Respective folders (like metrics ) will obtain subfolders 0 , 1 etc... for each seed. Example:","title":"num_seeds"},{"location":"segmentation/reference/#optimizer","text":"type : string Sets the optimizer. Example: optimizer: Adam","title":"optimizer"},{"location":"segmentation/reference/#primary_metric","text":"type : string Metric to track during the training process. Metric calculation results will be printed in the console and to metrics folder of the experiment. Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better. Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules. Metric name may have val_ prefix or _holdout postfix to indicate calculation against validation or holdout, respectively. Example: primary_metric: val_macro_f1","title":"primary_metric"},{"location":"segmentation/reference/#primary_metric_mode","text":"type : enum: auto,min,max default : auto In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result. Example: primary_metric_mode: max","title":"primary_metric_mode"},{"location":"segmentation/reference/#preprocessing","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. preprocessing instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Preprocessors contain some of the preprocessor utility instructions. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"preprocessing"},{"location":"segmentation/reference/#random_state","text":"type : integer The seed of randomness. Example:","title":"random_state"},{"location":"segmentation/reference/#shape","text":"type : array of integers Shape of the input picture, in the form heigth,width, number of channels, all images will be resized to this shape before processing Example: shape: [440,440,3]","title":"shape"},{"location":"segmentation/reference/#stages","text":"type : complex Sets up training process stages. Contains YAML array of stages, where each stage is a complex type that may contain properties described in the Stage properties section. Example: stages: - epochs: 6 - epochs: 6 lr: 0.01","title":"stages"},{"location":"segmentation/reference/#stratified","text":"type : boolean Whether to use stratified strategy when splitting training set. Example:","title":"stratified"},{"location":"segmentation/reference/#testsplit","text":"type : float 0-1 Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing. The split is shuffled. Example: testSplit: 0.4","title":"testSplit"},{"location":"segmentation/reference/#testsplitseed","text":"type : ```` Seed of randomness for the split of the training set. Example:","title":"testSplitSeed"},{"location":"segmentation/reference/#testtimeaugmentation","text":"type : string Test-time augumentation function name. Function must be reachable on project scope, accept and return numpy array. Example:","title":"testTimeAugmentation"},{"location":"segmentation/reference/#transforms","text":"type : complex If yes, why are we having pure IMGAUG in generic called just \"transforms\", maybe we should call it \"imageTransforms\" or simply \"imgaug\". Btw, isnt it crossing with preprocessing, maybe we should just create \"imgaug\" preprocessor with all these goodies inside? IMGAUG transformations sequence. Each object is mapped on IMGAUG transformer by name, parameters are mapped too. Example: transforms: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50","title":"transforms"},{"location":"segmentation/reference/#validationsplit","text":"type : float Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation. Example:","title":"validationSplit"},{"location":"segmentation/reference/#callback-types","text":"","title":"Callback types"},{"location":"segmentation/reference/#earlystopping","text":"Stop training when a monitored metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1","title":"EarlyStopping"},{"location":"segmentation/reference/#reducelronplateau","text":"Reduce learning rate when a metric has stopped improving. Properties: patience - integer, number of epochs with no improvement after which training will be stopped. cooldown - integer, number of epochs to wait before resuming normal operation after lr has been reduced. factor - number, factor by which the learning rate will be reduced. new_lr = lr * factor verbose - 0 or 1, verbosity mode. monitor - string, name of the metric to monitor mode - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. Example callbacks: ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"ReduceLROnPlateau"},{"location":"segmentation/reference/#cycliclr","text":"Cycles learning rate across epochs. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function. Properties: base_lr - number, initial learning rate which is the lower boundary in the cycle. max_lr - number, upper boundary in the cycle. mode - one of triangular , triangular2 or exp_range ; scaling function. gamma - number from 0 to 1, constant in 'exp_range' scaling function. step_size - integer > 0, number of training iterations (batches) per half cycle. Example callbacks: CyclicLR: base_lr: 0.001 max_lr: 0.006 step_size: 2000 mode: triangular","title":"CyclicLR"},{"location":"segmentation/reference/#lrvariator","text":"Changes learning rate between two values Properties: fromVal - initial learning rate value, defaults to the configuration LR setup. toVal - final learning value. style - one of the following: linear - changes LR linearly between two values. const - does not change from initial value. cos+ - -1 * cos(2x/pi) + 1 for x in [0;1] cos- - cos(2x/pi) for x in [0;1] cos - same as 'cos-' sin+ - sin(2x/pi) x in [0;1] sin- - -1 * sin(2x/pi) + 1 for x in [0;1] sin - same as 'sin+' any positive float or integer value - x^a for x in [0;1] Example","title":"LRVariator"},{"location":"segmentation/reference/#tensorboard","text":"This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model. Properties: log_dir - string; the path of the directory where to save the log files to be parsed by TensorBoard. histogram_freq - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations. batch_size - integer; size of batch of inputs to feed to the network for histograms computation. write_graph - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True. write_grads - boolean; whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0. write_images - boolean; whether to write model weights to visualize as image in TensorBoard. embeddings_freq - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data. embeddings_layer_names - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched. embeddings_metadata - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed. embeddings_data - data to be embedded at layers specified in embeddings_layer_names. update_freq - epoch or batch or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training. Example callbacks: TensorBoard: log_dir: './logs' batch_size: 32 write_graph: True update_freq: batch","title":"TensorBoard"},{"location":"segmentation/reference/#stage-properties","text":"","title":"Stage properties"},{"location":"segmentation/reference/#callbacks_1","text":"type : array of callback instances Sets up training-time callbacks. See individual callback descriptions . Example: callbacks: EarlyStopping: patience: 100 monitor: val_binary_accuracy verbose: 1 ReduceLROnPlateau: patience: 16 factor: 0.5 monitor: val_binary_accuracy mode: auto cooldown: 5 verbose: 1","title":"callbacks"},{"location":"segmentation/reference/#epochs","text":"type : integer Number of epochs to train for this stage. Example:","title":"epochs"},{"location":"segmentation/reference/#extra_callbacks","text":"","title":"extra_callbacks"},{"location":"segmentation/reference/#freeze_encoder_1","text":"type : boolean Whether to freeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"freeze_encoder"},{"location":"segmentation/reference/#initial_weights","text":"type : string Fil path to load stage NN initial weights from. Example: initial_weights: /initial.weights","title":"initial_weights"},{"location":"segmentation/reference/#negatives","text":"type : string or integer The support of binary data balancing for training set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"negatives"},{"location":"segmentation/reference/#loss_1","text":"type : string Sets the loss name. Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules. Example: loss: binary_crossentropy","title":"loss"},{"location":"segmentation/reference/#lr_1","text":"type : float Learning rate. Example:","title":"lr"},{"location":"segmentation/reference/#unfreeze_encoder","text":"type : boolean Whether to unfreeze encoder during the training process. Example: freeze_encoder: true stages: - epochs: 10 #Let's go for 10 epochs with frozen encoder - epochs: 100 #Now let's go for 100 epochs with trainable encoder unfreeze_encoder: true","title":"unfreeze_encoder"},{"location":"segmentation/reference/#validation_negatives","text":"type : string or integer The support of binary data balancing for validation set. Following values are acceptable: none - exclude negative examples from the data real - include all negative examples integer number(1 or 2 or anything), how many negative examples should be included per one positive example In order for the system to determine whether a particular example is positive or negative, the data set class defined by the dataset property should have isPositive method declared that accepts data set item and returns boolean. Example: stages: - epochs: 6 #Train for 6 epochs negatives: none #do not include negative examples in your training set validation_negatives: real #validation should contain all negative examples - lr: 0.0001 #let's use different starting learning rate epochs: 6 negatives: real validation_negatives: real - loss: lovasz_loss #let's override loss function lr: 0.00001 epochs: 6 initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file","title":"validation_negatives"},{"location":"segmentation/reference/#preprocessors","text":"type : complex Preprocessors are the custom python functions that transform dataset. Such functions should be defined in python files that are in a project scope ( modules ) folder and imported. Preprocessing functions should be also marked with @preprocessing.dataset_preprocessor annotation. Preprocessors instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"Preprocessors"},{"location":"segmentation/reference/#cache","text":"Caches its input. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example:","title":"cache"},{"location":"segmentation/reference/#disk-cache","text":"Caches its input on disk, including the full flow. On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. Properties: name - string; optionally sets up layer name to refer it from other layers. inputs - array of strings; lists layer inputs. Example: preprocessing: - binarize_target: - tokenize: - tokens_to_indexes: maxLen: 160 - disk-cache:","title":"disk-cache"},{"location":"segmentation/reference/#split-preprocessor","text":"An analogue of split for preprocessor operations. Example:","title":"split-preprocessor"},{"location":"segmentation/reference/#split-concat-preprocessor","text":"An analogue of split-concat for preprocessor operations. Example:","title":"split-concat-preprocessor"},{"location":"segmentation/reference/#seq-preprocessor","text":"An analogue of seq for preprocessor operations. Example:","title":"seq-preprocessor"},{"location":"segmentation/reference/#augmentation_1","text":"Preprocessor instruction, which body only runs during the training and is skipped when the inferring. augmentation: Fliplr: 0.5 Affine: translate_px: x: - -50 - +50 y: - -50 - +50 In this example, Fliplr key is automatically mapped on Fliplr agugmenter , their 0.5 parameter is mapped on the first p parameter of the augmenter. Named parameters are also mapped, in example translate_px key of Affine is mapped on translate_px parameter of Affine augmenter .","title":"augmentation"},{"location":"segmentation/reference/#fit-script-arguments","text":"","title":"fit script arguments"},{"location":"segmentation/reference/#fitpy-project","text":"type : string Folder to search for experiments, project root. Example: -m musket_core.fit --project \"path/to/project\"","title":"fit.py project"},{"location":"segmentation/reference/#fitpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: -m musket_core.fit --name \"experiment_name\" -m musket_core.fit --name \"experiment_name1, experiment_name2\"","title":"fit.py name"},{"location":"segmentation/reference/#fitpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: -m musket_core.fit --num_gpus=1","title":"fit.py num_gpus"},{"location":"segmentation/reference/#fitpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: -m musket_core.fit --gpus_per_net=1","title":"fit.py gpus_per_net"},{"location":"segmentation/reference/#fitpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: -m musket_core.fit --num_workers=1","title":"fit.py num_workers"},{"location":"segmentation/reference/#fitpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: -m musket_core.fit --allow_resume True","title":"fit.py allow_resume"},{"location":"segmentation/reference/#fitpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: -m musket_core.fit --force_recalc True","title":"fit.py force_recalc"},{"location":"segmentation/reference/#fitpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: -m musket_core.fit --launch_tasks True","title":"fit.py launch_tasks"},{"location":"segmentation/reference/#fitpy-only_report","text":"type : boolean Default: False Whether to only generate reports for cached data, no training occurs. Example: -m musket_core.fit --only_report True","title":"fit.py only_report"},{"location":"segmentation/reference/#fitpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: -m musket_core.fit --cache \"path/to/cache/folder\"","title":"fit.py cache"},{"location":"segmentation/reference/#fitpy-folds","text":"type : integer or comma-separated list of integers Folds to launch. By default all folds of experiment will be executed, this argument allows launching only some of them. Example: -m musket_core.fit --folds 1,2","title":"fit.py folds"},{"location":"segmentation/reference/#fitpy-time","text":"type : string This setting is not intented to be used directly. Example: -m musket_core.fit","title":"fit.py time"},{"location":"segmentation/reference/#task-script-arguments","text":"","title":"task script arguments"},{"location":"segmentation/reference/#taskpy-project","text":"type : string Folder to search for experiments, project root. Example: task.py --project \"path/to/project\"","title":"task.py project"},{"location":"segmentation/reference/#taskpy-name","text":"type : string or comma-separated list of strings Name of the experiment to launch, or a list of names. Example: task.py --name \"experiment_name\" task.py --name \"experiment_name1, experiment_name2\"","title":"task.py name"},{"location":"segmentation/reference/#taskpy-task","text":"type : string or comma-separated list of strings Default: all tasks. Name of the task to launch, or a list of names. Example: task.py --task \"task_name\" task.py --task \"task_name1, task_name2\" task.py --task \"all\"","title":"task.py task"},{"location":"segmentation/reference/#taskpy-num_gpus","text":"type : integer Default: 1 Number of GPUs to use during experiment launch. Example: task.py --num_gpus=1","title":"task.py num_gpus"},{"location":"segmentation/reference/#taskpy-gpus_per_net","text":"type : integer Default: 1 Maximum number of GPUs to use per single experiment. Example: task.py --gpus_per_net=1","title":"task.py gpus_per_net"},{"location":"segmentation/reference/#taskpy-num_workers","text":"type : integer Default: 1 Number of workers to use. Example: task.py --num_workers=1","title":"task.py num_workers"},{"location":"segmentation/reference/#taskpy-allow_resume","text":"type : boolean Default: False Whether to allow resuming of experiments, which will cause unfinished experiments to start from the best saved weights. Example: task.py --allow_resume True","title":"task.py allow_resume"},{"location":"segmentation/reference/#taskpy-force_recalc","text":"type : boolean Default: False Whether to force rebuilding of reports and predictions. Example: task.py --force_recalc True","title":"task.py force_recalc"},{"location":"segmentation/reference/#taskpy-launch_tasks","text":"type : boolean Default: False Whether to launch associated tasks. Example: task.py --launch_tasks True","title":"task.py launch_tasks"},{"location":"segmentation/reference/#taskpy-cache","text":"type : string Path to the cache folder. Cache folder will contain temporary cached data for executed experiments. Example: task.py --cache \"path/to/cache/folder\"","title":"task.py cache"},{"location":"segmentation/reference/#analyze-script-arguments","text":"","title":"analyze script arguments"},{"location":"segmentation/reference/#analyzepy-inputfolder","text":"type : string Folder to search for finished experiments in. Typically, project root. Example: analyze.py --inputFolder \"path/to/project\"","title":"analyze.py inputFolder"},{"location":"segmentation/reference/#analyzepy-output","text":"type : string Default: report.csv in project root. Output report file path. Example: analyze.py --output \"path/to/project/report/report.scv\"","title":"analyze.py output"},{"location":"segmentation/reference/#analyzepy-onlymetric","text":"type : string Name of the single metric to take into account. Example: analyze.py --onlyMetric \"metric_name\"","title":"analyze.py onlyMetric"},{"location":"segmentation/reference/#analyzepy-sortby","text":"type : string Name of the metric to sort result by. Example: analyze.py --sortBy \"metric_name\"","title":"analyze.py sortBy"},{"location":"text/","text":"Text support This package adds text-related datasets, preprocessors and algorithms (like BERT) to Generic pipeline of Musket ML . Installation pip install musket_text This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here .","title":"About"},{"location":"text/#text-support","text":"This package adds text-related datasets, preprocessors and algorithms (like BERT) to Generic pipeline of Musket ML .","title":"Text support"},{"location":"text/#installation","text":"pip install musket_text This package is a part of Musket ML framework, it is recommended to install the whole collection of the framework packages at once using instructions here .","title":"Installation"}]}