<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  
  
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Reference - Musket ML</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700" />

  <link rel="stylesheet" href="../../css/theme.css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" />
  
  <script>
    // Current page data
    var mkdocs_page_name = "Reference";
    var mkdocs_page_input_path = "generic\\reference.md";
    var mkdocs_page_url = null;
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js" defer></script>
  <script src="../../js/modernizr-2.8.3.min.js" defer></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/Python.min.js"></script>
  <script>hljs.initHighlightingOnLoad();</script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Musket ML</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                <p class="caption"><span class="caption-text">Generic</span></p>
                <ul class="current">
                    <li class="toctree-l1"><a class="reference internal" href="../">User guide</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../#reasons-to-use-generic-pipeline">Reasons to use Generic Pipeline</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#project-structure">Project structure</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#launching">Launching</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../#launching-experiments">Launching experiments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../#launching-tasks">Launching tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../#launching-project-analysis">Launching project analysis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#general-train-properties">General train properties</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#definining-networks">Definining networks</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../#built-in-nn-layers">Built-in NN layers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../#control-layers">Control layers</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="../#simple-data-flow-constructions">Simple Data Flow constructions</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../#repeat-and-with">Repeat and With</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../#conditional-layers">Conditional layers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../#shared-weights">Shared Weights</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../#wrapper-layers">Wrapper layers</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../#manually-controlling-data-flow">Manually controlling data flow</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#datasets">Datasets</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#callbacks">Callbacks</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#stages">Stages</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#balancing-your-data">Balancing your data</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#advanced-learning-rates">Advanced learning rates</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../#dynamic-learning-rates">Dynamic learning rates</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../#lr-finder">LR Finder</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#preprocessors">Preprocessors</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../#how-to-check-training-results">How to check training results</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1 current"><a class="reference internal current" href="./">Reference</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#pipeline-root-properties">Pipeline root properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#activation">activation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#aggregation_metric">aggregation_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#architecture">architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#augmentation">augmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#batch">batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#classes">classes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#callbacks">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#copyweights">copyWeights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#clipnorm">clipnorm</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#clipvalue">clipvalue</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dataset">dataset</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#datasets">datasets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dataset_augmenter">dataset_augmenter</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dropout">dropout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#declarations">declarations</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#extra_train_data">extra_train_data</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#folds_count">folds_count</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#freeze_encoder">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#final_metrics">final_metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#holdout">holdout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#imports">imports</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inference_batch">inference_batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#loss">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lr">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#metrics">metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#num_seeds">num_seeds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optimizer">optimizer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#primary_metric">primary_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#primary_metric_mode">primary_metric_mode</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#preprocessing">preprocessing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#random_state">random_state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#stages">stages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#stratified">stratified</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#testsplit">testSplit</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#testsplitseed">testSplitSeed</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#testtimeaugmentation">testTimeAugmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#transforms">transforms</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#validationsplit">validationSplit</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#callback-types">Callback types</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#earlystopping">EarlyStopping</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#reducelronplateau">ReduceLROnPlateau</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cycliclr">CyclicLR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lrvariator">LRVariator</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#tensorboard">TensorBoard</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#layer-types">Layer types</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#input">Input</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#gaussiannoise">GaussianNoise</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dropout_1">Dropout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#spatialdropout1d">SpatialDropout1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lstm">LSTM</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#globalmaxpool1d">GlobalMaxPool1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#globalaveragepooling1d">GlobalAveragePooling1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#batchnormalization">BatchNormalization</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#concatenate">Concatenate</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#add">Add</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#substract">Substract</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#mult">Mult</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#max">Max</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#min">Min</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#conv1d">Conv1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#conv2d">Conv2D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#maxpool1d">MaxPool1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#maxpool2d">MaxPool2D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#averagepooling1d">AveragePooling1D</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cudnnlstm">CuDNNLSTM</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#dense">Dense</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#flatten">Flatten</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#bidirectional">Bidirectional</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#utility-layers">Utility layers</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#split">split</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-concat">split-concat</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-concatenate">split-concatenate</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-add">split-add</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-substract">split-substract</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-mult">split-mult</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-min">split-min</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-max">split-max</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-dot">split-dot</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-dot-normalize">split-dot-normalize</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#seq">seq</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#input_1">input</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#pass">pass</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#transform-concat">transform-concat</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#transform-add">transform-add</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#stage-properties">Stage properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#callbacks_1">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#epochs">epochs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#extra_callbacks">extra_callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#freeze_encoder_1">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#initial_weights">initial_weights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#negatives">negatives</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#loss_1">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#lr_1">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#unfreeze_encoder">unfreeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#validation_negatives">validation_negatives</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#preprocessors">Preprocessors</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#cache">cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#disk-cache">disk-cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-preprocessor">split-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#split-concat-preprocessor">split-concat-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#seq-preprocessor">seq-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#augmentation_1">augmentation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#fit-script-arguments">fit script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-project">fit.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-name">fit.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-num_gpus">fit.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-gpus_per_net">fit.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-num_workers">fit.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-allow_resume">fit.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-force_recalc">fit.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-launch_tasks">fit.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-only_report">fit.py only_report</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-cache">fit.py cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-folds">fit.py folds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#fitpy-time">fit.py time</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#task-script-arguments">task script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-project">task.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-name">task.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-task">task.py task</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-num_gpus">task.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-gpus_per_net">task.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-num_workers">task.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-allow_resume">task.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-force_recalc">task.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-launch_tasks">task.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#taskpy-cache">task.py cache</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#analyze-script-arguments">analyze script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#analyzepy-inputfolder">analyze.py inputFolder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#analyzepy-output">analyze.py output</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#analyzepy-onlymetric">analyze.py onlyMetric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#analyzepy-sortby">analyze.py sortBy</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Segmentation</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../segmentation/">User guide</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#reasons-to-use-segmentation-pipeline">Reasons to use Segmentation Pipeline</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#launching">Launching</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#launching-experiments">Launching experiments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#launching-tasks">Launching tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#launching-project-analysis">Launching project analysis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#usage-guide">Usage guide</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#training-a-model">Training a model</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#general-train-properties">General train properties</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#defining-architecture">Defining architecture</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#image-and-mask-augmentations">Image and Mask Augmentations</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#freezing-and-unfreezing-encoder">Freezing and Unfreezing encoder</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#custom-datasets">Custom datasets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#multistage-training">Multistage training</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#balancing-your-data">Balancing your data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#advanced-learning-rates">Advanced learning rates</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#training-on-crops">Training on crops</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#using-trained-model">Using trained model</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="../../segmentation/#ensembling-predictions">Ensembling predictions</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#custom-evaluation-code">Custom evaluation code</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/#accessing-model">Accessing model</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#analyzing-experiments-results">Analyzing experiments results</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#what-is-supported">What is supported?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#custom-architectures-callbacks-metrics">Custom architectures, callbacks, metrics</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/#examples">Examples</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../segmentation/reference/">Reference</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#pipeline-root-properties">Pipeline root properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#activation">activation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#aggregation_metric">aggregation_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#architecture">architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#augmentation">augmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#backbone">backbone</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#batch">batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#classifier">classifier</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#classifier_lr">classifier_lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#classes">classes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#callbacks">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#compresspredictionsasints">compressPredictionsAsInts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#copyweights">copyWeights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#clipnorm">clipnorm</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#clipvalue">clipvalue</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#crops">crops</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#dataset">dataset</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#datasets">datasets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#dataset_augmenter">dataset_augmenter</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#dropout">dropout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#encoder_weights">encoder_weights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#extra_train_data">extra_train_data</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#folds_count">folds_count</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#freeze_encoder">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#final_metrics">final_metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#holdout">holdout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#imports">imports</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#inference_batch">inference_batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#loss">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#lr">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#manualresize">manualResize</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#metrics">metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#num_seeds">num_seeds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#optimizer">optimizer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#primary_metric">primary_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#primary_metric_mode">primary_metric_mode</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#preprocessing">preprocessing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#random_state">random_state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#shape">shape</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#stages">stages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#stratified">stratified</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#testsplit">testSplit</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#testsplitseed">testSplitSeed</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#testtimeaugmentation">testTimeAugmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#transforms">transforms</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#validationsplit">validationSplit</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#callback-types">Callback types</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#earlystopping">EarlyStopping</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#reducelronplateau">ReduceLROnPlateau</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#cycliclr">CyclicLR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#lrvariator">LRVariator</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#tensorboard">TensorBoard</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#stage-properties">Stage properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#callbacks_1">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#epochs">epochs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#extra_callbacks">extra_callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#freeze_encoder_1">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#initial_weights">initial_weights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#negatives">negatives</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#loss_1">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#lr_1">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#unfreeze_encoder">unfreeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#validation_negatives">validation_negatives</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#preprocessors">Preprocessors</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#cache">cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#disk-cache">disk-cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#split-preprocessor">split-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#split-concat-preprocessor">split-concat-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#seq-preprocessor">seq-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#augmentation_1">augmentation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#fit-script-arguments">fit script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-project">fit.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-name">fit.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-num_gpus">fit.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-gpus_per_net">fit.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-num_workers">fit.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-allow_resume">fit.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-force_recalc">fit.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-launch_tasks">fit.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-only_report">fit.py only_report</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-cache">fit.py cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-folds">fit.py folds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#fitpy-time">fit.py time</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#task-script-arguments">task script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-project">task.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-name">task.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-task">task.py task</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-num_gpus">task.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-gpus_per_net">task.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-num_workers">task.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-allow_resume">task.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-force_recalc">task.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-launch_tasks">task.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#taskpy-cache">task.py cache</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../segmentation/reference/#analyze-script-arguments">analyze script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#analyzepy-inputfolder">analyze.py inputFolder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#analyzepy-output">analyze.py output</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#analyzepy-onlymetric">analyze.py onlyMetric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../segmentation/reference/#analyzepy-sortby">analyze.py sortBy</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Classification</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../classification/">User guide</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#reasons-to-use-classification-pipeline">Reasons to use Classification Pipeline</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#launching">Launching</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#launching-experiments">Launching experiments</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#launching-tasks">Launching tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#launching-project-analysis">Launching project analysis</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#usage-guide">Usage guide</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#training-a-model">Training a model</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#general-train-properties">General train properties</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#defining-architecture">Defining architecture</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#image-augmentations">Image Augmentations</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#freezing-and-unfreezing-encoder">Freezing and Unfreezing encoder</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#custom-datasets">Custom datasets</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#multistage-training">Multistage training</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#balancing-your-data">Balancing your data</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#advanced-learning-rates">Advanced learning rates</a>
    </li>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#training-on-crops">Training on crops</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#using-trained-model">Using trained model</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="../../classification/#ensembling-predictions">Ensembling predictions</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#custom-evaluation-code">Custom evaluation code</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/#accessing-model">Accessing model</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#analyzing-experiments-results">Analyzing experiments results</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#what-is-supported">What is supported?</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/#custom-architectures-callbacks-metrics">Custom architectures, callbacks, metrics</a>
    </li>
    </ul>
                    </li>
                    <li class="toctree-l1"><a class="reference internal" href="../../classification/reference/">Reference</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#pipeline-root-properties">Pipeline root properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#activation">activation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#aggregation_metric">aggregation_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#architecture">architecture</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#augmentation">augmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#batch">batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#classes">classes</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#callbacks">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#copyweights">copyWeights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#clipnorm">clipnorm</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#clipvalue">clipvalue</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#crops">crops</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#dataset">dataset</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#datasets">datasets</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#dataset_augmenter">dataset_augmenter</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#dropout">dropout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#extra_train_data">extra_train_data</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#folds_count">folds_count</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#freeze_encoder">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#final_metrics">final_metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#holdout">holdout</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#imports">imports</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#inference_batch">inference_batch</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#loss">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#lr">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#manualresize">manualResize</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#metrics">metrics</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#num_seeds">num_seeds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#optimizer">optimizer</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#primary_metric">primary_metric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#primary_metric_mode">primary_metric_mode</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#preprocessing">preprocessing</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#random_state">random_state</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#shape">shape</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#stages">stages</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#stratified">stratified</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#testsplit">testSplit</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#testsplitseed">testSplitSeed</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#testtimeaugmentation">testTimeAugmentation</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#transforms">transforms</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#validationsplit">validationSplit</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#weights">weights</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#callback-types">Callback types</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#earlystopping">EarlyStopping</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#reducelronplateau">ReduceLROnPlateau</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#cycliclr">CyclicLR</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#lrvariator">LRVariator</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#tensorboard">TensorBoard</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#stage-properties">Stage properties</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#callbacks_1">callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#epochs">epochs</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#extra_callbacks">extra_callbacks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#freeze_encoder_1">freeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#initial_weights">initial_weights</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#negatives">negatives</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#loss_1">loss</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#lr_1">lr</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#unfreeze_encoder">unfreeze_encoder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#validation_negatives">validation_negatives</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#preprocessors">Preprocessors</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#cache">cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#disk-cache">disk-cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#split-preprocessor">split-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#split-concat-preprocessor">split-concat-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#seq-preprocessor">seq-preprocessor</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#augmentation_1">augmentation</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#fit-script-arguments">fit script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-project">fit.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-name">fit.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-num_gpus">fit.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-gpus_per_net">fit.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-num_workers">fit.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-allow_resume">fit.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-force_recalc">fit.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-launch_tasks">fit.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-only_report">fit.py only_report</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-cache">fit.py cache</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-folds">fit.py folds</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#fitpy-time">fit.py time</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#task-script-arguments">task script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-project">task.py project</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-name">task.py name</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-task">task.py task</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-num_gpus">task.py num_gpus</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-gpus_per_net">task.py gpus_per_net</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-num_workers">task.py num_workers</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-allow_resume">task.py allow_resume</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-force_recalc">task.py force_recalc</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-launch_tasks">task.py launch_tasks</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#taskpy-cache">task.py cache</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../classification/reference/#analyze-script-arguments">analyze script arguments</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#analyzepy-inputfolder">analyze.py inputFolder</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#analyzepy-output">analyze.py output</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#analyzepy-onlymetric">analyze.py onlyMetric</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../classification/reference/#analyzepy-sortby">analyze.py sortBy</a>
    </li>
        </ul>
    </li>
    </ul>
                    </li>
                </ul>
                <p class="caption"><span class="caption-text">Musket IDE</span></p>
                <ul>
                    <li class="toctree-l1"><a class="reference internal" href="../../ide/getting_started/">Setup</a>
    <ul>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#download">Download</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#installation">Installation</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#setting-up-a-project">Setting up a project</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#getting-dataset-from-kaggle">Getting dataset from Kaggle</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="../../ide/getting_started/#installing-kaggle-stuff">Installing kaggle stuff</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="../../ide/getting_started/#downloading-the-dataset">Downloading the dataset</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#creating-an-experiment">Creating an experiment</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#running-an-experiment">Running an experiment</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="../../ide/getting_started/#what-is-next">What is next</a>
    </li>
    </ul>
                    </li>
                </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Musket ML</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Generic &raquo;</li>
        
      
    
    <li>Reference</li>
    <li class="wy-breadcrumbs-aside">
      
    </li>
  </ul>
  
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="generic-pipeline-reference">Generic pipeline reference</h1>
<h2 id="pipeline-root-properties">Pipeline root properties</h2>
<h3 id="activation">activation</h3>
<p>TODO: does it have any use in the root of the file?</p>
<h3 id="aggregation_metric">aggregation_metric</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Metric to calculate against the combination of all stages and report in <code>allStages</code> section of summary.yaml file after all experiment instances are finished.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively. </p>
<p>Example:</p>
<pre><code class="yaml">aggregation_metric: matthews_correlation_holdout

</code></pre>

<h3 id="architecture">architecture</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Name of the <a href="#declarations">declaration</a> that will be used as an entry point or root of the main network. </p>
<p>Example:</p>
<pre><code class="yaml">declarations: 
   utilityDeclaration1:
   utilityDeclaration2:
   mainNetwork:
       - utilityDeclaration1: []
       - dense: [1,&quot;sigmoid&quot;]

architecture: mainNetwork

</code></pre>

<h3 id="augmentation">augmentation</h3>
<p><strong>type</strong>: ```` </p>
<p>TODO: isnt that a property from segmentation/classification, not generic pipeline?</p>
<p><strong>type</strong>: <code>complex</code> </p>
<p><a href="https://imgaug.readthedocs.io">IMGAUG</a> transformations sequence.
Each object is mapped on <a href="https://imgaug.readthedocs.io">IMGAUG</a> transformer by name, parameters are mapped too.</p>
<p>Example:</p>
<pre><code class="yaml">augmentation:
 Fliplr: 0.5
 Affine:
   translate_px:
     x:
       - -50
       - +50
     y:
       - -50
       - +50
</code></pre>

<p>In this example, <code>Fliplr</code> key is automatically mapped on <a href="https://imgaug.readthedocs.io/en/latest/source/api_augmenters_flip.html">Fliplr agugmenter</a>,
their <code>0.5</code> parameter is mapped on the first <code>p</code> parameter of the augmenter.
Named parameters are also mapped, in example <code>translate_px</code> key of <code>Affine</code> is mapped on <code>translate_px</code> parameter of <a href="https://imgaug.readthedocs.io/en/latest/source/augmenters.html?highlight=affine#affine">Affine augmenter</a>.</p>
<h3 id="batch">batch</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Sets up training batch size.</p>
<p>Example:</p>
<pre><code class="yaml">batch: 512
</code></pre>

<h3 id="classes">classes</h3>
<p><strong>type</strong>: ```` </p>
<p>TODO: does it have any use in generic pipeline?</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="callbacks">callbacks</h3>
<p><strong>type</strong>: <code>array of callback instances</code> </p>
<p>Sets up training-time callbacks. See individual <a href="#callback-types">callback descriptions</a>.</p>
<p>Example:</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h3 id="copyweights">copyWeights</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to copy saved weights.</p>
<p>Example:</p>
<pre><code class="yaml">copyWeights: true
</code></pre>

<h3 id="clipnorm">clipnorm</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>Maximum clip norm of a gradient for an optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">clipnorm: 1.0
</code></pre>

<h3 id="clipvalue">clipvalue</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>Clip value of a gradient for an optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">clipvalue: 0.5
</code></pre>

<h3 id="dataset">dataset</h3>
<p><strong>type</strong>: <code>complex object</code> </p>
<p>Key is a name of the python function in scope, which returns training data set.
Value is an array of parameters to pass to a function.</p>
<p>Example:</p>
<pre><code class="yaml">dataset:
  getTrain: [false,false]
</code></pre>

<h3 id="datasets">datasets</h3>
<p><strong>type</strong>: <code>map containing complex objects</code> </p>
<p>Sets up a list of available data sets to be referred by other entities.</p>
<p>For each object, key is a name of the python function in scope, which returns training dataset.
Value is an array of parameters to pass to a function.</p>
<p>Example:</p>
<pre><code class="yaml">datasets:
  test:
    getTest: [false,false]
</code></pre>

<h3 id="dataset_augmenter">dataset_augmenter</h3>
<p><strong>type</strong>: <code>complex object</code> </p>
<p>Sets up a custom augmenter function to be applied to a dataset.
Object must have a name property, whic will be used as a name of the python function in scope.
Other object properties are mapped as function arguments.
TODO: check that description and example are correct?</p>
<p>Example:</p>
<pre><code class="yaml">dataset_augmenter:
    name: TheAugmenter
    parameter: test
</code></pre>

<h3 id="dropout">dropout</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>TODO: does it have any use in generic pipeline root?</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="declarations">declarations</h3>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Sets up network layer building blocks. </p>
<p>Each declaration is an object with a key setting up declaration name
and value being a complex object containing <code>parameters</code> array listing 
this layer parameters and <code>body</code> containing an array of sub-layers or control statements,</p>
<p>If layer has no parameters, <code>parameters</code> property may be ommitted and <code>body</code> contents may 
come directly inside layer definition.</p>
<p>See <a href="#layer-types">Layer types</a> for details regarding building blocks.</p>
<p>Example:</p>
<pre><code class="yaml">declarations: 
   lstm2: 
      parameters: [count]
      body:
       - bidirectional:  
           - cuDNNLSTM: [count, true]
       - bidirectional:    
           - cuDNNLSTM: [count/2, false]
   net:
       - split-concat: 
          - word_indexes_embedding:  [ embeddings/glove.840B.300d.txt ]
          - word_indexes_embedding:  [ embeddings/paragram_300_sl999.txt ]
          - word_indexes_embedding:  [ embeddings/wiki-news-300d-1M.vec]
       - gaussianNoise: 0.05   
       - lstm2: [300]
       #- dropout: 0.5
       - dense: [1,&quot;sigmoid&quot;]
</code></pre>

<h3 id="extra_train_data">extra_train_data</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Name of the additional dataset that will be added (per element) to the training dataset before train launching.
TODO is that correct?</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="folds_count">folds_count</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of folds to train. Default is 5.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="freeze_encoder">freeze_encoder</h3>
<p>TODO isnt it for other pipelines?
<strong>type</strong>: <code>boolean</code> </p>
<p>Whether to freeze encoder during the training process.</p>
<p>Example:</p>
<pre><code class="yaml">freeze_encoder: true
stages:
  - epochs: 10 #Let's go for 10 epochs with frozen encoder

  - epochs: 100 #Now let's go for 100 epochs with trainable encoder
    unfreeze_encoder: true  
</code></pre>

<h3 id="final_metrics">final_metrics</h3>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Metrics to calculate against every stage and report in <code>stages</code> section of summary.yaml file after all experiment instances are finished.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">final_metrics: [measure]

</code></pre>

<h3 id="holdout">holdout</h3>
<p><strong>type</strong>: ```` </p>
<p>TODO what is this?</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="imports">imports</h3>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Imports python files from <code>modules</code> folder of the project and make their properly annotated contents to be available to be referred from YAML.</p>
<p>Example:</p>
<pre><code class="yaml">imports: [ layers, preprocessors ]

</code></pre>

<p>this will import <code>layers.py</code> and <code>preprocessors.py</code></p>
<h3 id="inference_batch">inference_batch</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Size of batch during inferring process.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="loss">loss</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the loss name.</p>
<p>Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules.</p>
<p>Example:</p>
<pre><code class="yaml">loss: binary_crossentropy
</code></pre>

<h3 id="lr">lr</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>Learning rate.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="metrics">metrics</h3>
<p><strong>type</strong>: <code>array of strings</code> </p>
<p>Array of metrics to track during the training process. Metric calculation results will be printed in the console and to <code>metrics</code> folder of the experiment.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">metrics: #We would like to track some metrics
  - binary_accuracy
  - binary_crossentropy
  - matthews_correlation

</code></pre>

<h3 id="num_seeds">num_seeds</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>If set, training process (for all folds) will be executed <code>num_seeds</code> times, each time resetting the random seeds.
Respective folders (like <code>metrics</code>) will obtain subfolders <code>0</code>, <code>1</code> etc... for each seed.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="optimizer">optimizer</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the optimizer.</p>
<p>Example:</p>
<pre><code class="yaml">optimizer: Adam
</code></pre>

<h3 id="primary_metric">primary_metric</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Metric to track during the training process. Metric calculation results will be printed in the console and to <code>metrics</code> folder of the experiment.</p>
<p>Besides tracking, this metric will be also used by default for metric-related activity, in example, for decision regarding which epoch results are better.</p>
<p>Uses metric name detection mechanism to search for the built-in metric or for a custom function with the same name across project modules.</p>
<p>Metric name may have <code>val_</code> prefix or <code>_holdout</code> postfix to indicate calculation against validation or holdout, respectively.</p>
<p>Example:</p>
<pre><code class="yaml">primary_metric: val_macro_f1
</code></pre>

<h3 id="primary_metric_mode">primary_metric_mode</h3>
<p><strong>type</strong>: <code>enum: auto,min,max</code> </p>
<p><strong>default</strong>: <code>auto</code> </p>
<p>In case of a usage of a primary metrics calculation results across several instances (i.e. batches), this will be a mathematical operation to find a final result.</p>
<p>Example:</p>
<pre><code class="yaml">primary_metric_mode: max
</code></pre>

<h3 id="preprocessing">preprocessing</h3>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Preprocessors are the custom python functions that transform dataset. </p>
<p>Such functions should be defined in python files that are in a project scope (<code>modules</code>) folder and imported.
Preprocessing functions should be also marked with <code>@preprocessing.dataset_preprocessor</code> annotation.</p>
<p><code>preprocessing</code> instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments.</p>
<p><a href="#preprocessors">Preprocessors</a> contain some of the preprocessor utility instructions.</p>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h3 id="random_state">random_state</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>The seed of randomness.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="stages">stages</h3>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Sets up training process stages. 
Contains YAML array of stages, where each stage is a complex type that may contain properties described in the <a href="#stage-properties">Stage properties</a> section.  </p>
<p>Example:</p>
<pre><code class="yaml">stages:
  - epochs: 6
  - epochs: 6
    lr: 0.01

</code></pre>

<h3 id="stratified">stratified</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to use stratified strategy when splitting training set.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="testsplit">testSplit</h3>
<p><strong>type</strong>: <code>float 0-1</code> </p>
<p>Splits the train set into two parts, using one part for train and leaving the other untouched for a later testing.
The split is shuffled.</p>
<p>Example:</p>
<pre><code class="yaml">testSplit: 0.4
</code></pre>

<h3 id="testsplitseed">testSplitSeed</h3>
<p><strong>type</strong>: ```` </p>
<p>Seed of randomness for the split of the training set.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="testtimeaugmentation">testTimeAugmentation</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Test-time augumentation function name.
Function must be reachable on project scope, accept and return numpy array.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="transforms">transforms</h3>
<p><strong>type</strong>: <code>complex</code> </p>
<p>TODO is that true? 
If yes, why are we having pure IMGAUG in generic called just "transforms", maybe we should call it "imageTransforms" or simply "imgaug". 
Btw, isnt it crossing with preprocessing, maybe we should just create "imgaug" preprocessor with all these goodies inside? </p>
<p><a href="https://imgaug.readthedocs.io">IMGAUG</a> transformations sequence.
Each object is mapped on <a href="https://imgaug.readthedocs.io">IMGAUG</a> transformer by name, parameters are mapped too.</p>
<p>Example:</p>
<pre><code class="yaml">transforms:
 Fliplr: 0.5
 Affine:
   translate_px:
     x:
       - -50
       - +50
     y:
       - -50
       - +50
</code></pre>

<h3 id="validationsplit">validationSplit</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>Float 0-1 setting up how much of the training set (after holdout is already cut off) to allocate for validation.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="callback-types">Callback types</h2>
<h3 id="earlystopping">EarlyStopping</h3>
<p>Stop training when a monitored metric has stopped improving.</p>
<p>Properties:</p>
<ul>
<li><strong>patience</strong> - integer, number of epochs with no improvement after which training will be stopped.</li>
<li><strong>verbose</strong> - 0 or 1, verbosity mode.</li>
<li><strong>monitor</strong> - string, name of the metric to monitor</li>
<li><strong>mode</strong> - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity. </li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
</code></pre>

<h3 id="reducelronplateau">ReduceLROnPlateau</h3>
<p>Reduce learning rate when a metric has stopped improving.</p>
<p>Properties:</p>
<ul>
<li><strong>patience</strong> - integer, number of epochs with no improvement after which training will be stopped.</li>
<li><strong>cooldown</strong> - integer, number of epochs to wait before resuming normal operation after lr has been reduced.</li>
<li><strong>factor</strong> - number, factor by which the learning rate will be reduced. new_lr = lr * factor</li>
<li><strong>verbose</strong> - 0 or 1, verbosity mode.</li>
<li><strong>monitor</strong> - string, name of the metric to monitor</li>
<li><strong>mode</strong> - auto, min or max; In min mode, training will stop when the quantity monitored has stopped decreasing; in max mode it will stop when the quantity monitored has stopped increasing; in auto mode, the direction is automatically inferred from the name of the monitored quantity.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h3 id="cycliclr">CyclicLR</h3>
<p>Cycles learning rate across epochs.</p>
<p>Functionally, it defines the cycle amplitude (max_lr - base_lr).
The lr at any cycle is the sum of base_lr
and some scaling of the amplitude; therefore
max_lr may not actually be reached depending on
scaling function.</p>
<p>Properties:</p>
<ul>
<li><strong>base_lr</strong> - number, initial learning rate which is the lower boundary in the cycle.</li>
<li><strong>max_lr</strong> - number, upper boundary in the cycle.</li>
<li><strong>mode</strong> - one of <code>triangular</code>, <code>triangular2</code> or <code>exp_range</code>; scaling function.</li>
<li><strong>gamma</strong> - number from 0 to 1, constant in 'exp_range' scaling function.</li>
<li><strong>step_size</strong> - integer &gt; 0, number of training iterations (batches) per half cycle.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  CyclicLR:
    base_lr: 0.001
    max_lr: 0.006
    step_size: 2000
    mode: triangular
</code></pre>

<h3 id="lrvariator">LRVariator</h3>
<p>Changes learning rate between two values</p>
<p>Properties:</p>
<ul>
<li><strong>fromVal</strong> - initial learning rate value, defaults to the configuration LR setup.</li>
<li><strong>toVal</strong> - final learning value.</li>
<li><strong>style</strong> - one of the following:</li>
<li><strong>linear</strong> - changes LR linearly between two values.</li>
<li><strong>const</strong> - does not change from initial value.</li>
<li><strong>cos+</strong> - <code>-1 * cos(2x/pi) + 1 for x in [0;1]</code></li>
<li><strong>cos-</strong> - <code>cos(2x/pi) for x in [0;1]</code></li>
<li><strong>cos</strong> - same as 'cos-'</li>
<li><strong>sin+</strong> - <code>sin(2x/pi) x in [0;1]</code></li>
<li><strong>sin-</strong> - <code>-1 * sin(2x/pi) + 1 for x in [0;1]</code></li>
<li><strong>sin</strong> - same as 'sin+'</li>
<li><strong>any positive float or integer value</strong> - x^a for x in [0;1]</li>
</ul>
<p>TODO: examples from lr_variation_callback.py look strange,
also it is unclear how to number of steps is being set up.
Example</p>
<pre><code class="yaml"></code></pre>

<h3 id="tensorboard">TensorBoard</h3>
<p>This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, as well as activation histograms for the different layers in your model.</p>
<p>Properties:</p>
<ul>
<li><strong>log_dir</strong> - string; the path of the directory where to save the log files to be parsed by TensorBoard.</li>
<li><strong>histogram_freq</strong> - integer; frequency (in epochs) at which to compute activation and weight histograms for the layers of the model. If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations.</li>
<li><strong>batch_size</strong> - integer; size of batch of inputs to feed to the network for histograms computation.</li>
<li><strong>write_graph</strong> - boolean; whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True.</li>
<li><strong>write_grads</strong> - boolean; whether to visualize gradient histograms in TensorBoard.  histogram_freq must be greater than 0.</li>
<li><strong>write_images</strong> - boolean; whether to write model weights to visualize as image in TensorBoard.</li>
<li><strong>embeddings_freq</strong> - number; frequency (in epochs) at which selected embedding layers will be saved. If set to 0, embeddings won't be computed. Data to be visualized in TensorBoard's Embedding tab must be passed as embeddings_data.</li>
<li><strong>embeddings_layer_names</strong> - array of strings; a list of names of layers to keep eye on. If None or empty list all the embedding layer will be watched.</li>
<li><strong>embeddings_metadata</strong> - a dictionary which maps layer name to a file name in which metadata for this embedding layer is saved. See the details about metadata files format. In case if the same metadata file is used for all embedding layers, string can be passed.</li>
<li><strong>embeddings_data</strong> -  data to be embedded at layers specified in  embeddings_layer_names. </li>
<li><strong>update_freq</strong> - <code>epoch</code> or <code>batch</code> or integer; When using 'batch', writes the losses and metrics to TensorBoard after each batch. The same applies for 'epoch'. If using an integer, let's say 10000, the callback will write the metrics and losses to TensorBoard every 10000 samples. Note that writing too frequently to TensorBoard can slow down your training.</li>
</ul>
<p>Example</p>
<pre><code class="yaml">callbacks:
  TensorBoard:
    log_dir: './logs'
    batch_size: 32
    write_graph: True
    update_freq: batch
</code></pre>

<h2 id="layer-types">Layer types</h2>
<h3 id="input">Input</h3>
<p>TODO: description</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>shape</strong> - array of integers; input shape</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="gaussiannoise">GaussianNoise</h3>
<p>Apply additive zero-centered Gaussian noise.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>stddev</strong> - float; standard deviation of the noise distribution.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="dropout_1">Dropout</h3>
<p>Applies Dropout to the input.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>rate</strong> - float; float between 0 and 1. Fraction of the input units to drop.</li>
<li><strong>seed</strong> - integer; integer to use as random seed</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">declarations:
  net:
    - dropout: 0.5
</code></pre>

<h3 id="spatialdropout1d">SpatialDropout1D</h3>
<p>Spatial 1D version of Dropout.</p>
<p>This version performs the same function as Dropout, however it drops entire 1D feature maps instead of individual elements. If adjacent frames within feature maps are strongly correlated (as is normally the case in early convolution layers) then regular dropout will not regularize the activations and will otherwise just result in an effective learning rate decrease. In this case, SpatialDropout1D will help promote independence between feature maps and should be used instead.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>rate</strong> - float between 0 and 1. Fraction of the input units to drop.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="lstm">LSTM</h3>
<p>Long Short-Term Memory layer</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>
<li><strong>activation</strong>: Activation function to use
    (see <a href="https://keras.io/activations/">activations</a>).
    Default: hyperbolic tangent (<code>tanh</code>).
    If you pass <code>None</code>, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>recurrent_activation</strong>: Activation function to use
    for the recurrent step
    (see <a href="https://keras.io/activations/">activations</a>).
    Default: hard sigmoid (<code>hard_sigmoid</code>).
    If you pass <code>None</code>, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,
    used for the linear transformation of the inputs.
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>
    weights matrix,
    used for the linear transformation of the recurrent state.
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>unit_forget_bias</strong>: Boolean.
    If True, add 1 to the bias of the forget gate at initialization.
    Setting it to true will also force <code>bias_initializer="zeros"</code>.
    This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al. (2015)</a>.</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>recurrent_regularizer</strong>: Regularizer function applied to
    the <code>recurrent_kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>recurrent_constraint</strong>: Constraint function applied to
    the <code>recurrent_kernel</code> weights matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>dropout</strong>: Float between 0 and 1.
    Fraction of the units to drop for
    the linear transformation of the inputs.</li>
<li><strong>recurrent_dropout</strong>: Float between 0 and 1.
    Fraction of the units to drop for
    the linear transformation of the recurrent state.</li>
<li><strong>implementation</strong>: Implementation mode, either 1 or 2.
    Mode 1 will structure its operations as a larger number of
    smaller dot products and additions, whereas mode 2 will
    batch them into fewer, larger operations. These modes will
    have different performance profiles on different hardware and
    for different applications.</li>
<li><strong>return_sequences</strong>: Boolean. Whether to return the last output
    in the output sequence, or the full sequence.</li>
<li><strong>return_state</strong>: Boolean. Whether to return the last state
    in addition to the output. The returned elements of the
    states list are the hidden state and the cell state, respectively.</li>
<li><strong>go_backwards</strong>: Boolean (default False).
    If True, process the input sequence backwards and return the
    reversed sequence.</li>
<li><strong>stateful</strong>: Boolean (default False). If True, the last state
    for each sample at index i in a batch will be used as initial
    state for the sample of index i in the following batch.</li>
<li><strong>unroll</strong>: Boolean (default False).
    If True, the network will be unrolled,
    else a symbolic loop will be used.
    Unrolling can speed-up a RNN,
    although it tends to be more memory-intensive.
    Unrolling is only suitable for short sequences.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="globalmaxpool1d">GlobalMaxPool1D</h3>
<p>Global max pooling operation for temporal data.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>data_format</strong> - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs.  channels_last corresponds to inputs with shape  (batch, steps, features) while channels_first corresponds to inputs with shape  (batch, features, steps).</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="globalaveragepooling1d">GlobalAveragePooling1D</h3>
<p>Global average pooling operation for temporal data.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>data_format</strong> - A string, one of channels_last (default) or channels_first. The ordering of the dimensions in the inputs.  channels_last corresponds to inputs with shape  (batch, steps, features) while channels_first corresponds to inputs with shape  (batch, features, steps).</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="batchnormalization">BatchNormalization</h3>
<p>Batch normalization layer.</p>
<p>Normalize the activations of the previous layer at each batch,
i.e. applies a transformation that maintains the mean activation
close to 0 and the activation standard deviation close to 1.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>axis</strong>: Integer, the axis that should be normalized
    (typically the features axis).
    For instance, after a <code>Conv2D</code> layer with
    <code>data_format="channels_first"</code>,
    set <code>axis=1</code> in <code>BatchNormalization</code>.</li>
<li><strong>momentum</strong>: Momentum for the moving mean and the moving variance.</li>
<li><strong>epsilon</strong>: Small float added to variance to avoid dividing by zero.</li>
<li><strong>center</strong>: If True, add offset of <code>beta</code> to normalized tensor.
    If False, <code>beta</code> is ignored.</li>
<li><strong>scale</strong>: If True, multiply by <code>gamma</code>.
    If False, <code>gamma</code> is not used.
    When the next layer is linear (also e.g. <code>nn.relu</code>),
    this can be disabled since the scaling
    will be done by the next layer.</li>
<li><strong>beta_initializer</strong>: Initializer for the beta weight.</li>
<li><strong>gamma_initializer</strong>: Initializer for the gamma weight.</li>
<li><strong>moving_mean_initializer</strong>: Initializer for the moving mean.</li>
<li><strong>moving_variance_initializer</strong>: Initializer for the moving variance.</li>
<li><strong>beta_regularizer</strong>: Optional regularizer for the beta weight.</li>
<li><strong>gamma_regularizer</strong>: Optional regularizer for the gamma weight.</li>
<li><strong>beta_constraint</strong>: Optional constraint for the beta weight.</li>
<li><strong>gamma_constraint</strong>: Optional constraint for the gamma weight.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="concatenate">Concatenate</h3>
<p>Layer that concatenates a list of inputs.</p>
<p>Example:</p>
<pre><code class="yaml">- concatenate: [lstmBranch,textFeatureBranch]
</code></pre>

<h3 id="add">Add</h3>
<p>Layer that adds a list of inputs.</p>
<p>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</p>
<p>Example:</p>
<pre><code class="yaml">- add: [first,second]
</code></pre>

<h3 id="substract">Substract</h3>
<p>ayer that subtracts two inputs.</p>
<p>It takes as input a list of tensors of size 2, both of the same shape, and returns a single tensor, (inputs[0] - inputs[1]), also of the same shape.</p>
<p>Example:</p>
<pre><code class="yaml">- substract: [first,second]
</code></pre>

<h3 id="mult">Mult</h3>
<p>Layer that multiplies (element-wise) a list of inputs.</p>
<p>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</p>
<p>Example:</p>
<pre><code class="yaml">- mult: [first,second]
</code></pre>

<h3 id="max">Max</h3>
<p>Layer that computes the maximum (element-wise) a list of inputs.</p>
<p>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</p>
<p>Example:</p>
<pre><code class="yaml">- max: [first,second]
</code></pre>

<h3 id="min">Min</h3>
<p>Layer that computes the minimum (element-wise) a list of inputs.</p>
<p>It takes as input a list of tensors, all of the same shape, and returns a single tensor (also of the same shape).</p>
<p>Example:</p>
<pre><code class="yaml">- min: [first,second]
</code></pre>

<h3 id="conv1d">Conv1D</h3>
<p>1D convolution layer (e.g. temporal convolution).</p>
<p>This layer creates a convolution kernel that is convolved with the layer input over a single spatial (or temporal) dimension to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.</p>
<p>When using this layer as the first layer in a model, provide an input_shape argument (tuple of integers or None, does not include the batch axis), e.g. input_shape=(10, 128) for time series sequences of 10 time steps with 128 features per step in data_format="channels_last", or (None, 128) for variable-length sequences with 128 features per step.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>filters</strong>: Integer, the dimensionality of the output space
    (i.e. the number of output filters in the convolution).</li>
<li><strong>kernel_size</strong>: An integer or tuple/list of a single integer,
    specifying the length of the 1D convolution window.</li>
<li><strong>strides</strong>: An integer or tuple/list of a single integer,
    specifying the stride length of the convolution.
    Specifying any stride value != 1 is incompatible with specifying
    any <code>dilation_rate</code> value != 1.</li>
<li><strong>padding</strong>: One of <code>"valid"</code>, <code>"causal"</code> or <code>"same"</code> (case-insensitive).
    <code>"valid"</code> means "no padding".
    <code>"same"</code> results in padding the input such that
    the output has the same length as the original input.
    <code>"causal"</code> results in causal (dilated) convolutions,
    e.g. <code>output[t]</code> does not depend on <code>input[t + 1:]</code>.
    A zero padding is used such that
    the output has the same length as the original input.
    Useful when modeling temporal data where the model
    should not violate the temporal order. See
    <a href="https://arxiv.org/abs/1609.03499">WaveNet: A Generative Model for Raw Audio, section 2.1</a>.</li>
<li><strong>data_format</strong>: A string,
    one of <code>"channels_last"</code> (default) or <code>"channels_first"</code>.
    The ordering of the dimensions in the inputs.
    <code>"channels_last"</code> corresponds to inputs with shape
    <code>(batch, steps, channels)</code>
    (default format for temporal data in Keras)
    while <code>"channels_first"</code> corresponds to inputs
    with shape <code>(batch, channels, steps)</code>.</li>
<li><strong>dilation_rate</strong>: an integer or tuple/list of a single integer, specifying
    the dilation rate to use for dilated convolution.
    Currently, specifying any <code>dilation_rate</code> value != 1 is
    incompatible with specifying any <code>strides</code> value != 1.</li>
<li><strong>activation</strong>: Activation function to use
    (see <a href="https://keras.io/activations/">activations</a>).
    If you don't specify anything, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to the kernel matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="conv2d">Conv2D</h3>
<p>2D convolution layer (e.g. spatial convolution over images).</p>
<p>This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. If use_bias is True, a bias vector is created and added to the outputs. Finally, if activation is not None, it is applied to the outputs as well.</p>
<p>When using this layer as the first layer in a model, provide the keyword argument input_shape (tuple of integers, does not include the batch axis), e.g. input_shape=(128, 128, 3) for 128x128 RGB pictures in data_format="channels_last".</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>filters</strong>: Integer, the dimensionality of the output space
    (i.e. the number of output filters in the convolution).</li>
<li><strong>kernel_size</strong>: An integer or tuple/list of 2 integers, specifying the
    height and width of the 2D convolution window.
    Can be a single integer to specify the same value for
    all spatial dimensions.</li>
<li><strong>strides</strong>: An integer or tuple/list of 2 integers,
    specifying the strides of the convolution
    along the height and width.
    Can be a single integer to specify the same value for
    all spatial dimensions.
    Specifying any stride value != 1 is incompatible with specifying
    any <code>dilation_rate</code> value != 1.</li>
<li><strong>padding</strong>: one of <code>"valid"</code> or <code>"same"</code> (case-insensitive).
    Note that <code>"same"</code> is slightly inconsistent across backends with
    <code>strides</code> != 1, as described
    <a href="https://github.com/keras-team/keras/pull/9473#issuecomment-372166860">here</a></li>
<li><strong>data_format</strong>: A string,
    one of <code>"channels_last"</code> or <code>"channels_first"</code>.
    The ordering of the dimensions in the inputs.
    <code>"channels_last"</code> corresponds to inputs with shape
    <code>(batch, height, width, channels)</code> while <code>"channels_first"</code>
    corresponds to inputs with shape
    <code>(batch, channels, height, width)</code>.
    It defaults to the <code>image_data_format</code> value found in your
    Keras config file at <code>~/.keras/keras.json</code>.
    If you never set it, then it will be "channels_last".</li>
<li><strong>dilation_rate</strong>: an integer or tuple/list of 2 integers, specifying
    the dilation rate to use for dilated convolution.
    Can be a single integer to specify the same value for
    all spatial dimensions.
    Currently, specifying any <code>dilation_rate</code> value != 1 is
    incompatible with specifying any stride value != 1.</li>
<li><strong>activation</strong>: Activation function to use
    (see <a href="https://keras.io/activations/">activations</a>).
    If you don't specify anything, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to the kernel matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="maxpool1d">MaxPool1D</h3>
<p>Max pooling operation for temporal data.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>pool_size</strong>: Integer, size of the max pooling windows.</li>
<li><strong>strides</strong>: Integer, or None. Factor by which to downscale.
    E.g. 2 will halve the input.
    If None, it will default to <code>pool_size</code>.</li>
<li><strong>padding</strong>: One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</li>
<li><strong>data_format</strong>: A string,
    one of <code>channels_last</code> (default) or <code>channels_first</code>.
    The ordering of the dimensions in the inputs.
    <code>channels_last</code> corresponds to inputs with shape
    <code>(batch, steps, features)</code> while <code>channels_first</code>
    corresponds to inputs with shape
    <code>(batch, features, steps)</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="maxpool2d">MaxPool2D</h3>
<p>Max pooling operation for spatial data.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>pool_size</strong>: integer or tuple of 2 integers,
    factors by which to downscale (vertical, horizontal).
    (2, 2) will halve the input in both spatial dimension.
    If only one integer is specified, the same window length
    will be used for both dimensions.</li>
<li><strong>strides</strong>: Integer, tuple of 2 integers, or None.
    Strides values.
    If None, it will default to <code>pool_size</code>.</li>
<li><strong>padding</strong>: One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</li>
<li><strong>data_format</strong>: A string,
    one of <code>channels_last</code> (default) or <code>channels_first</code>.
    The ordering of the dimensions in the inputs.
    <code>channels_last</code> corresponds to inputs with shape
    <code>(batch, height, width, channels)</code> while <code>channels_first</code>
    corresponds to inputs with shape
    <code>(batch, channels, height, width)</code>.
    It defaults to the <code>image_data_format</code> value found in your
    Keras config file at <code>~/.keras/keras.json</code>.
    If you never set it, then it will be "channels_last".</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="averagepooling1d">AveragePooling1D</h3>
<p>Average pooling for temporal data.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>pool_size</strong>: Integer, size of the average pooling windows.</li>
<li><strong>strides</strong>: Integer, or None. Factor by which to downscale.
    E.g. 2 will halve the input.
    If None, it will default to <code>pool_size</code>.</li>
<li><strong>padding</strong>: One of <code>"valid"</code> or <code>"same"</code> (case-insensitive).</li>
<li><strong>data_format</strong>: A string,
    one of <code>channels_last</code> (default) or <code>channels_first</code>.
    The ordering of the dimensions in the inputs.
    <code>channels_last</code> corresponds to inputs with shape
    <code>(batch, steps, features)</code> while <code>channels_first</code>
    corresponds to inputs with shape
    <code>(batch, features, steps)</code>.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="cudnnlstm">CuDNNLSTM</h3>
<p>Fast LSTM implementation with <a href="https://developer.nvidia.com/cudnn">CuDNN</a>.</p>
<p>Can only be run on GPU, with the TensorFlow backend.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,
    used for the linear transformation of the inputs.
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>
    weights matrix,
    used for the linear transformation of the recurrent state.
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>unit_forget_bias</strong>: Boolean.
    If True, add 1 to the bias of the forget gate at initialization.
    Setting it to true will also force <code>bias_initializer="zeros"</code>.
    This is recommended in <a href="http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf">Jozefowicz et al. (2015)</a>.</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>recurrent_regularizer</strong>: Regularizer function applied to
    the <code>recurrent_kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>recurrent_constraint</strong>: Constraint function applied to
    the <code>recurrent_kernel</code> weights matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>return_sequences</strong>: Boolean. Whether to return the last output.
    in the output sequence, or the full sequence.</li>
<li><strong>return_state</strong>: Boolean. Whether to return the last state
    in addition to the output.</li>
<li><strong>stateful</strong>: Boolean (default False). If True, the last state
    for each sample at index i in a batch will be used as initial
    state for the sample of index i in the following batch.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="dense">Dense</h3>
<p>Regular densely-connected NN layer.</p>
<p><code>Dense</code> implements the operation:
<code>output = activation(dot(input, kernel) + bias)</code>
where <code>activation</code> is the element-wise activation function
passed as the <code>activation</code> argument, <code>kernel</code> is a weights matrix
created by the layer, and <code>bias</code> is a bias vector created by the layer
(only applicable if <code>use_bias</code> is <code>True</code>).</p>
<p>Note: if the input to the layer has a rank greater than 2, then
it is flattened prior to the initial dot product with <code>kernel</code>.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>
<li><strong>activation</strong>: Activation function to use
    (see <a href="https://keras.io/activations/">activations</a>).
    If you don't specify anything, no activation is applied
    (ie. "linear" activation: <code>a(x) = x</code>).</li>
<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>
<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>bias_initializer</strong>: Initializer for the bias vector
    (see <a href="https://keras.io/initializers/">initializers</a>).</li>
<li><strong>kernel_regularizer</strong>: Regularizer function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>activity_regularizer</strong>: Regularizer function applied to
    the output of the layer (its "activation").
    (see <a href="https://keras.io/regularizers/">regularizer</a>).</li>
<li><strong>kernel_constraint</strong>: Constraint function applied to
    the <code>kernel</code> weights matrix
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector
    (see <a href="https://keras.io/constraints/">constraints</a>).</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="flatten">Flatten</h3>
<p>Flattens the input. Does not affect the batch size.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>data_format</strong>: A string,
    one of <code>channels_last</code> (default) or <code>channels_first</code>.
    The ordering of the dimensions in the inputs.
    The purpose of this argument is to preserve weight
    ordering when switching a model from one data format
    to another.
    <code>channels_last</code> corresponds to inputs with shape
    <code>(batch, ..., channels)</code> while <code>channels_first</code> corresponds to
    inputs with shape <code>(batch, channels, ...)</code>.
    It defaults to the <code>image_data_format</code> value found in your
    Keras config file at <code>~/.keras/keras.json</code>.
    If you never set it, then it will be "channels_last".</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="bidirectional">Bidirectional</h3>
<p>Bidirectional wrapper for RNNs.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li><strong>layer</strong>: <code>Recurrent</code> instance.</li>
<li><strong>merge_mode</strong>: Mode by which outputs of the
    forward and backward RNNs will be combined.
    One of {'sum', 'mul', 'concat', 'ave', None}.
    If None, the outputs will not be combined,
    they will be returned as a list.</li>
<li><strong>weights</strong>: Initial weights to load in the Bidirectional model</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="utility-layers">Utility layers</h2>
<h3 id="split">split</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Number of outputs is equal to a number of children.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-concat">split-concat</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a concatenation of child flows.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">- split-concat:
         - word_indexes_embedding:  [ embeddings/glove.840B.300d.txt ]
         - word_indexes_embedding:  [ embeddings/paragram_300_sl999.txt ]
         - word_indexes_embedding:  [ embeddings/wiki-news-300d-1M.vec]
- lstm2: [128]
</code></pre>

<h3 id="split-concatenate">split-concatenate</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a concatenation of child flows (equal to the usage of <a href="#concatenate">Concatenate</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">- split-concat:
         - word_indexes_embedding:  [ embeddings/glove.840B.300d.txt ]
         - word_indexes_embedding:  [ embeddings/paragram_300_sl999.txt ]
         - word_indexes_embedding:  [ embeddings/wiki-news-300d-1M.vec]
- lstm2: [128]
</code></pre>

<h3 id="split-add">split-add</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is an addition of child flows (equal to the usage of <a href="#add">Add</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-substract">split-substract</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a substraction of child flows (equal to the usage of <a href="#substract">Substract</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-mult">split-mult</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a multiplication of child flows (equal to the usage of <a href="#mult">Mult</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-min">split-min</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a minimum of child flows (equal to the usage of <a href="#min">Min</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-max">split-max</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a maximum of child flows (equal to the usage of <a href="#max">Max</a> layer).</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li>**** - </li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-dot">split-dot</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a dot product of child flows.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-dot-normalize">split-dot-normalize</h3>
<p>Splits current flow into several ones.
Each child is a separate flow with an input equal to the input of the split operation.</p>
<p>Output is a dot product with normalization of child flows.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="seq">seq</h3>
<p>Executes child elements as a sequence of operations, one by one.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="input_1">input</h3>
<p>Overrides current input with what is listed.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">input: [firstRef, secondRef]
</code></pre>

<h3 id="pass">pass</h3>
<p>Stops execution of this branch and drops its output.
TODO check that description is correct.</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="transform-concat">transform-concat</h3>
<p>TODO</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li>**** - </li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="transform-add">transform-add</h3>
<p>TODO</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
<li>**** - </li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="stage-properties">Stage properties</h2>
<h3 id="callbacks_1">callbacks</h3>
<p><strong>type</strong>: <code>array of callback instances</code> </p>
<p>Sets up training-time callbacks. See individual <a href="#callback-types">callback descriptions</a>.</p>
<p>Example:</p>
<pre><code class="yaml">callbacks:
  EarlyStopping:
    patience: 100
    monitor: val_binary_accuracy
    verbose: 1
  ReduceLROnPlateau:
    patience: 16
    factor: 0.5
    monitor: val_binary_accuracy
    mode: auto
    cooldown: 5
    verbose: 1
</code></pre>

<h3 id="epochs">epochs</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Number of epochs to train for this stage.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="extra_callbacks">extra_callbacks</h3>
<p>TODO</p>
<h3 id="freeze_encoder_1">freeze_encoder</h3>
<p>TODO is this for generic?</p>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to freeze encoder during the training process.</p>
<p>Example:</p>
<pre><code class="yaml">freeze_encoder: true
stages:
  - epochs: 10 #Let's go for 10 epochs with frozen encoder

  - epochs: 100 #Now let's go for 100 epochs with trainable encoder
    unfreeze_encoder: true  
</code></pre>

<h3 id="initial_weights">initial_weights</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Fil path to load stage NN initial weights from.</p>
<p>Example:</p>
<pre><code class="yaml">initial_weights: /initial.weights
</code></pre>

<h3 id="negatives">negatives</h3>
<p><strong>type</strong>: <code>string or integer</code> </p>
<p>The support of binary data balancing for training set.</p>
<p>Following values are acceptable:</p>
<ul>
<li>none - exclude negative examples from the data</li>
<li>real - include all negative examples </li>
<li>integer number(1 or 2 or anything), how many negative examples should be included per one positive example</li>
</ul>
<p>In order for the system to determine whether a particular example is positive or negative,
the data set class defined by the <a href="#dataset">dataset</a> property should have <code>isPositive</code> method declared 
that accepts data set item and returns boolean.</p>
<p>Example:</p>
<pre><code class="yaml">stages:
  - epochs: 6 #Train for 6 epochs
    negatives: none #do not include negative examples in your training set 
    validation_negatives: real #validation should contain all negative examples    

  - lr: 0.0001 #let's use different starting learning rate
    epochs: 6
    negatives: real
    validation_negatives: real

  - loss: lovasz_loss #let's override loss function
    lr: 0.00001
    epochs: 6
    initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file    
</code></pre>

<h3 id="loss_1">loss</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Sets the loss name.</p>
<p>Uses loss name detection mechanism to search for the built-in loss or for a custom function with the same name across project modules.</p>
<p>Example:</p>
<pre><code class="yaml">loss: binary_crossentropy
</code></pre>

<h3 id="lr_1">lr</h3>
<p><strong>type</strong>: <code>float</code> </p>
<p>Learning rate.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="unfreeze_encoder">unfreeze_encoder</h3>
<p>TODO is this for generic?</p>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Whether to unfreeze encoder during the training process.</p>
<p>Example:</p>
<pre><code class="yaml">freeze_encoder: true
stages:
  - epochs: 10 #Let's go for 10 epochs with frozen encoder

  - epochs: 100 #Now let's go for 100 epochs with trainable encoder
    unfreeze_encoder: true  
</code></pre>

<h3 id="validation_negatives">validation_negatives</h3>
<p><strong>type</strong>: <code>string or integer</code> </p>
<p>The support of binary data balancing for validation set.</p>
<p>Following values are acceptable:</p>
<ul>
<li>none - exclude negative examples from the data</li>
<li>real - include all negative examples </li>
<li>integer number(1 or 2 or anything), how many negative examples should be included per one positive example</li>
</ul>
<p>In order for the system to determine whether a particular example is positive or negative,
the data set class defined by the <a href="#dataset">dataset</a> property should have <code>isPositive</code> method declared 
that accepts data set item and returns boolean.</p>
<p>Example:</p>
<pre><code class="yaml">stages:
  - epochs: 6 #Train for 6 epochs
    negatives: none #do not include negative examples in your training set 
    validation_negatives: real #validation should contain all negative examples    

  - lr: 0.0001 #let's use different starting learning rate
    epochs: 6
    negatives: real
    validation_negatives: real

  - loss: lovasz_loss #let's override loss function
    lr: 0.00001
    epochs: 6
    initial_weights: ./fpn-resnext2/weights/best-0.1.weights #let's load weights from this file    
</code></pre>

<h2 id="preprocessors">Preprocessors</h2>
<p><strong>type</strong>: <code>complex</code> </p>
<p>Preprocessors are the custom python functions that transform dataset. </p>
<p>Such functions should be defined in python files that are in a project scope (<code>modules</code>) folder and imported.
Preprocessing functions should be also marked with <code>@preprocessing.dataset_preprocessor</code> annotation.</p>
<p><code>Preprocessors</code> instruction then can be used to chain preprocessors as needed for this particular experiment, and even cache the result on disk to be reused between experiments.</p>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h3 id="cache">cache</h3>
<p>Caches its input.
TODO what for?</p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="disk-cache">disk-cache</h3>
<p>Caches its input on disk, including the full flow. 
On subsequent launches if nothing was changed in the flow, takes its output from disk instead of re-launching previous operations. </p>
<p>Properties:</p>
<ul>
<li><strong>name</strong> - string; optionally sets up layer name to refer it from other layers.</li>
<li><strong>inputs</strong> - array of strings; lists layer inputs.</li>
</ul>
<p>Example:</p>
<pre><code class="yaml">preprocessing: 
  - binarize_target: 
  - tokenize:  
  - tokens_to_indexes:
       maxLen: 160
  - disk-cache: 
</code></pre>

<h3 id="split-preprocessor">split-preprocessor</h3>
<p>An analogue of <a href="#split">split</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="split-concat-preprocessor">split-concat-preprocessor</h3>
<p>An analogue of <a href="#split-concat">split-concat</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="seq-preprocessor">seq-preprocessor</h3>
<p>An analogue of <a href="#seq-concat">seq</a> for preprocessor operations.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h3 id="augmentation_1">augmentation</h3>
<p>Preprocessor instruction, which body only runs during the training and is skipped when the inferring.</p>
<p>Example:</p>
<pre><code class="yaml">
</code></pre>

<h2 id="fit-script-arguments">fit script arguments</h2>
<h3 id="fitpy-project">fit.py project</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Folder to search for experiments, project root.</p>
<p>Example:</p>
<p><code>fit.py --project "path/to/project"</code></p>
<h3 id="fitpy-name">fit.py name</h3>
<p><strong>type</strong>: <code>string or comma-separated list of strings</code> </p>
<p>Name of the experiment to launch, or a list of names.</p>
<p>Example:</p>
<p><code>fit.py --name "experiment_name"</code></p>
<p><code>fit.py --name "experiment_name1, experiment_name2"</code></p>
<h3 id="fitpy-num_gpus">fit.py num_gpus</h3>
<p><strong>type</strong>: <code>integer</code></p>
<p>Default: 1</p>
<p>Number of GPUs to use during experiment launch.</p>
<p>Example:
<code>fit.py --num_gpus=1</code></p>
<h3 id="fitpy-gpus_per_net">fit.py gpus_per_net</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Default: 1</p>
<p>Maximum number of GPUs to use per single experiment.</p>
<p>Example:
<code>fit.py --gpus_per_net=1</code></p>
<h3 id="fitpy-num_workers">fit.py num_workers</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Default: 1</p>
<p>Number of workers to use.</p>
<p>Example:
<code>fit.py --num_workers=1</code></p>
<h3 id="fitpy-allow_resume">fit.py allow_resume</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to allow resuming of experiments, 
which will cause unfinished experiments to start from the best saved weights.</p>
<p>Example:
<code>fit.py --allow_resume True</code></p>
<h3 id="fitpy-force_recalc">fit.py force_recalc</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to force rebuilding of reports and predictions.</p>
<p>Example:
<code>fit.py --force_recalc True</code></p>
<h3 id="fitpy-launch_tasks">fit.py launch_tasks</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to launch associated tasks.</p>
<p>Example:
<code>fit.py --launch_tasks True</code></p>
<h3 id="fitpy-only_report">fit.py only_report</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to only generate reports for cached data, no training occurs.</p>
<p>Example:
<code>fit.py --only_report True</code></p>
<h3 id="fitpy-cache">fit.py cache</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Path to the cache folder. 
Cache folder will contain temporary cached data for executed experiments.</p>
<p>Example:
<code>fit.py --cache "path/to/cache/folder"</code></p>
<h3 id="fitpy-folds">fit.py folds</h3>
<p><strong>type</strong>: <code>integer or comma-separated list of integers</code> </p>
<p>Folds to launch. By default all folds of experiment will be executed, 
this argument allows launching only some of them. </p>
<p>Example:
<code>fit.py --folds 1,2</code></p>
<h3 id="fitpy-time">fit.py time</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>TODO </p>
<p>Example:
<code>fit.py</code></p>
<h2 id="task-script-arguments">task script arguments</h2>
<h3 id="taskpy-project">task.py project</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Folder to search for experiments, project root.</p>
<p>Example:</p>
<p><code>task.py --project "path/to/project"</code></p>
<h3 id="taskpy-name">task.py name</h3>
<p><strong>type</strong>: <code>string or comma-separated list of strings</code> </p>
<p>Name of the experiment to launch, or a list of names.</p>
<p>Example:</p>
<p><code>task.py --name "experiment_name"</code></p>
<p><code>task.py --name "experiment_name1, experiment_name2"</code></p>
<h3 id="taskpy-task">task.py task</h3>
<p><strong>type</strong>: <code>string or comma-separated list of strings</code> </p>
<p>Default: all tasks.</p>
<p>Name of the task to launch, or a list of names.</p>
<p>Example:</p>
<p><code>task.py --task "task_name"</code></p>
<p><code>task.py --task "task_name1, task_name2"</code></p>
<p><code>task.py --task "all"</code></p>
<h3 id="taskpy-num_gpus">task.py num_gpus</h3>
<p><strong>type</strong>: <code>integer</code></p>
<p>Default: 1</p>
<p>Number of GPUs to use during experiment launch.</p>
<p>Example:
<code>task.py --num_gpus=1</code></p>
<h3 id="taskpy-gpus_per_net">task.py gpus_per_net</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Default: 1</p>
<p>Maximum number of GPUs to use per single experiment.</p>
<p>Example:
<code>task.py --gpus_per_net=1</code></p>
<h3 id="taskpy-num_workers">task.py num_workers</h3>
<p><strong>type</strong>: <code>integer</code> </p>
<p>Default: 1</p>
<p>Number of workers to use.</p>
<p>Example:
<code>task.py --num_workers=1</code></p>
<h3 id="taskpy-allow_resume">task.py allow_resume</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to allow resuming of experiments, 
which will cause unfinished experiments to start from the best saved weights.</p>
<p>Example:
<code>task.py --allow_resume True</code></p>
<h3 id="taskpy-force_recalc">task.py force_recalc</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to force rebuilding of reports and predictions.</p>
<p>Example:
<code>task.py --force_recalc True</code></p>
<h3 id="taskpy-launch_tasks">task.py launch_tasks</h3>
<p><strong>type</strong>: <code>boolean</code> </p>
<p>Default: False</p>
<p>Whether to launch associated tasks.</p>
<p>Example:
<code>task.py --launch_tasks True</code></p>
<h3 id="taskpy-cache">task.py cache</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Path to the cache folder. 
Cache folder will contain temporary cached data for executed experiments.</p>
<p>Example:
<code>task.py --cache "path/to/cache/folder"</code></p>
<h2 id="analyze-script-arguments">analyze script arguments</h2>
<h3 id="analyzepy-inputfolder">analyze.py inputFolder</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Folder to search for finished experiments in. Typically, project root.</p>
<p>Example:</p>
<p><code>analyze.py --inputFolder "path/to/project"</code></p>
<h3 id="analyzepy-output">analyze.py output</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Default: <code>report.csv</code> in project root.</p>
<p>Output report file path.</p>
<p>Example:</p>
<p><code>analyze.py --output "path/to/project/report/report.scv"</code></p>
<h3 id="analyzepy-onlymetric">analyze.py onlyMetric</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Name of the single metric to take into account.</p>
<p>Example:</p>
<p><code>analyze.py --onlyMetric "metric_name"</code></p>
<h3 id="analyzepy-sortby">analyze.py sortBy</h3>
<p><strong>type</strong>: <code>string</code> </p>
<p>Name of the metric to sort result by.</p>
<p>Example:</p>
<p><code>analyze.py --sortBy "metric_name"</code></p>
              
            </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
      
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="versions">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
      
        <span><a href="../" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../../segmentation/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script>var base_url = '../..';</script>
    <script src="../../js/theme.js" defer></script>
      <script src="../../search/main.js" defer></script>
    <script type="text/javascript" defer>
        window.onload = function () {
            SphinxRtdTheme.Navigation.enable(false);
        };
    </script>

</body>
</html>
